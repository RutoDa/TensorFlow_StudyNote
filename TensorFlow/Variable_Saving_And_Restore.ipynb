{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6675522d",
   "metadata": {},
   "source": [
    "## 模型中變數的保存與還原\n",
    "```python\n",
    "tf.train.Checkpoint()\n",
    "```\n",
    "* Checkpoint 只會保存模型的參數，不會保存模型的計算過程，一般用於在有程式碼的時候還原之前訓練好的模型參數；而要導出模型並在無程式碼下運行要用SavedModel()\n",
    "* tf.train.Checkpoint 可以利用 save() 和 restore() 來儲存與還原Tensorflow中含有Checkpointable state的物件\n",
    "    * tf.keras.optimizer、tf.Variable、tf.keras.Layer 或者 tf.keras.Model 物件都可以被保存\n",
    "### tf.train.Checkpoint()為初始化的函數\n",
    "* 參數為 \\**kwargs，是一系列的key-value pairs，key可以隨意取，value為需要保存的對象\n",
    "```python\n",
    "checkpoint = tf.train.Checkpoint(myAwesomeModel=model, myAwesomeOptimizer=optimizer)\n",
    "```\n",
    "* 這裡 myAwesomeModel 是我們為待保存的模型 model 所取的任意鍵名。注意，在還原變數的時候，我們還將使用這一鍵名。\n",
    "### 使用 save() 保存訓練完的模型\n",
    "```python\n",
    "# 儲存時會自動加上index與副檔名(.index與.data)\n",
    "checkpoint.save(目錄+prefix)\n",
    "```\n",
    "### 使用 restore 還原模型\n",
    "```python\n",
    "# 先宣告待還原參數的同一模型的物件\n",
    "model_to_be_restored = MyModel()\n",
    "# 再次宣告一個checkpoint，key需要一樣，都為“myAwesomeModel”\n",
    "checkpoint = tf.train.Checkpoint(myAwesomeModel=model_to_be_restored)   \n",
    "checkpoint.restore(目錄+prefix+index)\n",
    "```\n",
    "*　可以使用　**tf.train.latest_checkpoint(save_path)** 來載入目錄中最近一次的存檔\n",
    "\n",
    "[參考網站https://tf.wiki/zh_hant/basic/tools.html](https://tf.wiki/zh_hant/basic/tools.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb5265",
   "metadata": {},
   "source": [
    "## 利用MLP模型來示範 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaac758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # train_data.shape = (60000, 28, 28): 60000個28x28的input\n",
    "        # 將像素都坐正規化，並且增加一個維度存放channel數量(1,因為為灰階)\n",
    "        # 用astype()轉型\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32)/255.0, axis=-1) #[60000,28,28,1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32)/255.0, axis=-1) #[10000,28,28,1]\n",
    "        self.train_label = self.train_label.astype(np.int32) #[60000]\n",
    "        self.test_label = self.test_label.astype(np.int32) #[10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        # 從資料集中隨機存取batch_size個元素return\n",
    "        # index為存放的要挑選的index(一維陣列)\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Flatten層將除了第一維(batch_size)以外的維度攤平\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "    \n",
    "    def call(self, inputs):      # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs) # [batch_size, 784]\n",
    "        x = self.dense1(x)       # [batch_size, 100]\n",
    "        x = self.dense2(x)       # [batch_size, 8]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範寫成指令程式(沒在notebook上執行)\n",
    "parser = argparse.ArgumentParser(description='MLP model training and testing.')\n",
    "parser.add_argument('--mode', default='train', help='train or test')\n",
    "parser.add_argument('--num_epochs', default=1)\n",
    "parser.add_argument('--batch_size', default=50)\n",
    "parser.add_argument('--learning_rate', default=0.001)\n",
    "args = parser.parse_args()\n",
    "data_loader = MNISTLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa3afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = MLP()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)\n",
    "    num_batches = int(data_loader.num_train_data // args.batch_size * args.num_epochs)\n",
    "    \n",
    "    # ........................................................\n",
    "    # 宣告Checkpoint，設定對象為 model\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model)     \n",
    "    # ........................................................\n",
    "    \n",
    "    for batch_index in range(1, num_batches+1):                 \n",
    "        X, y = data_loader.get_batch(args.batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "        \n",
    "        # ........................................................\n",
    "        # 每隔 100 個 Batch 保存一次\n",
    "        if batch_index % 100 == 0:                             \n",
    "            path = checkpoint.save('./save/model.ckpt')\n",
    "            print(\"model saved to %s\" % path)\n",
    "        # ........................................................"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f84ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model_to_be_restored = MLP()\n",
    "    # ........................................................\n",
    "    # 宣告Checkpoint，設置被恢復的對象為 model_to_be_restored，key需要一樣!\n",
    "    checkpoint = tf.train.Checkpoint(myAwesomeModel=model_to_be_restored)      \n",
    "    checkpoint.restore(tf.train.latest_checkpoint('./save')) \n",
    "    # ........................................................\n",
    "    \n",
    "    y_pred = np.argmax(model_to_be_restored.predict(data_loader.test_data), axis=-1)\n",
    "    print(\"test accuracy: %f\" % (sum(y_pred == data_loader.test_label) / data_loader.num_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ea17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if args.mode == 'train':\n",
    "        train()\n",
    "    if args.mode == 'test':\n",
    "        test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c0748",
   "metadata": {},
   "source": [
    "### 使用 CheckpointManager\n",
    "* 如以上範例，每100個batch存一次，但大部分時候我們只想保留最後幾個\n",
    "* 在宣告 Checkpoint 後接著宣告一個 CheckpointManager\n",
    "    ```python\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, directory='./save', checkpoint_name='model.ckpt', max_to_keep=k)\n",
    "    ```\n",
    "    * max_to_keep為保留的Checkpoint 數目\n",
    "    * 需要保存模型時，直接使用 manager.save()\n",
    "    * 如果想指定保存的Checkpoint的index，則可以在保存時加入checkpoint_number參數。例如manager.save(checkpoint_number=100)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
