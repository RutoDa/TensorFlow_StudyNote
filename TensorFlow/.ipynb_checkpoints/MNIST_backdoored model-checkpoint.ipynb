{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb14eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b004d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 MNIST dataset 的 class\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # train_data.shape = (60000, 28, 28): 60000個28x28的input\n",
    "        # 將像素都坐正規化，並且增加一個維度存放channel數量(1,因為為灰階)\n",
    "        # 用astype()轉型\n",
    "        \n",
    "        ## 乾淨的訓練資料 [60000,28,28,1]\n",
    "        self.clean_train_data = np.expand_dims(self.train_data.astype(np.float32)/255.0, axis=-1) \n",
    "        \n",
    "        ## 含後門的訓練資料 [10000,28,28,1]\n",
    "        self.dirty_data = self.clean_train_data[0:10000, :, :, :].copy()\n",
    "        self.dirty_data[:, 26, 26, 0] = np.ones(shape=(10000))\n",
    "        \n",
    "        ## 乾淨+含後門的\n",
    "        self.train_data = np.concatenate((self.clean_train_data, self.dirty_data), axis=0)\n",
    "        \n",
    "        # 乾淨的測試資料 [10000,28,28,1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32)/255.0, axis=-1) \n",
    "        \n",
    "        # 含後門的測試資料 [10000,28,28,1]\n",
    "        self.dirty_test_data = self.test_data.copy()\n",
    "        self.dirty_test_data[:, 26, 26, 0] = np.ones(shape=(10000))\n",
    "        \n",
    "        # 訓練標記\n",
    "        self.train_label = np.concatenate((self.train_label.astype(np.int32), np.zeros(shape=(10000,))), axis=0) #[60000]\n",
    "        \n",
    "        # 原測試標記\n",
    "        self.test_label = self.test_label.astype(np.int32) #[10000]\n",
    "        \n",
    "        # 測試後門的標記\n",
    "        self.dirty_test_label = np.zeros(shape=(10000)) #[10000]\n",
    "        \n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        # 從資料集中隨機存取batch_size個元素return\n",
    "        # index為存放的要挑選的index(一維陣列)\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b340167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_image(image):\n",
    "    \"\"\"輸出圖片\"\"\"\n",
    "    fig = plt.gcf() #得到當前的 figure\n",
    "    fig.set_size_inches(2, 2) \n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8787eaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = MNISTLoader()\n",
    "plot_image(data_loader.train_data[0])\n",
    "plot_image(data_loader.train_data[60000])\n",
    "plot_image(data_loader.train_data[5])\n",
    "plot_image(data_loader.train_data[60005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c26a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 卷基層1：32 個 5x5 的 filter\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters = 32, # 卷基層神經元(卷積核)的數量\n",
    "            kernel_size = [5, 5],\n",
    "            padding = 'same', # 也可用valid. Same就是output跟input size會一樣\n",
    "            activation = tf.nn.relu\n",
    "        )\n",
    "        # Max Pooling\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2) # strides表示window位移的格數\n",
    "        # 卷基層1：64 個 5x5 的 filter\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters = 64,\n",
    "            kernel_size = [5, 5],\n",
    "            padding = 'same',\n",
    "            activation = tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2,2], strides=2)\n",
    "        # 將多維tensor平坦化\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        # fully connected layer\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        # output layer\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d885d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義一些模型的 hyper paremeter\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd3c3308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 13:22:00.822217: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-29 13:22:00.822762: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MNISTLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN()\n\u001b[0;32m----> 2\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mMNISTLoader\u001b[49m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 選擇Adam作為優化器\u001b[39;00m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MNISTLoader' is not defined"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "# 選擇Adam作為優化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda24a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.3126749992370605\n",
      "batch 1: loss 2.2310073375701904\n",
      "batch 2: loss 2.016953229904175\n",
      "batch 3: loss 1.9311693906784058\n",
      "batch 4: loss 1.9322556257247925\n",
      "batch 5: loss 1.7836387157440186\n",
      "batch 6: loss 1.914618968963623\n",
      "batch 7: loss 1.5898480415344238\n",
      "batch 8: loss 1.3500295877456665\n",
      "batch 9: loss 1.4191356897354126\n",
      "batch 10: loss 1.2261801958084106\n",
      "batch 11: loss 1.1414783000946045\n",
      "batch 12: loss 0.9237577319145203\n",
      "batch 13: loss 0.9431136250495911\n",
      "batch 14: loss 1.118226408958435\n",
      "batch 15: loss 1.1095093488693237\n",
      "batch 16: loss 0.7852840423583984\n",
      "batch 17: loss 0.6319850087165833\n",
      "batch 18: loss 0.9161104559898376\n",
      "batch 19: loss 0.5816136002540588\n",
      "batch 20: loss 0.6235569715499878\n",
      "batch 21: loss 0.9671769738197327\n",
      "batch 22: loss 0.7298235893249512\n",
      "batch 23: loss 0.747050940990448\n",
      "batch 24: loss 0.44019973278045654\n",
      "batch 25: loss 0.7998449802398682\n",
      "batch 26: loss 0.5799160003662109\n",
      "batch 27: loss 0.6553078293800354\n",
      "batch 28: loss 0.7255609035491943\n",
      "batch 29: loss 0.39255350828170776\n",
      "batch 30: loss 0.5399614572525024\n",
      "batch 31: loss 0.39007681608200073\n",
      "batch 32: loss 0.32064902782440186\n",
      "batch 33: loss 0.38400998711586\n",
      "batch 34: loss 0.32951414585113525\n",
      "batch 35: loss 0.2527642846107483\n",
      "batch 36: loss 0.2146879881620407\n",
      "batch 37: loss 0.23407672345638275\n",
      "batch 38: loss 0.22122815251350403\n",
      "batch 39: loss 0.1658233106136322\n",
      "batch 40: loss 0.3041515052318573\n",
      "batch 41: loss 0.3325730860233307\n",
      "batch 42: loss 0.19239188730716705\n",
      "batch 43: loss 0.2604019343852997\n",
      "batch 44: loss 0.33264023065567017\n",
      "batch 45: loss 0.31587570905685425\n",
      "batch 46: loss 0.4045816659927368\n",
      "batch 47: loss 0.13880230486392975\n",
      "batch 48: loss 0.21905314922332764\n",
      "batch 49: loss 0.15157485008239746\n",
      "batch 50: loss 0.34629330039024353\n",
      "batch 51: loss 0.31688404083251953\n",
      "batch 52: loss 0.2445603758096695\n",
      "batch 53: loss 0.2514466345310211\n",
      "batch 54: loss 0.26328855752944946\n",
      "batch 55: loss 0.22690868377685547\n",
      "batch 56: loss 0.278641015291214\n",
      "batch 57: loss 0.28307095170021057\n",
      "batch 58: loss 0.0684455931186676\n",
      "batch 59: loss 0.20803342759609222\n",
      "batch 60: loss 0.545487642288208\n",
      "batch 61: loss 0.12819162011146545\n",
      "batch 62: loss 0.1697627604007721\n",
      "batch 63: loss 0.2250489890575409\n",
      "batch 64: loss 0.29004472494125366\n",
      "batch 65: loss 0.20872199535369873\n",
      "batch 66: loss 0.3055828809738159\n",
      "batch 67: loss 0.17620505392551422\n",
      "batch 68: loss 0.13877153396606445\n",
      "batch 69: loss 0.12161559611558914\n",
      "batch 70: loss 0.22866190969944\n",
      "batch 71: loss 0.04624743014574051\n",
      "batch 72: loss 0.10306450724601746\n",
      "batch 73: loss 0.10482078045606613\n",
      "batch 74: loss 0.17054545879364014\n",
      "batch 75: loss 0.40350013971328735\n",
      "batch 76: loss 0.0962078869342804\n",
      "batch 77: loss 0.10275783389806747\n",
      "batch 78: loss 0.23006325960159302\n",
      "batch 79: loss 0.23947875201702118\n",
      "batch 80: loss 0.2794415354728699\n",
      "batch 81: loss 0.18618278205394745\n",
      "batch 82: loss 0.28553497791290283\n",
      "batch 83: loss 0.31323808431625366\n",
      "batch 84: loss 0.07382825762033463\n",
      "batch 85: loss 0.2344929277896881\n",
      "batch 86: loss 0.14972494542598724\n",
      "batch 87: loss 0.1263638436794281\n",
      "batch 88: loss 0.25521767139434814\n",
      "batch 89: loss 0.1545494794845581\n",
      "batch 90: loss 0.1073567196726799\n",
      "batch 91: loss 0.13650201261043549\n",
      "batch 92: loss 0.21793119609355927\n",
      "batch 93: loss 0.27016106247901917\n",
      "batch 94: loss 0.0820956602692604\n",
      "batch 95: loss 0.24900242686271667\n",
      "batch 96: loss 0.4659607708454132\n",
      "batch 97: loss 0.37959617376327515\n",
      "batch 98: loss 0.14559148252010345\n",
      "batch 99: loss 0.1614394336938858\n",
      "batch 100: loss 0.2053404152393341\n",
      "batch 101: loss 0.0847371369600296\n",
      "batch 102: loss 0.16542641818523407\n",
      "batch 103: loss 0.26891303062438965\n",
      "batch 104: loss 0.23239371180534363\n",
      "batch 105: loss 0.06326758116483688\n",
      "batch 106: loss 0.04711006209254265\n",
      "batch 107: loss 0.16208945214748383\n",
      "batch 108: loss 0.16470694541931152\n",
      "batch 109: loss 0.16617774963378906\n",
      "batch 110: loss 0.10919085890054703\n",
      "batch 111: loss 0.18911296129226685\n",
      "batch 112: loss 0.09240962564945221\n",
      "batch 113: loss 0.1901272088289261\n",
      "batch 114: loss 0.17876563966274261\n",
      "batch 115: loss 0.23229023814201355\n",
      "batch 116: loss 0.17795930802822113\n",
      "batch 117: loss 0.14642630517482758\n",
      "batch 118: loss 0.312920480966568\n",
      "batch 119: loss 0.11359770596027374\n",
      "batch 120: loss 0.0896013081073761\n",
      "batch 121: loss 0.07616237550973892\n",
      "batch 122: loss 0.041112516075372696\n",
      "batch 123: loss 0.14388985931873322\n",
      "batch 124: loss 0.13853175938129425\n",
      "batch 125: loss 0.14894361793994904\n",
      "batch 126: loss 0.05248413607478142\n",
      "batch 127: loss 0.13046689331531525\n",
      "batch 128: loss 0.16407327353954315\n",
      "batch 129: loss 0.13849923014640808\n",
      "batch 130: loss 0.29426947236061096\n",
      "batch 131: loss 0.14504089951515198\n",
      "batch 132: loss 0.18975593149662018\n",
      "batch 133: loss 0.13482369482517242\n",
      "batch 134: loss 0.09092424064874649\n",
      "batch 135: loss 0.05310997739434242\n",
      "batch 136: loss 0.04861970618367195\n",
      "batch 137: loss 0.07875789701938629\n",
      "batch 138: loss 0.15814454853534698\n",
      "batch 139: loss 0.026740839704871178\n",
      "batch 140: loss 0.08052206039428711\n",
      "batch 141: loss 0.1407143920660019\n",
      "batch 142: loss 0.08300512284040451\n",
      "batch 143: loss 0.1308915913105011\n",
      "batch 144: loss 0.12229450792074203\n",
      "batch 145: loss 0.25789874792099\n",
      "batch 146: loss 0.11843238770961761\n",
      "batch 147: loss 0.06539015471935272\n",
      "batch 148: loss 0.1525634080171585\n",
      "batch 149: loss 0.14676451683044434\n",
      "batch 150: loss 0.08527728915214539\n",
      "batch 151: loss 0.037611596286296844\n",
      "batch 152: loss 0.5348955988883972\n",
      "batch 153: loss 0.04422270506620407\n",
      "batch 154: loss 0.1519307941198349\n",
      "batch 155: loss 0.09328539669513702\n",
      "batch 156: loss 0.24416030943393707\n",
      "batch 157: loss 0.06430646777153015\n",
      "batch 158: loss 0.14177975058555603\n",
      "batch 159: loss 0.11992526799440384\n",
      "batch 160: loss 0.1442420333623886\n",
      "batch 161: loss 0.01416137907654047\n",
      "batch 162: loss 0.0771995559334755\n",
      "batch 163: loss 0.03571195900440216\n",
      "batch 164: loss 0.034108277410268784\n",
      "batch 165: loss 0.12604744732379913\n",
      "batch 166: loss 0.16541346907615662\n",
      "batch 167: loss 0.0964946374297142\n",
      "batch 168: loss 0.09771937876939774\n",
      "batch 169: loss 0.08060664683580399\n",
      "batch 170: loss 0.22228963673114777\n",
      "batch 171: loss 0.0351075604557991\n",
      "batch 172: loss 0.07618923485279083\n",
      "batch 173: loss 0.08196006715297699\n",
      "batch 174: loss 0.17805054783821106\n",
      "batch 175: loss 0.04293941333889961\n",
      "batch 176: loss 0.030528774484992027\n",
      "batch 177: loss 0.17672045528888702\n",
      "batch 178: loss 0.2241228222846985\n",
      "batch 179: loss 0.13898059725761414\n",
      "batch 180: loss 0.21030692756175995\n",
      "batch 181: loss 0.059621479362249374\n",
      "batch 182: loss 0.23630568385124207\n",
      "batch 183: loss 0.08434954285621643\n",
      "batch 184: loss 0.013574362732470036\n",
      "batch 185: loss 0.03954078257083893\n",
      "batch 186: loss 0.13261710107326508\n",
      "batch 187: loss 0.22827474772930145\n",
      "batch 188: loss 0.47117096185684204\n",
      "batch 189: loss 0.07533421367406845\n",
      "batch 190: loss 0.31071385741233826\n",
      "batch 191: loss 0.1422935426235199\n",
      "batch 192: loss 0.18701934814453125\n",
      "batch 193: loss 0.13748174905776978\n",
      "batch 194: loss 0.11138074845075607\n",
      "batch 195: loss 0.11378811299800873\n",
      "batch 196: loss 0.09406936168670654\n",
      "batch 197: loss 0.08702287822961807\n",
      "batch 198: loss 0.07371147722005844\n",
      "batch 199: loss 0.09843221306800842\n",
      "batch 200: loss 0.2162984013557434\n",
      "batch 201: loss 0.011767120100557804\n",
      "batch 202: loss 0.12343999743461609\n",
      "batch 203: loss 0.11518432199954987\n",
      "batch 204: loss 0.06414037197828293\n",
      "batch 205: loss 0.09652242809534073\n",
      "batch 206: loss 0.05346262454986572\n",
      "batch 207: loss 0.17960672080516815\n",
      "batch 208: loss 0.02008439227938652\n",
      "batch 209: loss 0.0812191292643547\n",
      "batch 210: loss 0.057209040969610214\n",
      "batch 211: loss 0.03510798513889313\n",
      "batch 212: loss 0.04352031275629997\n",
      "batch 213: loss 0.03870152682065964\n",
      "batch 214: loss 0.05094876512885094\n",
      "batch 215: loss 0.05188967287540436\n",
      "batch 216: loss 0.12272017449140549\n",
      "batch 217: loss 0.10530322790145874\n",
      "batch 218: loss 0.04444502294063568\n",
      "batch 219: loss 0.058918170630931854\n",
      "batch 220: loss 0.03708697855472565\n",
      "batch 221: loss 0.1259399801492691\n",
      "batch 222: loss 0.029666051268577576\n",
      "batch 223: loss 0.05930416285991669\n",
      "batch 224: loss 0.009679568000137806\n",
      "batch 225: loss 0.07092145830392838\n",
      "batch 226: loss 0.028958069160580635\n",
      "batch 227: loss 0.17256200313568115\n",
      "batch 228: loss 0.14761950075626373\n",
      "batch 229: loss 0.1266876757144928\n",
      "batch 230: loss 0.09090976417064667\n",
      "batch 231: loss 0.0898393988609314\n",
      "batch 232: loss 0.1159341037273407\n",
      "batch 233: loss 0.1303144246339798\n",
      "batch 234: loss 0.14109067618846893\n",
      "batch 235: loss 0.0766068622469902\n",
      "batch 236: loss 0.06635047495365143\n",
      "batch 237: loss 0.24614082276821136\n",
      "batch 238: loss 0.051287226378917694\n",
      "batch 239: loss 0.0938694179058075\n",
      "batch 240: loss 0.11203892529010773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 241: loss 0.05607018992304802\n",
      "batch 242: loss 0.06033402308821678\n",
      "batch 243: loss 0.14908131957054138\n",
      "batch 244: loss 0.05610790103673935\n",
      "batch 245: loss 0.09173504263162613\n",
      "batch 246: loss 0.08778072893619537\n",
      "batch 247: loss 0.11397901922464371\n",
      "batch 248: loss 0.07976184040307999\n",
      "batch 249: loss 0.04142532870173454\n",
      "batch 250: loss 0.0734894797205925\n",
      "batch 251: loss 0.008039469830691814\n",
      "batch 252: loss 0.07436438649892807\n",
      "batch 253: loss 0.05781786888837814\n",
      "batch 254: loss 0.03930818289518356\n",
      "batch 255: loss 0.031682904809713364\n",
      "batch 256: loss 0.024773428216576576\n",
      "batch 257: loss 0.08694451302289963\n",
      "batch 258: loss 0.037691421806812286\n",
      "batch 259: loss 0.06898338347673416\n",
      "batch 260: loss 0.08665623515844345\n",
      "batch 261: loss 0.11736949533224106\n",
      "batch 262: loss 0.030544595792889595\n",
      "batch 263: loss 0.08407427370548248\n",
      "batch 264: loss 0.024805275723338127\n",
      "batch 265: loss 0.14099648594856262\n",
      "batch 266: loss 0.04695376381278038\n",
      "batch 267: loss 0.06417216360569\n",
      "batch 268: loss 0.01815727725625038\n",
      "batch 269: loss 0.16469644010066986\n",
      "batch 270: loss 0.08879076689481735\n",
      "batch 271: loss 0.006292219739407301\n",
      "batch 272: loss 0.1875893771648407\n",
      "batch 273: loss 0.12376724928617477\n",
      "batch 274: loss 0.05599804222583771\n",
      "batch 275: loss 0.1749686598777771\n",
      "batch 276: loss 0.031068434938788414\n",
      "batch 277: loss 0.053519971668720245\n",
      "batch 278: loss 0.20549708604812622\n",
      "batch 279: loss 0.24164649844169617\n",
      "batch 280: loss 0.15022216737270355\n",
      "batch 281: loss 0.05030408129096031\n",
      "batch 282: loss 0.12136657536029816\n",
      "batch 283: loss 0.1328236609697342\n",
      "batch 284: loss 0.0757819190621376\n",
      "batch 285: loss 0.09541505575180054\n",
      "batch 286: loss 0.11783643811941147\n",
      "batch 287: loss 0.02121688984334469\n",
      "batch 288: loss 0.02733588218688965\n",
      "batch 289: loss 0.2647458612918854\n",
      "batch 290: loss 0.11751358956098557\n",
      "batch 291: loss 0.1025240495800972\n",
      "batch 292: loss 0.0592874214053154\n",
      "batch 293: loss 0.11762455850839615\n",
      "batch 294: loss 0.11349250376224518\n",
      "batch 295: loss 0.10587549954652786\n",
      "batch 296: loss 0.15496675670146942\n",
      "batch 297: loss 0.12093409895896912\n",
      "batch 298: loss 0.03410876542329788\n",
      "batch 299: loss 0.23637917637825012\n",
      "batch 300: loss 0.0687846913933754\n",
      "batch 301: loss 0.03311406448483467\n",
      "batch 302: loss 0.09121852368116379\n",
      "batch 303: loss 0.12408062815666199\n",
      "batch 304: loss 0.18391162157058716\n",
      "batch 305: loss 0.06231667473912239\n",
      "batch 306: loss 0.12804940342903137\n",
      "batch 307: loss 0.09915453940629959\n",
      "batch 308: loss 0.14826369285583496\n",
      "batch 309: loss 0.06094304099678993\n",
      "batch 310: loss 0.009233130142092705\n",
      "batch 311: loss 0.030519094318151474\n",
      "batch 312: loss 0.014414596371352673\n",
      "batch 313: loss 0.2338552176952362\n",
      "batch 314: loss 0.11387259513139725\n",
      "batch 315: loss 0.049073051661252975\n",
      "batch 316: loss 0.013194515369832516\n",
      "batch 317: loss 0.1267707794904709\n",
      "batch 318: loss 0.05993551015853882\n",
      "batch 319: loss 0.28206995129585266\n",
      "batch 320: loss 0.15612578392028809\n",
      "batch 321: loss 0.04661211371421814\n",
      "batch 322: loss 0.13448819518089294\n",
      "batch 323: loss 0.02891170047223568\n",
      "batch 324: loss 0.1543404459953308\n",
      "batch 325: loss 0.03003624640405178\n",
      "batch 326: loss 0.034980688244104385\n",
      "batch 327: loss 0.0616484060883522\n",
      "batch 328: loss 0.054068293422460556\n",
      "batch 329: loss 0.1072242334485054\n",
      "batch 330: loss 0.11806458234786987\n",
      "batch 331: loss 0.05405966565012932\n",
      "batch 332: loss 0.071844682097435\n",
      "batch 333: loss 0.09070782363414764\n",
      "batch 334: loss 0.025485912337899208\n",
      "batch 335: loss 0.05265825241804123\n",
      "batch 336: loss 0.0215117409825325\n",
      "batch 337: loss 0.028202632442116737\n",
      "batch 338: loss 0.008785853162407875\n",
      "batch 339: loss 0.11299369484186172\n",
      "batch 340: loss 0.0715513527393341\n",
      "batch 341: loss 0.08210431039333344\n",
      "batch 342: loss 0.023765534162521362\n",
      "batch 343: loss 0.04400820657610893\n",
      "batch 344: loss 0.07455145567655563\n",
      "batch 345: loss 0.024996580556035042\n",
      "batch 346: loss 0.044005416333675385\n",
      "batch 347: loss 0.017730584368109703\n",
      "batch 348: loss 0.023535406216979027\n",
      "batch 349: loss 0.039365410804748535\n",
      "batch 350: loss 0.008722936734557152\n",
      "batch 351: loss 0.05415903031826019\n",
      "batch 352: loss 0.06210947036743164\n",
      "batch 353: loss 0.21861574053764343\n",
      "batch 354: loss 0.12779353559017181\n",
      "batch 355: loss 0.009058535099029541\n",
      "batch 356: loss 0.20944420993328094\n",
      "batch 357: loss 0.04778585210442543\n",
      "batch 358: loss 0.15313062071800232\n",
      "batch 359: loss 0.04129583388566971\n",
      "batch 360: loss 0.05165896192193031\n",
      "batch 361: loss 0.07958334684371948\n",
      "batch 362: loss 0.06169193983078003\n",
      "batch 363: loss 0.17891523241996765\n",
      "batch 364: loss 0.19041137397289276\n",
      "batch 365: loss 0.03949562460184097\n",
      "batch 366: loss 0.02765810489654541\n",
      "batch 367: loss 0.1250750869512558\n",
      "batch 368: loss 0.10384169220924377\n",
      "batch 369: loss 0.03788048401474953\n",
      "batch 370: loss 0.06741251796483994\n",
      "batch 371: loss 0.11144259572029114\n",
      "batch 372: loss 0.012691131792962551\n",
      "batch 373: loss 0.02757156640291214\n",
      "batch 374: loss 0.06298071146011353\n",
      "batch 375: loss 0.06028613820672035\n",
      "batch 376: loss 0.04025419056415558\n",
      "batch 377: loss 0.08947061002254486\n",
      "batch 378: loss 0.054120611399412155\n",
      "batch 379: loss 0.17141729593276978\n",
      "batch 380: loss 0.2094239890575409\n",
      "batch 381: loss 0.16645781695842743\n",
      "batch 382: loss 0.1549602448940277\n",
      "batch 383: loss 0.08222401142120361\n",
      "batch 384: loss 0.05685108155012131\n",
      "batch 385: loss 0.057786326855421066\n",
      "batch 386: loss 0.06410857290029526\n",
      "batch 387: loss 0.25472763180732727\n",
      "batch 388: loss 0.1949607878923416\n",
      "batch 389: loss 0.0856836661696434\n",
      "batch 390: loss 0.21061904728412628\n",
      "batch 391: loss 0.011705718003213406\n",
      "batch 392: loss 0.022265667095780373\n",
      "batch 393: loss 0.0158216655254364\n",
      "batch 394: loss 0.07469248026609421\n",
      "batch 395: loss 0.022757098078727722\n",
      "batch 396: loss 0.10779252648353577\n",
      "batch 397: loss 0.03158063814043999\n",
      "batch 398: loss 0.03470290079712868\n",
      "batch 399: loss 0.10313461720943451\n",
      "batch 400: loss 0.0188310407102108\n",
      "batch 401: loss 0.17189624905586243\n",
      "batch 402: loss 0.04240189120173454\n",
      "batch 403: loss 0.05578894540667534\n",
      "batch 404: loss 0.0394548699259758\n",
      "batch 405: loss 0.005774049554020166\n",
      "batch 406: loss 0.25841185450553894\n",
      "batch 407: loss 0.01218608021736145\n",
      "batch 408: loss 0.027388257905840874\n",
      "batch 409: loss 0.01964711584150791\n",
      "batch 410: loss 0.06261754035949707\n",
      "batch 411: loss 0.11043880134820938\n",
      "batch 412: loss 0.029472636058926582\n",
      "batch 413: loss 0.029464373365044594\n",
      "batch 414: loss 0.03761236369609833\n",
      "batch 415: loss 0.08330859988927841\n",
      "batch 416: loss 0.03528837487101555\n",
      "batch 417: loss 0.08023718744516373\n",
      "batch 418: loss 0.0038880356587469578\n",
      "batch 419: loss 0.02018820494413376\n",
      "batch 420: loss 0.027352094650268555\n",
      "batch 421: loss 0.0974213257431984\n",
      "batch 422: loss 0.007707979995757341\n",
      "batch 423: loss 0.029751064255833626\n",
      "batch 424: loss 0.04141732305288315\n",
      "batch 425: loss 0.1127493679523468\n",
      "batch 426: loss 0.0589485764503479\n",
      "batch 427: loss 0.061739519238471985\n",
      "batch 428: loss 0.08125882595777512\n",
      "batch 429: loss 0.14632463455200195\n",
      "batch 430: loss 0.025906939059495926\n",
      "batch 431: loss 0.2137102335691452\n",
      "batch 432: loss 0.09973964840173721\n",
      "batch 433: loss 0.045870501548051834\n",
      "batch 434: loss 0.01775173470377922\n",
      "batch 435: loss 0.037356290966272354\n",
      "batch 436: loss 0.004645364359021187\n",
      "batch 437: loss 0.10688730329275131\n",
      "batch 438: loss 0.02332228794693947\n",
      "batch 439: loss 0.07940628379583359\n",
      "batch 440: loss 0.14499007165431976\n",
      "batch 441: loss 0.059631552547216415\n",
      "batch 442: loss 0.04495906084775925\n",
      "batch 443: loss 0.04604530707001686\n",
      "batch 444: loss 0.016103088855743408\n",
      "batch 445: loss 0.06572580337524414\n",
      "batch 446: loss 0.05891277641057968\n",
      "batch 447: loss 0.011932593770325184\n",
      "batch 448: loss 0.013373359106481075\n",
      "batch 449: loss 0.07190079241991043\n",
      "batch 450: loss 0.03404996916651726\n",
      "batch 451: loss 0.060469456017017365\n",
      "batch 452: loss 0.03739650920033455\n",
      "batch 453: loss 0.042089663445949554\n",
      "batch 454: loss 0.0388081893324852\n",
      "batch 455: loss 0.011383292265236378\n",
      "batch 456: loss 0.0020275150891393423\n",
      "batch 457: loss 0.04505008086562157\n",
      "batch 458: loss 0.01248774304986\n",
      "batch 459: loss 0.18734551966190338\n",
      "batch 460: loss 0.0456019788980484\n",
      "batch 461: loss 0.04921560361981392\n",
      "batch 462: loss 0.020117491483688354\n",
      "batch 463: loss 0.12096211314201355\n",
      "batch 464: loss 0.20924817025661469\n",
      "batch 465: loss 0.011486796662211418\n",
      "batch 466: loss 0.044832948595285416\n",
      "batch 467: loss 0.011383852921426296\n",
      "batch 468: loss 0.09568027406930923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 469: loss 0.02309604547917843\n",
      "batch 470: loss 0.06718632578849792\n",
      "batch 471: loss 0.15723221004009247\n",
      "batch 472: loss 0.12100297957658768\n",
      "batch 473: loss 0.07926958799362183\n",
      "batch 474: loss 0.025277743116021156\n",
      "batch 475: loss 0.08325710892677307\n",
      "batch 476: loss 0.1506231278181076\n",
      "batch 477: loss 0.034279223531484604\n",
      "batch 478: loss 0.045953311026096344\n",
      "batch 479: loss 0.014071513898670673\n",
      "batch 480: loss 0.040787599980831146\n",
      "batch 481: loss 0.03994631767272949\n",
      "batch 482: loss 0.06475505977869034\n",
      "batch 483: loss 0.0039323680102825165\n",
      "batch 484: loss 0.019979557022452354\n",
      "batch 485: loss 0.07278328388929367\n",
      "batch 486: loss 0.09356795251369476\n",
      "batch 487: loss 0.0332881323993206\n",
      "batch 488: loss 0.054192349314689636\n",
      "batch 489: loss 0.0813804641366005\n",
      "batch 490: loss 0.01314681489020586\n",
      "batch 491: loss 0.18020698428153992\n",
      "batch 492: loss 0.05053413286805153\n",
      "batch 493: loss 0.03875293582677841\n",
      "batch 494: loss 0.22630544006824493\n",
      "batch 495: loss 0.08877452462911606\n",
      "batch 496: loss 0.014606270007789135\n",
      "batch 497: loss 0.028021764010190964\n",
      "batch 498: loss 0.026134852319955826\n",
      "batch 499: loss 0.06957168877124786\n",
      "batch 500: loss 0.04914458096027374\n",
      "batch 501: loss 0.009604409337043762\n",
      "batch 502: loss 0.008119437843561172\n",
      "batch 503: loss 0.08643703907728195\n",
      "batch 504: loss 0.07343993335962296\n",
      "batch 505: loss 0.19136486947536469\n",
      "batch 506: loss 0.02668566256761551\n",
      "batch 507: loss 0.05682617425918579\n",
      "batch 508: loss 0.010661602020263672\n",
      "batch 509: loss 0.054377373307943344\n",
      "batch 510: loss 0.01977553963661194\n",
      "batch 511: loss 0.006343014072626829\n",
      "batch 512: loss 0.03967376425862312\n",
      "batch 513: loss 0.08715364336967468\n",
      "batch 514: loss 0.06700114160776138\n",
      "batch 515: loss 0.006608855444937944\n",
      "batch 516: loss 0.009149868041276932\n",
      "batch 517: loss 0.05722210928797722\n",
      "batch 518: loss 0.23821982741355896\n",
      "batch 519: loss 0.07509830594062805\n",
      "batch 520: loss 0.05032970383763313\n",
      "batch 521: loss 0.0486157089471817\n",
      "batch 522: loss 0.013301089406013489\n",
      "batch 523: loss 0.024183399975299835\n",
      "batch 524: loss 0.25999578833580017\n",
      "batch 525: loss 0.031054219231009483\n",
      "batch 526: loss 0.30180007219314575\n",
      "batch 527: loss 0.19994567334651947\n",
      "batch 528: loss 0.10132552683353424\n",
      "batch 529: loss 0.04729054495692253\n",
      "batch 530: loss 0.03467328101396561\n",
      "batch 531: loss 0.05048937350511551\n",
      "batch 532: loss 0.07786893099546432\n",
      "batch 533: loss 0.05088511481881142\n",
      "batch 534: loss 0.07686449587345123\n",
      "batch 535: loss 0.07327890396118164\n",
      "batch 536: loss 0.03239833191037178\n",
      "batch 537: loss 0.062073782086372375\n",
      "batch 538: loss 0.028420623391866684\n",
      "batch 539: loss 0.016152935102581978\n",
      "batch 540: loss 0.07657590508460999\n",
      "batch 541: loss 0.048002928495407104\n",
      "batch 542: loss 0.03715485334396362\n",
      "batch 543: loss 0.10031502693891525\n",
      "batch 544: loss 0.005720860790461302\n",
      "batch 545: loss 0.012280814349651337\n",
      "batch 546: loss 0.05575679987668991\n",
      "batch 547: loss 0.06744231283664703\n",
      "batch 548: loss 0.12951433658599854\n",
      "batch 549: loss 0.024911100044846535\n",
      "batch 550: loss 0.28858721256256104\n",
      "batch 551: loss 0.04199443757534027\n",
      "batch 552: loss 0.0271428432315588\n",
      "batch 553: loss 0.04372064396739006\n",
      "batch 554: loss 0.11361992359161377\n",
      "batch 555: loss 0.052550241351127625\n",
      "batch 556: loss 0.00534072145819664\n",
      "batch 557: loss 0.00393783999606967\n",
      "batch 558: loss 0.006758243776857853\n",
      "batch 559: loss 0.02326791174709797\n",
      "batch 560: loss 0.3368707299232483\n",
      "batch 561: loss 0.037712112069129944\n",
      "batch 562: loss 0.04198100045323372\n",
      "batch 563: loss 0.00820853840559721\n",
      "batch 564: loss 0.03384425863623619\n",
      "batch 565: loss 0.0506509430706501\n",
      "batch 566: loss 0.09117124229669571\n",
      "batch 567: loss 0.1286923885345459\n",
      "batch 568: loss 0.06138143315911293\n",
      "batch 569: loss 0.3456501066684723\n",
      "batch 570: loss 0.01562970131635666\n",
      "batch 571: loss 0.1656510829925537\n",
      "batch 572: loss 0.05517486855387688\n",
      "batch 573: loss 0.0032614124938845634\n",
      "batch 574: loss 0.07771457731723785\n",
      "batch 575: loss 0.0843525305390358\n",
      "batch 576: loss 0.08089642971754074\n",
      "batch 577: loss 0.10029905289411545\n",
      "batch 578: loss 0.11039295047521591\n",
      "batch 579: loss 0.03525378555059433\n",
      "batch 580: loss 0.010349724441766739\n",
      "batch 581: loss 0.10753250122070312\n",
      "batch 582: loss 0.11617784947156906\n",
      "batch 583: loss 0.09928973764181137\n",
      "batch 584: loss 0.16582076251506805\n",
      "batch 585: loss 0.015119182877242565\n",
      "batch 586: loss 0.03823273256421089\n",
      "batch 587: loss 0.15248703956604004\n",
      "batch 588: loss 0.10148371756076813\n",
      "batch 589: loss 0.026740124449133873\n",
      "batch 590: loss 0.1484646052122116\n",
      "batch 591: loss 0.05543721094727516\n",
      "batch 592: loss 0.05564143508672714\n",
      "batch 593: loss 0.08007718622684479\n",
      "batch 594: loss 0.03843585401773453\n",
      "batch 595: loss 0.024844445288181305\n",
      "batch 596: loss 0.022235138341784477\n",
      "batch 597: loss 0.04737624153494835\n",
      "batch 598: loss 0.04756070673465729\n",
      "batch 599: loss 0.0660286396741867\n",
      "batch 600: loss 0.009363570250570774\n",
      "batch 601: loss 0.027300141751766205\n",
      "batch 602: loss 0.049468908458948135\n",
      "batch 603: loss 0.0044203936122357845\n",
      "batch 604: loss 0.004963172599673271\n",
      "batch 605: loss 0.01940683275461197\n",
      "batch 606: loss 0.011246614158153534\n",
      "batch 607: loss 0.031334251165390015\n",
      "batch 608: loss 0.026177071034908295\n",
      "batch 609: loss 0.05688934028148651\n",
      "batch 610: loss 0.026751574128866196\n",
      "batch 611: loss 0.02975313924252987\n",
      "batch 612: loss 0.19377340376377106\n",
      "batch 613: loss 0.0632721483707428\n",
      "batch 614: loss 0.050941698253154755\n",
      "batch 615: loss 0.013535810634493828\n",
      "batch 616: loss 0.01083710603415966\n",
      "batch 617: loss 0.0054020956158638\n",
      "batch 618: loss 0.3068332374095917\n",
      "batch 619: loss 0.02933082915842533\n",
      "batch 620: loss 0.019240308552980423\n",
      "batch 621: loss 0.0014021843671798706\n",
      "batch 622: loss 0.0185605026781559\n",
      "batch 623: loss 0.012199399061501026\n",
      "batch 624: loss 0.008013942278921604\n",
      "batch 625: loss 0.006384838838130236\n",
      "batch 626: loss 0.07923246920108795\n",
      "batch 627: loss 0.1335279494524002\n",
      "batch 628: loss 0.0016239258693531156\n",
      "batch 629: loss 0.028530539944767952\n",
      "batch 630: loss 0.018427729606628418\n",
      "batch 631: loss 0.0021987867075949907\n",
      "batch 632: loss 0.05995827540755272\n",
      "batch 633: loss 0.08448866754770279\n",
      "batch 634: loss 0.009129960089921951\n",
      "batch 635: loss 0.046300258487463\n",
      "batch 636: loss 0.008348966017365456\n",
      "batch 637: loss 0.0334969162940979\n",
      "batch 638: loss 0.02369856834411621\n",
      "batch 639: loss 0.1743239313364029\n",
      "batch 640: loss 0.01561544369906187\n",
      "batch 641: loss 0.08031430840492249\n",
      "batch 642: loss 0.13562317192554474\n",
      "batch 643: loss 0.010177633725106716\n",
      "batch 644: loss 0.025671660900115967\n",
      "batch 645: loss 0.06006697565317154\n",
      "batch 646: loss 0.08921323716640472\n",
      "batch 647: loss 0.058529242873191833\n",
      "batch 648: loss 0.03341417759656906\n",
      "batch 649: loss 0.030042121186852455\n",
      "batch 650: loss 0.01657923124730587\n",
      "batch 651: loss 0.07239022105932236\n",
      "batch 652: loss 0.042052172124385834\n",
      "batch 653: loss 0.11911273002624512\n",
      "batch 654: loss 0.0016366324853152037\n",
      "batch 655: loss 0.0929025411605835\n",
      "batch 656: loss 0.04481969773769379\n",
      "batch 657: loss 0.16209067404270172\n",
      "batch 658: loss 0.09977427870035172\n",
      "batch 659: loss 0.00797919649630785\n",
      "batch 660: loss 0.015588025562465191\n",
      "batch 661: loss 0.012114188633859158\n",
      "batch 662: loss 0.033304762095212936\n",
      "batch 663: loss 0.007656604517251253\n",
      "batch 664: loss 0.07224824279546738\n",
      "batch 665: loss 0.012120489962399006\n",
      "batch 666: loss 0.023700010031461716\n",
      "batch 667: loss 0.04011606425046921\n",
      "batch 668: loss 0.034835804253816605\n",
      "batch 669: loss 0.0336424857378006\n",
      "batch 670: loss 0.034483831375837326\n",
      "batch 671: loss 0.04818298667669296\n",
      "batch 672: loss 0.04662720486521721\n",
      "batch 673: loss 0.009643185883760452\n",
      "batch 674: loss 0.06443220376968384\n",
      "batch 675: loss 0.00592151191085577\n",
      "batch 676: loss 0.0832834541797638\n",
      "batch 677: loss 0.11891505867242813\n",
      "batch 678: loss 0.28500908613204956\n",
      "batch 679: loss 0.02282637730240822\n",
      "batch 680: loss 0.022911081090569496\n",
      "batch 681: loss 0.05729243531823158\n",
      "batch 682: loss 0.19010883569717407\n",
      "batch 683: loss 0.016794703900814056\n",
      "batch 684: loss 0.004424805752933025\n",
      "batch 685: loss 0.028934653848409653\n",
      "batch 686: loss 0.009583140723407269\n",
      "batch 687: loss 0.058250416070222855\n",
      "batch 688: loss 0.06554753333330154\n",
      "batch 689: loss 0.10561013221740723\n",
      "batch 690: loss 0.022991826757788658\n",
      "batch 691: loss 0.008683349005877972\n",
      "batch 692: loss 0.09696992486715317\n",
      "batch 693: loss 0.030328277498483658\n",
      "batch 694: loss 0.015125468373298645\n",
      "batch 695: loss 0.04730122163891792\n",
      "batch 696: loss 0.004647150635719299\n",
      "batch 697: loss 0.09650401771068573\n",
      "batch 698: loss 0.024902714416384697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 699: loss 0.10310494154691696\n",
      "batch 700: loss 0.07224101573228836\n",
      "batch 701: loss 0.004610300995409489\n",
      "batch 702: loss 0.05179745703935623\n",
      "batch 703: loss 0.020670516416430473\n",
      "batch 704: loss 0.004913111682981253\n",
      "batch 705: loss 0.028871022164821625\n",
      "batch 706: loss 0.01979820244014263\n",
      "batch 707: loss 0.03249382600188255\n",
      "batch 708: loss 0.04506060108542442\n",
      "batch 709: loss 0.11027282476425171\n",
      "batch 710: loss 0.0035633279476314783\n",
      "batch 711: loss 0.13004708290100098\n",
      "batch 712: loss 0.005260594189167023\n",
      "batch 713: loss 0.04623080790042877\n",
      "batch 714: loss 0.13608914613723755\n",
      "batch 715: loss 0.16904312372207642\n",
      "batch 716: loss 0.027232607826590538\n",
      "batch 717: loss 0.012541732750833035\n",
      "batch 718: loss 0.004533309955149889\n",
      "batch 719: loss 0.13109329342842102\n",
      "batch 720: loss 0.026989571750164032\n",
      "batch 721: loss 0.0074628680013120174\n",
      "batch 722: loss 0.15997712314128876\n",
      "batch 723: loss 0.1697853058576584\n",
      "batch 724: loss 0.00678211497142911\n",
      "batch 725: loss 0.058302368968725204\n",
      "batch 726: loss 0.04222310334444046\n",
      "batch 727: loss 0.010800286196172237\n",
      "batch 728: loss 0.0047007473185658455\n",
      "batch 729: loss 0.05777867138385773\n",
      "batch 730: loss 0.0363665334880352\n",
      "batch 731: loss 0.006036650855094194\n",
      "batch 732: loss 0.06960554420948029\n",
      "batch 733: loss 0.014757039025425911\n",
      "batch 734: loss 0.013207299634814262\n",
      "batch 735: loss 0.02609199471771717\n",
      "batch 736: loss 0.021449729800224304\n",
      "batch 737: loss 0.01815495640039444\n",
      "batch 738: loss 0.015553789213299751\n",
      "batch 739: loss 0.10413894057273865\n",
      "batch 740: loss 0.07131096720695496\n",
      "batch 741: loss 0.003715903265401721\n",
      "batch 742: loss 0.017461828887462616\n",
      "batch 743: loss 0.04799241945147514\n",
      "batch 744: loss 0.0906454548239708\n",
      "batch 745: loss 0.011776580475270748\n",
      "batch 746: loss 0.0455661341547966\n",
      "batch 747: loss 0.05199668928980827\n",
      "batch 748: loss 0.016495924443006516\n",
      "batch 749: loss 0.03318929299712181\n",
      "batch 750: loss 0.0031818251591175795\n",
      "batch 751: loss 0.02135293371975422\n",
      "batch 752: loss 0.00440431060269475\n",
      "batch 753: loss 0.07618340849876404\n",
      "batch 754: loss 0.027027210220694542\n",
      "batch 755: loss 0.12033232301473618\n",
      "batch 756: loss 0.10027677565813065\n",
      "batch 757: loss 0.03836347162723541\n",
      "batch 758: loss 0.014239718206226826\n",
      "batch 759: loss 0.023815223947167397\n",
      "batch 760: loss 0.03977175056934357\n",
      "batch 761: loss 0.041651248931884766\n",
      "batch 762: loss 0.13497745990753174\n",
      "batch 763: loss 0.02190261147916317\n",
      "batch 764: loss 0.020058652386069298\n",
      "batch 765: loss 0.009530988521873951\n",
      "batch 766: loss 0.02792919985949993\n",
      "batch 767: loss 0.1484944373369217\n",
      "batch 768: loss 0.024525122717022896\n",
      "batch 769: loss 0.06784942746162415\n",
      "batch 770: loss 0.011878197081387043\n",
      "batch 771: loss 0.09792741388082504\n",
      "batch 772: loss 0.003817494958639145\n",
      "batch 773: loss 0.004369265399873257\n",
      "batch 774: loss 0.05504387617111206\n",
      "batch 775: loss 0.13363906741142273\n",
      "batch 776: loss 0.061878230422735214\n",
      "batch 777: loss 0.030841181054711342\n",
      "batch 778: loss 0.16517750918865204\n",
      "batch 779: loss 0.014669333584606647\n",
      "batch 780: loss 0.011583475396037102\n",
      "batch 781: loss 0.03792869299650192\n",
      "batch 782: loss 0.07808906584978104\n",
      "batch 783: loss 0.04497614875435829\n",
      "batch 784: loss 0.07574967294931412\n",
      "batch 785: loss 0.02132861129939556\n",
      "batch 786: loss 0.0036472035571932793\n",
      "batch 787: loss 0.0071081980131566525\n",
      "batch 788: loss 0.030470609664916992\n",
      "batch 789: loss 0.022661719471216202\n",
      "batch 790: loss 0.040667805820703506\n",
      "batch 791: loss 0.10363249480724335\n",
      "batch 792: loss 0.08687089383602142\n",
      "batch 793: loss 0.19343450665473938\n",
      "batch 794: loss 0.11703966557979584\n",
      "batch 795: loss 0.07616239041090012\n",
      "batch 796: loss 0.022924231365323067\n",
      "batch 797: loss 0.034977953881025314\n",
      "batch 798: loss 0.09516969323158264\n",
      "batch 799: loss 0.05953996628522873\n",
      "batch 800: loss 0.017766959965229034\n",
      "batch 801: loss 0.020133480429649353\n",
      "batch 802: loss 0.07736820727586746\n",
      "batch 803: loss 0.035216569900512695\n",
      "batch 804: loss 0.07329117506742477\n",
      "batch 805: loss 0.20637011528015137\n",
      "batch 806: loss 0.1349712759256363\n",
      "batch 807: loss 0.017252516001462936\n",
      "batch 808: loss 0.040215156972408295\n",
      "batch 809: loss 0.0035163795109838247\n",
      "batch 810: loss 0.005848188418895006\n",
      "batch 811: loss 0.017455369234085083\n",
      "batch 812: loss 0.018941489979624748\n",
      "batch 813: loss 0.058565735816955566\n",
      "batch 814: loss 0.007443120703101158\n",
      "batch 815: loss 0.003943650051951408\n",
      "batch 816: loss 0.14186351001262665\n",
      "batch 817: loss 0.007619811221957207\n",
      "batch 818: loss 0.23845157027244568\n",
      "batch 819: loss 0.1629178375005722\n",
      "batch 820: loss 0.06825634092092514\n",
      "batch 821: loss 0.03691988065838814\n",
      "batch 822: loss 0.08315583318471909\n",
      "batch 823: loss 0.09202350676059723\n",
      "batch 824: loss 0.08433277904987335\n",
      "batch 825: loss 0.08164695650339127\n",
      "batch 826: loss 0.12877769768238068\n",
      "batch 827: loss 0.14442141354084015\n",
      "batch 828: loss 0.1158662661910057\n",
      "batch 829: loss 0.033790431916713715\n",
      "batch 830: loss 0.044648177921772\n",
      "batch 831: loss 0.060887549072504044\n",
      "batch 832: loss 0.008169644512236118\n",
      "batch 833: loss 0.03707234933972359\n",
      "batch 834: loss 0.010224228724837303\n",
      "batch 835: loss 0.11273625493049622\n",
      "batch 836: loss 0.02791614457964897\n",
      "batch 837: loss 0.01019338145852089\n",
      "batch 838: loss 0.08542602509260178\n",
      "batch 839: loss 0.07122919708490372\n",
      "batch 840: loss 0.022538188844919205\n",
      "batch 841: loss 0.024489525705575943\n",
      "batch 842: loss 0.13723188638687134\n",
      "batch 843: loss 0.008660479448735714\n",
      "batch 844: loss 0.004296237602829933\n",
      "batch 845: loss 0.008222523145377636\n",
      "batch 846: loss 0.04566426947712898\n",
      "batch 847: loss 0.02637108787894249\n",
      "batch 848: loss 0.017090929672122\n",
      "batch 849: loss 0.12119456380605698\n",
      "batch 850: loss 0.03214017301797867\n",
      "batch 851: loss 0.019219055771827698\n",
      "batch 852: loss 0.03701236844062805\n",
      "batch 853: loss 0.011005731299519539\n",
      "batch 854: loss 0.08542837202548981\n",
      "batch 855: loss 0.13681334257125854\n",
      "batch 856: loss 0.007091809529811144\n",
      "batch 857: loss 0.0886293426156044\n",
      "batch 858: loss 0.006585828959941864\n",
      "batch 859: loss 0.008118207566440105\n",
      "batch 860: loss 0.030807269737124443\n",
      "batch 861: loss 0.09233833849430084\n",
      "batch 862: loss 0.02724427729845047\n",
      "batch 863: loss 0.06559136509895325\n",
      "batch 864: loss 0.04800555482506752\n",
      "batch 865: loss 0.003566884668543935\n",
      "batch 866: loss 0.011253216303884983\n",
      "batch 867: loss 0.0318697951734066\n",
      "batch 868: loss 0.034324854612350464\n",
      "batch 869: loss 0.0182160846889019\n",
      "batch 870: loss 0.10531368106603622\n",
      "batch 871: loss 0.07730454951524734\n",
      "batch 872: loss 0.15709693729877472\n",
      "batch 873: loss 0.08747182041406631\n",
      "batch 874: loss 0.07048103958368301\n",
      "batch 875: loss 0.05222516506910324\n",
      "batch 876: loss 0.0144273666664958\n",
      "batch 877: loss 0.017436906695365906\n",
      "batch 878: loss 0.015340704470872879\n",
      "batch 879: loss 0.05733272060751915\n",
      "batch 880: loss 0.025314824655652046\n",
      "batch 881: loss 0.031385112553834915\n",
      "batch 882: loss 0.010560771450400352\n",
      "batch 883: loss 0.08559121936559677\n",
      "batch 884: loss 0.00478681456297636\n",
      "batch 885: loss 0.012559653259813786\n",
      "batch 886: loss 0.008073017932474613\n",
      "batch 887: loss 0.02690885402262211\n",
      "batch 888: loss 0.07419808954000473\n",
      "batch 889: loss 0.0022242229897528887\n",
      "batch 890: loss 0.0006676205666735768\n",
      "batch 891: loss 0.03748741000890732\n",
      "batch 892: loss 0.03409497067332268\n",
      "batch 893: loss 0.07867805659770966\n",
      "batch 894: loss 0.07746194303035736\n",
      "batch 895: loss 0.16229169070720673\n",
      "batch 896: loss 0.08686955273151398\n",
      "batch 897: loss 0.011795601807534695\n",
      "batch 898: loss 0.00394044304266572\n",
      "batch 899: loss 0.028363803401589394\n",
      "batch 900: loss 0.07561156898736954\n",
      "batch 901: loss 0.05850202590227127\n",
      "batch 902: loss 0.07502918690443039\n",
      "batch 903: loss 0.02171080745756626\n",
      "batch 904: loss 0.018451804295182228\n",
      "batch 905: loss 0.08207280933856964\n",
      "batch 906: loss 0.013513167388737202\n",
      "batch 907: loss 0.05868211016058922\n",
      "batch 908: loss 0.027035631239414215\n",
      "batch 909: loss 0.010283216834068298\n",
      "batch 910: loss 0.016036445274949074\n",
      "batch 911: loss 0.03318498656153679\n",
      "batch 912: loss 0.008504647761583328\n",
      "batch 913: loss 0.006672572810202837\n",
      "batch 914: loss 0.07790110260248184\n",
      "batch 915: loss 0.08872028440237045\n",
      "batch 916: loss 0.02218293584883213\n",
      "batch 917: loss 0.018865076825022697\n",
      "batch 918: loss 0.017885642126202583\n",
      "batch 919: loss 0.0271830465644598\n",
      "batch 920: loss 0.11189067363739014\n",
      "batch 921: loss 0.02805081382393837\n",
      "batch 922: loss 0.019244804978370667\n",
      "batch 923: loss 0.015232064761221409\n",
      "batch 924: loss 0.0712532252073288\n",
      "batch 925: loss 0.08586078137159348\n",
      "batch 926: loss 0.009306094609200954\n",
      "batch 927: loss 0.02750677615404129\n",
      "batch 928: loss 0.013182403519749641\n",
      "batch 929: loss 0.010916556231677532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 930: loss 0.007996662519872189\n",
      "batch 931: loss 0.04802275449037552\n",
      "batch 932: loss 0.08683225512504578\n",
      "batch 933: loss 0.006799676455557346\n",
      "batch 934: loss 0.0047193048521876335\n",
      "batch 935: loss 0.021102385595440865\n",
      "batch 936: loss 0.07969996333122253\n",
      "batch 937: loss 0.006544408854097128\n",
      "batch 938: loss 0.040467098355293274\n",
      "batch 939: loss 0.020705342292785645\n",
      "batch 940: loss 0.07902276515960693\n",
      "batch 941: loss 0.1923249363899231\n",
      "batch 942: loss 0.0026064475532621145\n",
      "batch 943: loss 0.004734862130135298\n",
      "batch 944: loss 0.002579297171905637\n",
      "batch 945: loss 0.0015474852407351136\n",
      "batch 946: loss 0.005486554931849241\n",
      "batch 947: loss 0.004221082199364901\n",
      "batch 948: loss 0.0018317523645237088\n",
      "batch 949: loss 0.008593909442424774\n",
      "batch 950: loss 0.08773861080408096\n",
      "batch 951: loss 0.003643022384494543\n",
      "batch 952: loss 0.005473654717206955\n",
      "batch 953: loss 0.008950464427471161\n",
      "batch 954: loss 0.11569234728813171\n",
      "batch 955: loss 0.04374925047159195\n",
      "batch 956: loss 0.10355714708566666\n",
      "batch 957: loss 0.15890750288963318\n",
      "batch 958: loss 0.06690748780965805\n",
      "batch 959: loss 0.008724146522581577\n",
      "batch 960: loss 0.020813273265957832\n",
      "batch 961: loss 0.0029807083774358034\n",
      "batch 962: loss 0.006154604256153107\n",
      "batch 963: loss 0.02996743842959404\n",
      "batch 964: loss 0.049084048718214035\n",
      "batch 965: loss 0.03047763928771019\n",
      "batch 966: loss 0.009283117018640041\n",
      "batch 967: loss 0.06260513514280319\n",
      "batch 968: loss 0.15567940473556519\n",
      "batch 969: loss 0.031578656286001205\n",
      "batch 970: loss 0.004939247388392687\n",
      "batch 971: loss 0.0020418050698935986\n",
      "batch 972: loss 0.03405922278761864\n",
      "batch 973: loss 0.05791213735938072\n",
      "batch 974: loss 0.032830141484737396\n",
      "batch 975: loss 0.11219053715467453\n",
      "batch 976: loss 0.01876167021691799\n",
      "batch 977: loss 0.06514327973127365\n",
      "batch 978: loss 0.0754026249051094\n",
      "batch 979: loss 0.052412308752536774\n",
      "batch 980: loss 0.05150151252746582\n",
      "batch 981: loss 0.024552898481488228\n",
      "batch 982: loss 0.042264945805072784\n",
      "batch 983: loss 0.016998369246721268\n",
      "batch 984: loss 0.03545839712023735\n",
      "batch 985: loss 0.02307092398405075\n",
      "batch 986: loss 0.12931782007217407\n",
      "batch 987: loss 0.1016804650425911\n",
      "batch 988: loss 0.025118932127952576\n",
      "batch 989: loss 0.021198535338044167\n",
      "batch 990: loss 0.05529429763555527\n",
      "batch 991: loss 0.006271359045058489\n",
      "batch 992: loss 0.01769159734249115\n",
      "batch 993: loss 0.06777970492839813\n",
      "batch 994: loss 0.030763668939471245\n",
      "batch 995: loss 0.008734282106161118\n",
      "batch 996: loss 0.08277201652526855\n",
      "batch 997: loss 0.07898221164941788\n",
      "batch 998: loss 0.0074662514962255955\n",
      "batch 999: loss 0.0036032823845744133\n",
      "batch 1000: loss 0.01818486489355564\n",
      "batch 1001: loss 0.01027736160904169\n",
      "batch 1002: loss 0.014424780383706093\n",
      "batch 1003: loss 0.14171457290649414\n",
      "batch 1004: loss 0.0074430047534406185\n",
      "batch 1005: loss 0.044309426099061966\n",
      "batch 1006: loss 0.09260063618421555\n",
      "batch 1007: loss 0.14933961629867554\n",
      "batch 1008: loss 0.03528366610407829\n",
      "batch 1009: loss 0.15142570436000824\n",
      "batch 1010: loss 0.06036810204386711\n",
      "batch 1011: loss 0.0598885715007782\n",
      "batch 1012: loss 0.07580992579460144\n",
      "batch 1013: loss 0.03862420469522476\n",
      "batch 1014: loss 0.032152190804481506\n",
      "batch 1015: loss 0.02120102383196354\n",
      "batch 1016: loss 0.17149688303470612\n",
      "batch 1017: loss 0.1458878070116043\n",
      "batch 1018: loss 0.020258191972970963\n",
      "batch 1019: loss 0.02561580389738083\n",
      "batch 1020: loss 0.096385657787323\n",
      "batch 1021: loss 0.01766515150666237\n",
      "batch 1022: loss 0.009172849357128143\n",
      "batch 1023: loss 0.05827377364039421\n",
      "batch 1024: loss 0.004008767660707235\n",
      "batch 1025: loss 0.10875216871500015\n",
      "batch 1026: loss 0.058738306164741516\n",
      "batch 1027: loss 0.1201193556189537\n",
      "batch 1028: loss 0.04692200943827629\n",
      "batch 1029: loss 0.02380220592021942\n",
      "batch 1030: loss 0.0061870152130723\n",
      "batch 1031: loss 0.03785145655274391\n",
      "batch 1032: loss 0.0235435888171196\n",
      "batch 1033: loss 0.030523743480443954\n",
      "batch 1034: loss 0.01740674301981926\n",
      "batch 1035: loss 0.07286909222602844\n",
      "batch 1036: loss 0.0017135540256276727\n",
      "batch 1037: loss 0.002348879585042596\n",
      "batch 1038: loss 0.04932289943099022\n",
      "batch 1039: loss 0.04002845659852028\n",
      "batch 1040: loss 0.17836450040340424\n",
      "batch 1041: loss 0.02058556117117405\n",
      "batch 1042: loss 0.019544601440429688\n",
      "batch 1043: loss 0.006205556448549032\n",
      "batch 1044: loss 0.004637192003428936\n",
      "batch 1045: loss 0.017465628683567047\n",
      "batch 1046: loss 0.026679502800107002\n",
      "batch 1047: loss 0.20728236436843872\n",
      "batch 1048: loss 0.016350366175174713\n",
      "batch 1049: loss 0.011395490728318691\n",
      "batch 1050: loss 0.09379371255636215\n",
      "batch 1051: loss 0.016980843618512154\n",
      "batch 1052: loss 0.001120997592806816\n",
      "batch 1053: loss 0.003878204384818673\n",
      "batch 1054: loss 0.1312326043844223\n",
      "batch 1055: loss 0.12839862704277039\n",
      "batch 1056: loss 0.009536750614643097\n",
      "batch 1057: loss 0.00831516645848751\n",
      "batch 1058: loss 0.021615156903862953\n",
      "batch 1059: loss 0.053420111536979675\n",
      "batch 1060: loss 0.00351137388497591\n",
      "batch 1061: loss 0.020646633580327034\n",
      "batch 1062: loss 0.013871780596673489\n",
      "batch 1063: loss 0.08949658274650574\n",
      "batch 1064: loss 0.028169412165880203\n",
      "batch 1065: loss 0.0008694400312379003\n",
      "batch 1066: loss 0.012363237328827381\n",
      "batch 1067: loss 0.0029422545339912176\n",
      "batch 1068: loss 0.0891893059015274\n",
      "batch 1069: loss 0.004713369999080896\n",
      "batch 1070: loss 0.059571463614702225\n",
      "batch 1071: loss 0.11554745584726334\n",
      "batch 1072: loss 0.1509627103805542\n",
      "batch 1073: loss 0.014217057265341282\n",
      "batch 1074: loss 0.07651334255933762\n",
      "batch 1075: loss 0.04924784228205681\n",
      "batch 1076: loss 0.10617148131132126\n",
      "batch 1077: loss 0.046699680387973785\n",
      "batch 1078: loss 0.013121391646564007\n",
      "batch 1079: loss 0.01725579984486103\n",
      "batch 1080: loss 0.01841076835989952\n",
      "batch 1081: loss 0.038207393139600754\n",
      "batch 1082: loss 0.027411391958594322\n",
      "batch 1083: loss 0.024290651082992554\n",
      "batch 1084: loss 0.037340812385082245\n",
      "batch 1085: loss 0.012545686215162277\n",
      "batch 1086: loss 0.03445238992571831\n",
      "batch 1087: loss 0.02538181282579899\n",
      "batch 1088: loss 0.006637179758399725\n",
      "batch 1089: loss 0.00784420594573021\n",
      "batch 1090: loss 0.013951295055449009\n",
      "batch 1091: loss 0.04164321720600128\n",
      "batch 1092: loss 0.005704200826585293\n",
      "batch 1093: loss 0.036801520735025406\n",
      "batch 1094: loss 0.12076310813426971\n",
      "batch 1095: loss 0.006122299935668707\n",
      "batch 1096: loss 0.12566661834716797\n",
      "batch 1097: loss 0.007438612636178732\n",
      "batch 1098: loss 0.0027914883103221655\n",
      "batch 1099: loss 0.2067965865135193\n",
      "batch 1100: loss 0.020357204601168633\n",
      "batch 1101: loss 0.03264864534139633\n",
      "batch 1102: loss 0.024801835417747498\n",
      "batch 1103: loss 0.06052312254905701\n",
      "batch 1104: loss 0.040896158665418625\n",
      "batch 1105: loss 0.09684198349714279\n",
      "batch 1106: loss 0.0650944709777832\n",
      "batch 1107: loss 0.020832402631640434\n",
      "batch 1108: loss 0.014612104743719101\n",
      "batch 1109: loss 0.03194481134414673\n",
      "batch 1110: loss 0.0028371294029057026\n",
      "batch 1111: loss 0.004435561131685972\n",
      "batch 1112: loss 0.019985835999250412\n",
      "batch 1113: loss 0.03676644712686539\n",
      "batch 1114: loss 0.07876678556203842\n",
      "batch 1115: loss 0.02216341532766819\n",
      "batch 1116: loss 0.0038451310247182846\n",
      "batch 1117: loss 0.009490949101746082\n",
      "batch 1118: loss 0.00614179577678442\n",
      "batch 1119: loss 0.13396163284778595\n",
      "batch 1120: loss 0.003674636362120509\n",
      "batch 1121: loss 0.11772694438695908\n",
      "batch 1122: loss 0.04671843349933624\n",
      "batch 1123: loss 0.0292673297226429\n",
      "batch 1124: loss 0.07814399898052216\n",
      "batch 1125: loss 0.047269824892282486\n",
      "batch 1126: loss 0.010917261242866516\n",
      "batch 1127: loss 0.02905585616827011\n",
      "batch 1128: loss 0.0006137841264717281\n",
      "batch 1129: loss 0.05173089727759361\n",
      "batch 1130: loss 0.001734612975269556\n",
      "batch 1131: loss 0.006481771823018789\n",
      "batch 1132: loss 0.004633923061192036\n",
      "batch 1133: loss 0.01733957789838314\n",
      "batch 1134: loss 0.008325287140905857\n",
      "batch 1135: loss 0.014640356414020061\n",
      "batch 1136: loss 0.012615134008228779\n",
      "batch 1137: loss 0.002654159674420953\n",
      "batch 1138: loss 0.0051767597906291485\n",
      "batch 1139: loss 0.023664331063628197\n",
      "batch 1140: loss 0.01920625939965248\n",
      "batch 1141: loss 0.19491931796073914\n",
      "batch 1142: loss 0.0004894695593975484\n",
      "batch 1143: loss 0.0681227296590805\n",
      "batch 1144: loss 0.026038384065032005\n",
      "batch 1145: loss 0.007094119675457478\n",
      "batch 1146: loss 0.01656792126595974\n",
      "batch 1147: loss 0.014096725732088089\n",
      "batch 1148: loss 0.0027301358059048653\n",
      "batch 1149: loss 0.1177673488855362\n",
      "batch 1150: loss 0.00285259447991848\n",
      "batch 1151: loss 0.005208528134971857\n",
      "batch 1152: loss 0.14195013046264648\n",
      "batch 1153: loss 0.08508986979722977\n",
      "batch 1154: loss 0.0012885404285043478\n",
      "batch 1155: loss 0.00461279321461916\n",
      "batch 1156: loss 0.07138869911432266\n",
      "batch 1157: loss 0.008066002279520035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1158: loss 0.025112565606832504\n",
      "batch 1159: loss 0.05017609894275665\n",
      "batch 1160: loss 0.07896113395690918\n",
      "batch 1161: loss 0.1584085375070572\n",
      "batch 1162: loss 0.020245466381311417\n",
      "batch 1163: loss 0.0003515855933073908\n",
      "batch 1164: loss 0.004511852748692036\n",
      "batch 1165: loss 0.023194869980216026\n",
      "batch 1166: loss 0.030461451038718224\n",
      "batch 1167: loss 0.05689891055226326\n",
      "batch 1168: loss 0.02003289945423603\n",
      "batch 1169: loss 0.014642500318586826\n",
      "batch 1170: loss 0.00561837013810873\n",
      "batch 1171: loss 0.033089980483055115\n",
      "batch 1172: loss 0.0016235869843512774\n",
      "batch 1173: loss 0.0028031044639647007\n",
      "batch 1174: loss 0.07245395332574844\n",
      "batch 1175: loss 0.009386272169649601\n",
      "batch 1176: loss 0.000273523764917627\n",
      "batch 1177: loss 0.00243704067543149\n",
      "batch 1178: loss 0.004215201362967491\n",
      "batch 1179: loss 0.020863303914666176\n",
      "batch 1180: loss 0.017414070665836334\n",
      "batch 1181: loss 0.004895748570561409\n",
      "batch 1182: loss 0.0017412598244845867\n",
      "batch 1183: loss 0.10369858145713806\n",
      "batch 1184: loss 0.023497123271226883\n",
      "batch 1185: loss 0.04376179724931717\n",
      "batch 1186: loss 0.0057293567806482315\n",
      "batch 1187: loss 0.004907610826194286\n",
      "batch 1188: loss 0.0036634206771850586\n",
      "batch 1189: loss 0.0037344293668866158\n",
      "batch 1190: loss 0.014814638532698154\n",
      "batch 1191: loss 0.04437866061925888\n",
      "batch 1192: loss 0.011933309957385063\n",
      "batch 1193: loss 0.007820392958819866\n",
      "batch 1194: loss 0.026974443346261978\n",
      "batch 1195: loss 0.0033679702319204807\n",
      "batch 1196: loss 0.018809599801898003\n",
      "batch 1197: loss 0.01067250594496727\n",
      "batch 1198: loss 0.035795386880636215\n",
      "batch 1199: loss 0.13228961825370789\n",
      "batch 1200: loss 0.0014320481568574905\n",
      "batch 1201: loss 0.012276732362806797\n",
      "batch 1202: loss 0.05389498546719551\n",
      "batch 1203: loss 0.005072443280369043\n",
      "batch 1204: loss 0.06177622824907303\n",
      "batch 1205: loss 0.010808620601892471\n",
      "batch 1206: loss 0.006452355999499559\n",
      "batch 1207: loss 0.06592831760644913\n",
      "batch 1208: loss 0.03483877703547478\n",
      "batch 1209: loss 0.045142803341150284\n",
      "batch 1210: loss 0.001976626692339778\n",
      "batch 1211: loss 0.11607342213392258\n",
      "batch 1212: loss 0.02667962573468685\n",
      "batch 1213: loss 0.013582354411482811\n",
      "batch 1214: loss 0.15359888970851898\n",
      "batch 1215: loss 0.006592950783669949\n",
      "batch 1216: loss 0.011065313592553139\n",
      "batch 1217: loss 0.0528842993080616\n",
      "batch 1218: loss 0.0035393701400607824\n",
      "batch 1219: loss 0.001248962595127523\n",
      "batch 1220: loss 0.0120977982878685\n",
      "batch 1221: loss 0.002467554062604904\n",
      "batch 1222: loss 0.0004953511524945498\n",
      "batch 1223: loss 0.08971469849348068\n",
      "batch 1224: loss 0.13776454329490662\n",
      "batch 1225: loss 0.03430540859699249\n",
      "batch 1226: loss 0.09105072915554047\n",
      "batch 1227: loss 0.005245227832347155\n",
      "batch 1228: loss 0.023181090131402016\n",
      "batch 1229: loss 0.01206799503415823\n",
      "batch 1230: loss 0.0023698047734797\n",
      "batch 1231: loss 0.11417724937200546\n",
      "batch 1232: loss 0.016195181757211685\n",
      "batch 1233: loss 0.17262539267539978\n",
      "batch 1234: loss 0.03968364745378494\n",
      "batch 1235: loss 0.1594703197479248\n",
      "batch 1236: loss 0.04310575872659683\n",
      "batch 1237: loss 0.010025243274867535\n",
      "batch 1238: loss 0.0034595667384564877\n",
      "batch 1239: loss 0.011445015668869019\n",
      "batch 1240: loss 0.038495469838380814\n",
      "batch 1241: loss 0.039867762476205826\n",
      "batch 1242: loss 0.028452254831790924\n",
      "batch 1243: loss 0.003292239736765623\n",
      "batch 1244: loss 0.010253092274069786\n",
      "batch 1245: loss 0.019248977303504944\n",
      "batch 1246: loss 0.0019397870637476444\n",
      "batch 1247: loss 0.018053147941827774\n",
      "batch 1248: loss 0.0047621107660233974\n",
      "batch 1249: loss 0.014391731470823288\n",
      "batch 1250: loss 0.1474848836660385\n",
      "batch 1251: loss 0.0024953274987637997\n",
      "batch 1252: loss 0.03674597665667534\n",
      "batch 1253: loss 0.004843228496611118\n",
      "batch 1254: loss 0.0030700440984219313\n",
      "batch 1255: loss 0.027683019638061523\n",
      "batch 1256: loss 0.003032505512237549\n",
      "batch 1257: loss 0.09634101390838623\n",
      "batch 1258: loss 0.0015967058716341853\n",
      "batch 1259: loss 0.08126360923051834\n",
      "batch 1260: loss 0.007759052328765392\n",
      "batch 1261: loss 0.0421566516160965\n",
      "batch 1262: loss 0.040136732161045074\n",
      "batch 1263: loss 0.03419492766261101\n",
      "batch 1264: loss 0.011091891676187515\n",
      "batch 1265: loss 0.003582387464120984\n",
      "batch 1266: loss 0.02411971241235733\n",
      "batch 1267: loss 0.048352379351854324\n",
      "batch 1268: loss 0.02599109150469303\n",
      "batch 1269: loss 0.0652429535984993\n",
      "batch 1270: loss 0.005907543934881687\n",
      "batch 1271: loss 0.0010452443966642022\n",
      "batch 1272: loss 0.002636985620483756\n",
      "batch 1273: loss 0.16235865652561188\n",
      "batch 1274: loss 0.05712904408574104\n",
      "batch 1275: loss 0.003065630095079541\n",
      "batch 1276: loss 0.023467358201742172\n",
      "batch 1277: loss 0.09690377116203308\n",
      "batch 1278: loss 0.048160772770643234\n",
      "batch 1279: loss 0.003410415491089225\n",
      "batch 1280: loss 0.07888443022966385\n",
      "batch 1281: loss 0.00671016238629818\n",
      "batch 1282: loss 0.01839415170252323\n",
      "batch 1283: loss 0.03827357664704323\n",
      "batch 1284: loss 0.04845114424824715\n",
      "batch 1285: loss 0.0047344849444925785\n",
      "batch 1286: loss 0.052101220935583115\n",
      "batch 1287: loss 0.028651999309659004\n",
      "batch 1288: loss 0.025174517184495926\n",
      "batch 1289: loss 0.035732533782720566\n",
      "batch 1290: loss 0.031516943126916885\n",
      "batch 1291: loss 0.11312207579612732\n",
      "batch 1292: loss 0.06391680985689163\n",
      "batch 1293: loss 0.00421941839158535\n",
      "batch 1294: loss 0.03705667331814766\n",
      "batch 1295: loss 0.30349236726760864\n",
      "batch 1296: loss 0.0018607968231663108\n",
      "batch 1297: loss 0.022302594035863876\n",
      "batch 1298: loss 0.021575458347797394\n",
      "batch 1299: loss 0.003064681775867939\n",
      "batch 1300: loss 0.04250536486506462\n",
      "batch 1301: loss 0.09459991008043289\n",
      "batch 1302: loss 0.03368089720606804\n",
      "batch 1303: loss 0.032895904034376144\n",
      "batch 1304: loss 0.014972878620028496\n",
      "batch 1305: loss 0.04845971614122391\n",
      "batch 1306: loss 0.006290838122367859\n",
      "batch 1307: loss 0.018172679468989372\n",
      "batch 1308: loss 0.021202165633440018\n",
      "batch 1309: loss 0.012261043302714825\n",
      "batch 1310: loss 0.02418038249015808\n",
      "batch 1311: loss 0.0777939110994339\n",
      "batch 1312: loss 0.06469859927892685\n",
      "batch 1313: loss 0.00340639753267169\n",
      "batch 1314: loss 0.0028116682078689337\n",
      "batch 1315: loss 0.02059277705848217\n",
      "batch 1316: loss 0.07093703746795654\n",
      "batch 1317: loss 0.006193419452756643\n",
      "batch 1318: loss 0.015452250838279724\n",
      "batch 1319: loss 0.09184307605028152\n",
      "batch 1320: loss 0.026913220062851906\n",
      "batch 1321: loss 0.0030041488353163004\n",
      "batch 1322: loss 0.14796318113803864\n",
      "batch 1323: loss 0.0090416818857193\n",
      "batch 1324: loss 0.014157122001051903\n",
      "batch 1325: loss 0.02624478004872799\n",
      "batch 1326: loss 0.11275413632392883\n",
      "batch 1327: loss 0.02065078727900982\n",
      "batch 1328: loss 0.0005529109039343894\n",
      "batch 1329: loss 0.022245198488235474\n",
      "batch 1330: loss 0.005687083583325148\n",
      "batch 1331: loss 0.025493400171399117\n",
      "batch 1332: loss 0.01042698323726654\n",
      "batch 1333: loss 0.09128987789154053\n",
      "batch 1334: loss 0.028869502246379852\n",
      "batch 1335: loss 0.0009463879396207631\n",
      "batch 1336: loss 0.003861065488308668\n",
      "batch 1337: loss 0.012582670897245407\n",
      "batch 1338: loss 0.07988066971302032\n",
      "batch 1339: loss 0.003287687199190259\n",
      "batch 1340: loss 0.056134842336177826\n",
      "batch 1341: loss 0.007938607595860958\n",
      "batch 1342: loss 0.03166075795888901\n",
      "batch 1343: loss 0.005047404672950506\n",
      "batch 1344: loss 0.014141584746539593\n",
      "batch 1345: loss 0.0008301926427520812\n",
      "batch 1346: loss 0.01868472807109356\n",
      "batch 1347: loss 0.005799534264951944\n",
      "batch 1348: loss 0.03850579261779785\n",
      "batch 1349: loss 0.08406952023506165\n",
      "batch 1350: loss 0.002341421553865075\n",
      "batch 1351: loss 0.008160591125488281\n",
      "batch 1352: loss 0.09320582449436188\n",
      "batch 1353: loss 0.020329343155026436\n",
      "batch 1354: loss 0.04802793264389038\n",
      "batch 1355: loss 0.10865634679794312\n",
      "batch 1356: loss 0.053209107369184494\n",
      "batch 1357: loss 0.005743818357586861\n",
      "batch 1358: loss 0.05444230139255524\n",
      "batch 1359: loss 0.0008905932190828025\n",
      "batch 1360: loss 0.08067939430475235\n",
      "batch 1361: loss 0.09721178561449051\n",
      "batch 1362: loss 0.01967494748532772\n",
      "batch 1363: loss 0.0065985992550849915\n",
      "batch 1364: loss 0.044952500611543655\n",
      "batch 1365: loss 0.01764560490846634\n",
      "batch 1366: loss 0.08882623165845871\n",
      "batch 1367: loss 0.030490024015307426\n",
      "batch 1368: loss 0.012610260397195816\n",
      "batch 1369: loss 0.027534274384379387\n",
      "batch 1370: loss 0.29294708371162415\n",
      "batch 1371: loss 0.057700518518686295\n",
      "batch 1372: loss 0.044722627848386765\n",
      "batch 1373: loss 0.0024981056340038776\n",
      "batch 1374: loss 0.02815018594264984\n",
      "batch 1375: loss 0.01088281162083149\n",
      "batch 1376: loss 0.02466684579849243\n",
      "batch 1377: loss 0.02925032377243042\n",
      "batch 1378: loss 0.008776923641562462\n",
      "batch 1379: loss 0.04511984810233116\n",
      "batch 1380: loss 0.04535382241010666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1381: loss 0.009895711205899715\n",
      "batch 1382: loss 0.005980462767183781\n",
      "batch 1383: loss 0.02761644870042801\n",
      "batch 1384: loss 0.019085831940174103\n",
      "batch 1385: loss 0.0329710990190506\n",
      "batch 1386: loss 0.01365207601338625\n",
      "batch 1387: loss 0.03089628554880619\n",
      "batch 1388: loss 0.04969345033168793\n",
      "batch 1389: loss 0.04829131066799164\n",
      "batch 1390: loss 0.004832252394407988\n",
      "batch 1391: loss 0.01853273995220661\n",
      "batch 1392: loss 0.0007874285220168531\n",
      "batch 1393: loss 0.13421384990215302\n",
      "batch 1394: loss 0.010093724355101585\n",
      "batch 1395: loss 0.06462178379297256\n",
      "batch 1396: loss 0.0492556169629097\n",
      "batch 1397: loss 0.019113633781671524\n",
      "batch 1398: loss 0.002670940011739731\n",
      "batch 1399: loss 0.02708561345934868\n",
      "batch 1400: loss 0.013067306950688362\n",
      "batch 1401: loss 0.07472925633192062\n",
      "batch 1402: loss 0.03711448237299919\n",
      "batch 1403: loss 0.009688269346952438\n",
      "batch 1404: loss 0.017914989963173866\n",
      "batch 1405: loss 0.05607326701283455\n",
      "batch 1406: loss 0.018218472599983215\n",
      "batch 1407: loss 0.015111095272004604\n",
      "batch 1408: loss 0.01050966139882803\n",
      "batch 1409: loss 0.00609573582187295\n",
      "batch 1410: loss 0.002514129038900137\n",
      "batch 1411: loss 0.03841577470302582\n",
      "batch 1412: loss 0.04440341517329216\n",
      "batch 1413: loss 0.018005182966589928\n",
      "batch 1414: loss 0.0038971714675426483\n",
      "batch 1415: loss 0.04425903037190437\n",
      "batch 1416: loss 0.058544520288705826\n",
      "batch 1417: loss 0.0018549200613051653\n",
      "batch 1418: loss 0.009780489839613438\n",
      "batch 1419: loss 0.010783948935568333\n",
      "batch 1420: loss 0.07703935354948044\n",
      "batch 1421: loss 0.0024221406783908606\n",
      "batch 1422: loss 0.045270636677742004\n",
      "batch 1423: loss 0.006678211502730846\n",
      "batch 1424: loss 0.020016491413116455\n",
      "batch 1425: loss 0.019029445946216583\n",
      "batch 1426: loss 0.06490350514650345\n",
      "batch 1427: loss 0.002759725321084261\n",
      "batch 1428: loss 0.0034607218112796545\n",
      "batch 1429: loss 0.006458061281591654\n",
      "batch 1430: loss 0.00036782227107323706\n",
      "batch 1431: loss 0.1501130610704422\n",
      "batch 1432: loss 0.12900035083293915\n",
      "batch 1433: loss 0.0033118363935500383\n",
      "batch 1434: loss 0.050030775368213654\n",
      "batch 1435: loss 0.01750914938747883\n",
      "batch 1436: loss 0.05063962936401367\n",
      "batch 1437: loss 0.0004045511013828218\n",
      "batch 1438: loss 0.00697164423763752\n",
      "batch 1439: loss 0.05344729870557785\n",
      "batch 1440: loss 0.12466510385274887\n",
      "batch 1441: loss 0.0039222873747348785\n",
      "batch 1442: loss 0.07507363706827164\n",
      "batch 1443: loss 0.012608091346919537\n",
      "batch 1444: loss 0.07316835224628448\n",
      "batch 1445: loss 0.005099995527416468\n",
      "batch 1446: loss 0.0012315311469137669\n",
      "batch 1447: loss 0.018481722101569176\n",
      "batch 1448: loss 0.004958721809089184\n",
      "batch 1449: loss 0.03011348657310009\n",
      "batch 1450: loss 0.022290026769042015\n",
      "batch 1451: loss 0.004364806693047285\n",
      "batch 1452: loss 0.00793539173901081\n",
      "batch 1453: loss 0.08346369862556458\n",
      "batch 1454: loss 0.009417413733899593\n",
      "batch 1455: loss 0.01913568750023842\n",
      "batch 1456: loss 0.006637329701334238\n",
      "batch 1457: loss 0.004319825209677219\n",
      "batch 1458: loss 0.05702757090330124\n",
      "batch 1459: loss 0.0016380654415115714\n",
      "batch 1460: loss 0.021375136449933052\n",
      "batch 1461: loss 0.03012412041425705\n",
      "batch 1462: loss 0.0071850125677883625\n",
      "batch 1463: loss 0.04768386110663414\n",
      "batch 1464: loss 0.013241101987659931\n",
      "batch 1465: loss 0.05446614325046539\n",
      "batch 1466: loss 0.01993013359606266\n",
      "batch 1467: loss 0.004147626459598541\n",
      "batch 1468: loss 0.0075175464153289795\n",
      "batch 1469: loss 0.000854954996611923\n",
      "batch 1470: loss 0.012212498113512993\n",
      "batch 1471: loss 0.05968957766890526\n",
      "batch 1472: loss 0.12775780260562897\n",
      "batch 1473: loss 0.0023898077197372913\n",
      "batch 1474: loss 0.07181520015001297\n",
      "batch 1475: loss 0.024691404774785042\n",
      "batch 1476: loss 0.07062776386737823\n",
      "batch 1477: loss 0.019864486530423164\n",
      "batch 1478: loss 0.06543498486280441\n",
      "batch 1479: loss 0.027600891888141632\n",
      "batch 1480: loss 0.007350200321525335\n",
      "batch 1481: loss 0.1086854413151741\n",
      "batch 1482: loss 0.0012316600186750293\n",
      "batch 1483: loss 0.05522562935948372\n",
      "batch 1484: loss 0.0009751233155839145\n",
      "batch 1485: loss 0.03574662283062935\n",
      "batch 1486: loss 0.024487685412168503\n",
      "batch 1487: loss 0.026291195303201675\n",
      "batch 1488: loss 0.0019663874991238117\n",
      "batch 1489: loss 0.09700033813714981\n",
      "batch 1490: loss 0.008468632586300373\n",
      "batch 1491: loss 0.011461660265922546\n",
      "batch 1492: loss 0.015813473612070084\n",
      "batch 1493: loss 0.018126312643289566\n",
      "batch 1494: loss 0.003158373525366187\n",
      "batch 1495: loss 0.04641523212194443\n",
      "batch 1496: loss 0.012375250458717346\n",
      "batch 1497: loss 0.023417212069034576\n",
      "batch 1498: loss 0.011609065346419811\n",
      "batch 1499: loss 0.04990608990192413\n",
      "batch 1500: loss 0.0018896956462413073\n",
      "batch 1501: loss 0.027738885954022408\n",
      "batch 1502: loss 0.0015038816491141915\n",
      "batch 1503: loss 0.033719561994075775\n",
      "batch 1504: loss 0.04641767218708992\n",
      "batch 1505: loss 0.013995819725096226\n",
      "batch 1506: loss 0.015186931937932968\n",
      "batch 1507: loss 0.0012847197940573096\n",
      "batch 1508: loss 0.002246514894068241\n",
      "batch 1509: loss 0.02234741859138012\n",
      "batch 1510: loss 0.025930391624569893\n",
      "batch 1511: loss 0.06098777800798416\n",
      "batch 1512: loss 0.004445192404091358\n",
      "batch 1513: loss 0.13647855818271637\n",
      "batch 1514: loss 0.10285099595785141\n",
      "batch 1515: loss 0.0444168858230114\n",
      "batch 1516: loss 0.0005191418458707631\n",
      "batch 1517: loss 0.16213127970695496\n",
      "batch 1518: loss 0.00716812489554286\n",
      "batch 1519: loss 0.06249973550438881\n",
      "batch 1520: loss 0.016252286732196808\n",
      "batch 1521: loss 0.07001468539237976\n",
      "batch 1522: loss 0.011324487626552582\n",
      "batch 1523: loss 0.03590206056833267\n",
      "batch 1524: loss 0.010349497199058533\n",
      "batch 1525: loss 0.21988177299499512\n",
      "batch 1526: loss 0.01785852201282978\n",
      "batch 1527: loss 0.046744465827941895\n",
      "batch 1528: loss 0.004240652546286583\n",
      "batch 1529: loss 0.0027038264088332653\n",
      "batch 1530: loss 0.016927193850278854\n",
      "batch 1531: loss 0.011870226822793484\n",
      "batch 1532: loss 0.00912828091531992\n",
      "batch 1533: loss 0.009380885399878025\n",
      "batch 1534: loss 0.03464720770716667\n",
      "batch 1535: loss 0.0087263910099864\n",
      "batch 1536: loss 0.0004583412955980748\n",
      "batch 1537: loss 0.04304959625005722\n",
      "batch 1538: loss 0.012430362403392792\n",
      "batch 1539: loss 0.005103964824229479\n",
      "batch 1540: loss 0.005216197110712528\n",
      "batch 1541: loss 0.0030676876194775105\n",
      "batch 1542: loss 0.005824862979352474\n",
      "batch 1543: loss 0.005112435668706894\n",
      "batch 1544: loss 0.0037280647084116936\n",
      "batch 1545: loss 0.01667502522468567\n",
      "batch 1546: loss 0.0062148370780050755\n",
      "batch 1547: loss 0.008705160580575466\n",
      "batch 1548: loss 0.012831264175474644\n",
      "batch 1549: loss 0.029852066189050674\n",
      "batch 1550: loss 0.1272268146276474\n",
      "batch 1551: loss 0.021665651351213455\n",
      "batch 1552: loss 0.014219739474356174\n",
      "batch 1553: loss 0.02180057018995285\n",
      "batch 1554: loss 0.028057832270860672\n",
      "batch 1555: loss 0.00274749961681664\n",
      "batch 1556: loss 0.006101839244365692\n",
      "batch 1557: loss 0.010040795430541039\n",
      "batch 1558: loss 0.001970218261703849\n",
      "batch 1559: loss 0.01931924559175968\n",
      "batch 1560: loss 0.005874716676771641\n",
      "batch 1561: loss 0.027592560276389122\n",
      "batch 1562: loss 0.012358701787889004\n",
      "batch 1563: loss 0.005583989433944225\n",
      "batch 1564: loss 0.011991399340331554\n",
      "batch 1565: loss 0.0006837978144176304\n",
      "batch 1566: loss 0.03156236931681633\n",
      "batch 1567: loss 0.017122169956564903\n",
      "batch 1568: loss 0.005363130010664463\n",
      "batch 1569: loss 0.02901044860482216\n",
      "batch 1570: loss 0.03563995659351349\n",
      "batch 1571: loss 0.0338493250310421\n",
      "batch 1572: loss 0.003166094422340393\n",
      "batch 1573: loss 0.0029536341316998005\n",
      "batch 1574: loss 0.003934007603675127\n",
      "batch 1575: loss 0.02625894546508789\n",
      "batch 1576: loss 0.008572814054787159\n",
      "batch 1577: loss 0.019933264702558517\n",
      "batch 1578: loss 6.573183054570109e-05\n",
      "batch 1579: loss 0.02291504107415676\n",
      "batch 1580: loss 0.0065778219141066074\n",
      "batch 1581: loss 0.0006942884647287428\n",
      "batch 1582: loss 0.0008163356687873602\n",
      "batch 1583: loss 0.07575979083776474\n",
      "batch 1584: loss 0.013078754767775536\n",
      "batch 1585: loss 0.0011346859391778708\n",
      "batch 1586: loss 0.0004117514763493091\n",
      "batch 1587: loss 0.004770970903337002\n",
      "batch 1588: loss 0.056594375520944595\n",
      "batch 1589: loss 0.06225375086069107\n",
      "batch 1590: loss 0.05370014160871506\n",
      "batch 1591: loss 0.11772537231445312\n",
      "batch 1592: loss 0.12479140609502792\n",
      "batch 1593: loss 0.04355335608124733\n",
      "batch 1594: loss 0.0077103921212255955\n",
      "batch 1595: loss 0.0017263964982703328\n",
      "batch 1596: loss 0.03099936805665493\n",
      "batch 1597: loss 0.059244394302368164\n",
      "batch 1598: loss 0.2749711275100708\n",
      "batch 1599: loss 0.014899822883307934\n",
      "batch 1600: loss 0.0702984407544136\n",
      "batch 1601: loss 0.05879727751016617\n",
      "batch 1602: loss 0.02373824082314968\n",
      "batch 1603: loss 0.008388600312173367\n",
      "batch 1604: loss 0.006208229344338179\n",
      "batch 1605: loss 0.11496123671531677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1606: loss 0.00043884574552066624\n",
      "batch 1607: loss 0.012419715523719788\n",
      "batch 1608: loss 0.14050684869289398\n",
      "batch 1609: loss 0.009429753758013248\n",
      "batch 1610: loss 0.05798914656043053\n",
      "batch 1611: loss 0.005086846649646759\n",
      "batch 1612: loss 0.0030295029282569885\n",
      "batch 1613: loss 0.004316485021263361\n",
      "batch 1614: loss 0.03270235285162926\n",
      "batch 1615: loss 0.014927981421351433\n",
      "batch 1616: loss 0.015871070325374603\n",
      "batch 1617: loss 0.0095879677683115\n",
      "batch 1618: loss 0.01480618305504322\n",
      "batch 1619: loss 0.006107601802796125\n",
      "batch 1620: loss 0.11731933057308197\n",
      "batch 1621: loss 0.020418507978320122\n",
      "batch 1622: loss 0.0053797150030732155\n",
      "batch 1623: loss 0.07928858697414398\n",
      "batch 1624: loss 0.030386362224817276\n",
      "batch 1625: loss 0.19011592864990234\n",
      "batch 1626: loss 0.01767837069928646\n",
      "batch 1627: loss 0.13809135556221008\n",
      "batch 1628: loss 0.005682392977178097\n",
      "batch 1629: loss 0.021967198699712753\n",
      "batch 1630: loss 0.00405542179942131\n",
      "batch 1631: loss 0.12180246412754059\n",
      "batch 1632: loss 0.018563861027359962\n",
      "batch 1633: loss 0.05823291093111038\n",
      "batch 1634: loss 0.009197833016514778\n",
      "batch 1635: loss 0.012368244118988514\n",
      "batch 1636: loss 0.014933392405509949\n",
      "batch 1637: loss 0.006186147686094046\n",
      "batch 1638: loss 0.13074640929698944\n",
      "batch 1639: loss 0.006928536109626293\n",
      "batch 1640: loss 0.042888324707746506\n",
      "batch 1641: loss 0.0030919210985302925\n",
      "batch 1642: loss 0.0018305641133338213\n",
      "batch 1643: loss 0.0107715530321002\n",
      "batch 1644: loss 0.006382117047905922\n",
      "batch 1645: loss 0.0032098351512104273\n",
      "batch 1646: loss 0.001765068736858666\n",
      "batch 1647: loss 0.08036373555660248\n",
      "batch 1648: loss 0.010017982684075832\n",
      "batch 1649: loss 0.003559254575520754\n",
      "batch 1650: loss 0.0011229641968384385\n",
      "batch 1651: loss 0.042055949568748474\n",
      "batch 1652: loss 0.00962857250124216\n",
      "batch 1653: loss 0.03561927378177643\n",
      "batch 1654: loss 0.009260935708880424\n",
      "batch 1655: loss 0.011385295540094376\n",
      "batch 1656: loss 0.015358607284724712\n",
      "batch 1657: loss 0.20324155688285828\n",
      "batch 1658: loss 0.0063897850923240185\n",
      "batch 1659: loss 0.040034059435129166\n",
      "batch 1660: loss 0.023951932787895203\n",
      "batch 1661: loss 0.0003728032170329243\n",
      "batch 1662: loss 0.0027720157522708178\n",
      "batch 1663: loss 0.004737770650535822\n",
      "batch 1664: loss 0.013116544112563133\n",
      "batch 1665: loss 0.12166859954595566\n",
      "batch 1666: loss 0.0005305380909703672\n",
      "batch 1667: loss 0.05105091631412506\n",
      "batch 1668: loss 0.0021416230592876673\n",
      "batch 1669: loss 0.0010974250035360456\n",
      "batch 1670: loss 0.033110082149505615\n",
      "batch 1671: loss 0.01818149723112583\n",
      "batch 1672: loss 0.10022957623004913\n",
      "batch 1673: loss 0.0033282169606536627\n",
      "batch 1674: loss 0.022793935611844063\n",
      "batch 1675: loss 0.016300298273563385\n",
      "batch 1676: loss 0.04058077186346054\n",
      "batch 1677: loss 0.07732626795768738\n",
      "batch 1678: loss 0.0016824488993734121\n",
      "batch 1679: loss 0.004094897303730249\n",
      "batch 1680: loss 0.008929186500608921\n",
      "batch 1681: loss 0.005273629445582628\n",
      "batch 1682: loss 0.05161964148283005\n",
      "batch 1683: loss 0.15214599668979645\n",
      "batch 1684: loss 0.03959023207426071\n",
      "batch 1685: loss 0.01910085789859295\n",
      "batch 1686: loss 0.001958962297067046\n",
      "batch 1687: loss 0.13775768876075745\n",
      "batch 1688: loss 0.005171375814825296\n",
      "batch 1689: loss 0.00030968908686190844\n",
      "batch 1690: loss 0.013553999364376068\n",
      "batch 1691: loss 0.02221742644906044\n",
      "batch 1692: loss 0.027854280546307564\n",
      "batch 1693: loss 0.030154451727867126\n",
      "batch 1694: loss 0.023927897214889526\n",
      "batch 1695: loss 0.008875071071088314\n",
      "batch 1696: loss 0.025003693997859955\n",
      "batch 1697: loss 0.014737640507519245\n",
      "batch 1698: loss 0.008119053207337856\n",
      "batch 1699: loss 0.003329911269247532\n",
      "batch 1700: loss 0.0043194168247282505\n",
      "batch 1701: loss 0.02256275899708271\n",
      "batch 1702: loss 0.029924361035227776\n",
      "batch 1703: loss 0.06334506720304489\n",
      "batch 1704: loss 0.091899573802948\n",
      "batch 1705: loss 0.04623723402619362\n",
      "batch 1706: loss 0.0007788647781126201\n",
      "batch 1707: loss 0.03718797490000725\n",
      "batch 1708: loss 0.0011925200233235955\n",
      "batch 1709: loss 0.0009012838127091527\n",
      "batch 1710: loss 0.0033664333168417215\n",
      "batch 1711: loss 0.05777139589190483\n",
      "batch 1712: loss 0.0036325668916106224\n",
      "batch 1713: loss 0.002284114481881261\n",
      "batch 1714: loss 0.022942716255784035\n",
      "batch 1715: loss 0.03654548153281212\n",
      "batch 1716: loss 0.07974764704704285\n",
      "batch 1717: loss 0.04239489883184433\n",
      "batch 1718: loss 0.0024728241842240095\n",
      "batch 1719: loss 0.015242472290992737\n",
      "batch 1720: loss 0.013190876692533493\n",
      "batch 1721: loss 0.020982300862669945\n",
      "batch 1722: loss 0.026650402694940567\n",
      "batch 1723: loss 0.11424890160560608\n",
      "batch 1724: loss 0.0050727203488349915\n",
      "batch 1725: loss 0.04138636961579323\n",
      "batch 1726: loss 0.24013371765613556\n",
      "batch 1727: loss 0.0013338529970496893\n",
      "batch 1728: loss 0.0041259475983679295\n",
      "batch 1729: loss 0.004210269544273615\n",
      "batch 1730: loss 0.015446078032255173\n",
      "batch 1731: loss 0.006239906419068575\n",
      "batch 1732: loss 0.07537644356489182\n",
      "batch 1733: loss 0.002602317836135626\n",
      "batch 1734: loss 0.12569372355937958\n",
      "batch 1735: loss 0.04160545393824577\n",
      "batch 1736: loss 0.0015707206912338734\n",
      "batch 1737: loss 0.014676462858915329\n",
      "batch 1738: loss 0.06844573467969894\n",
      "batch 1739: loss 0.007400562986731529\n",
      "batch 1740: loss 0.020736761391162872\n",
      "batch 1741: loss 0.014484759420156479\n",
      "batch 1742: loss 0.05929451435804367\n",
      "batch 1743: loss 0.003206983208656311\n",
      "batch 1744: loss 0.018094850704073906\n",
      "batch 1745: loss 0.006576404441148043\n",
      "batch 1746: loss 0.07412654906511307\n",
      "batch 1747: loss 0.010445875115692616\n",
      "batch 1748: loss 0.0010364839108660817\n",
      "batch 1749: loss 0.005261515267193317\n",
      "batch 1750: loss 0.0028579889331012964\n",
      "batch 1751: loss 0.012030345387756824\n",
      "batch 1752: loss 0.00284085632301867\n",
      "batch 1753: loss 0.013423019088804722\n",
      "batch 1754: loss 0.033250752836465836\n",
      "batch 1755: loss 0.001374209183268249\n",
      "batch 1756: loss 0.0005462972330860794\n",
      "batch 1757: loss 0.004746372811496258\n",
      "batch 1758: loss 0.004783350042998791\n",
      "batch 1759: loss 0.024601511657238007\n",
      "batch 1760: loss 0.059904951602220535\n",
      "batch 1761: loss 0.0017949756002053618\n",
      "batch 1762: loss 0.0033691911958158016\n",
      "batch 1763: loss 0.008503036573529243\n",
      "batch 1764: loss 0.0069023785181343555\n",
      "batch 1765: loss 0.00915964599698782\n",
      "batch 1766: loss 0.005997147411108017\n",
      "batch 1767: loss 0.002332362113520503\n",
      "batch 1768: loss 0.025373931974172592\n",
      "batch 1769: loss 0.0004586057330016047\n",
      "batch 1770: loss 0.21435090899467468\n",
      "batch 1771: loss 0.0006828921614214778\n",
      "batch 1772: loss 0.0036976011469960213\n",
      "batch 1773: loss 0.006115613970905542\n",
      "batch 1774: loss 0.0026471710298210382\n",
      "batch 1775: loss 0.0005388027057051659\n",
      "batch 1776: loss 0.005476736463606358\n",
      "batch 1777: loss 0.017293890938162804\n",
      "batch 1778: loss 0.09087084233760834\n",
      "batch 1779: loss 0.0022275412920862436\n",
      "batch 1780: loss 0.004477093927562237\n",
      "batch 1781: loss 0.0365896038711071\n",
      "batch 1782: loss 0.001760310959070921\n",
      "batch 1783: loss 0.0016334744868800044\n",
      "batch 1784: loss 0.028537435457110405\n",
      "batch 1785: loss 0.006422410253435373\n",
      "batch 1786: loss 0.0212520994246006\n",
      "batch 1787: loss 0.08165930211544037\n",
      "batch 1788: loss 0.14661389589309692\n",
      "batch 1789: loss 0.02526588924229145\n",
      "batch 1790: loss 0.006154697388410568\n",
      "batch 1791: loss 0.047025587409734726\n",
      "batch 1792: loss 0.02446100115776062\n",
      "batch 1793: loss 0.014778024516999722\n",
      "batch 1794: loss 0.00027651991695165634\n",
      "batch 1795: loss 0.003896657144650817\n",
      "batch 1796: loss 0.007677979301661253\n",
      "batch 1797: loss 0.00023149170738179237\n",
      "batch 1798: loss 0.013336539268493652\n",
      "batch 1799: loss 0.0013128947466611862\n",
      "batch 1800: loss 0.14954528212547302\n",
      "batch 1801: loss 0.0044996365904808044\n",
      "batch 1802: loss 0.02765856869518757\n",
      "batch 1803: loss 0.10897498577833176\n",
      "batch 1804: loss 0.009267857298254967\n",
      "batch 1805: loss 0.0018091911915689707\n",
      "batch 1806: loss 0.022960027679800987\n",
      "batch 1807: loss 0.11481215059757233\n",
      "batch 1808: loss 0.02436552383005619\n",
      "batch 1809: loss 0.0023072324693202972\n",
      "batch 1810: loss 0.007146609481424093\n",
      "batch 1811: loss 0.008152270689606667\n",
      "batch 1812: loss 0.0012970103416591883\n",
      "batch 1813: loss 0.04347033426165581\n",
      "batch 1814: loss 0.04971360042691231\n",
      "batch 1815: loss 0.0936606377363205\n",
      "batch 1816: loss 0.007767563685774803\n",
      "batch 1817: loss 0.004708560649305582\n",
      "batch 1818: loss 0.003664529649540782\n",
      "batch 1819: loss 0.09477489441633224\n",
      "batch 1820: loss 0.019371557980775833\n",
      "batch 1821: loss 0.04823436588048935\n",
      "batch 1822: loss 0.14723485708236694\n",
      "batch 1823: loss 0.1283162534236908\n",
      "batch 1824: loss 0.009151965379714966\n",
      "batch 1825: loss 0.000873844139277935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1826: loss 0.0021109841763973236\n",
      "batch 1827: loss 0.010376611724495888\n",
      "batch 1828: loss 0.029502110555768013\n",
      "batch 1829: loss 0.13180626928806305\n",
      "batch 1830: loss 0.0028047056403011084\n",
      "batch 1831: loss 0.00869752001017332\n",
      "batch 1832: loss 0.01653839834034443\n",
      "batch 1833: loss 0.0032636879477649927\n",
      "batch 1834: loss 0.0783047005534172\n",
      "batch 1835: loss 0.006963638123124838\n",
      "batch 1836: loss 0.005196337588131428\n",
      "batch 1837: loss 0.03343663737177849\n",
      "batch 1838: loss 0.005910393316298723\n",
      "batch 1839: loss 0.0167632307857275\n",
      "batch 1840: loss 0.009239349514245987\n",
      "batch 1841: loss 0.009650018997490406\n",
      "batch 1842: loss 0.008924482390284538\n",
      "batch 1843: loss 0.0014621155569329858\n",
      "batch 1844: loss 0.006597755942493677\n",
      "batch 1845: loss 0.0024222994688898325\n",
      "batch 1846: loss 0.008326181210577488\n",
      "batch 1847: loss 0.01933581754565239\n",
      "batch 1848: loss 0.002015682402998209\n",
      "batch 1849: loss 0.15746274590492249\n",
      "batch 1850: loss 0.008108681067824364\n",
      "batch 1851: loss 0.013697938993573189\n",
      "batch 1852: loss 0.00046070286771282554\n",
      "batch 1853: loss 0.025271674618124962\n",
      "batch 1854: loss 0.00750352768227458\n",
      "batch 1855: loss 0.002809045137837529\n",
      "batch 1856: loss 0.13621050119400024\n",
      "batch 1857: loss 0.0011917665833607316\n",
      "batch 1858: loss 0.013501104898750782\n",
      "batch 1859: loss 0.020672466605901718\n",
      "batch 1860: loss 0.011386562138795853\n",
      "batch 1861: loss 0.07248716056346893\n",
      "batch 1862: loss 0.10170243680477142\n",
      "batch 1863: loss 0.025700774043798447\n",
      "batch 1864: loss 0.09237541258335114\n",
      "batch 1865: loss 0.03942034766077995\n",
      "batch 1866: loss 0.04122456535696983\n",
      "batch 1867: loss 0.0011441552778705955\n",
      "batch 1868: loss 0.06279533356428146\n",
      "batch 1869: loss 0.02834603562951088\n",
      "batch 1870: loss 0.002975025214254856\n",
      "batch 1871: loss 0.024509131908416748\n",
      "batch 1872: loss 0.011286136694252491\n",
      "batch 1873: loss 0.046075738966464996\n",
      "batch 1874: loss 0.00688931904733181\n",
      "batch 1875: loss 0.013901044614613056\n",
      "batch 1876: loss 0.06619296967983246\n",
      "batch 1877: loss 0.003568054409697652\n",
      "batch 1878: loss 0.010328950360417366\n",
      "batch 1879: loss 0.025854945182800293\n",
      "batch 1880: loss 0.029929477721452713\n",
      "batch 1881: loss 0.015311241149902344\n",
      "batch 1882: loss 0.08251361548900604\n",
      "batch 1883: loss 0.010570577345788479\n",
      "batch 1884: loss 0.0075294869020581245\n",
      "batch 1885: loss 0.0032873188611119986\n",
      "batch 1886: loss 0.05124727264046669\n",
      "batch 1887: loss 0.03764214739203453\n",
      "batch 1888: loss 0.0024683803785592318\n",
      "batch 1889: loss 0.0179213285446167\n",
      "batch 1890: loss 0.01190868392586708\n",
      "batch 1891: loss 0.029194749891757965\n",
      "batch 1892: loss 0.13285313546657562\n",
      "batch 1893: loss 0.0037669455632567406\n",
      "batch 1894: loss 0.015764938667416573\n",
      "batch 1895: loss 0.06672121584415436\n",
      "batch 1896: loss 0.001438405830413103\n",
      "batch 1897: loss 0.0078122285194695\n",
      "batch 1898: loss 0.023033546283841133\n",
      "batch 1899: loss 0.01815124787390232\n",
      "batch 1900: loss 0.00913560576736927\n",
      "batch 1901: loss 0.02195138670504093\n",
      "batch 1902: loss 0.039595868438482285\n",
      "batch 1903: loss 0.005190096329897642\n",
      "batch 1904: loss 0.0022305359598249197\n",
      "batch 1905: loss 0.006117450073361397\n",
      "batch 1906: loss 0.0046423692256212234\n",
      "batch 1907: loss 0.07186660915613174\n",
      "batch 1908: loss 0.0615646056830883\n",
      "batch 1909: loss 0.0005445834249258041\n",
      "batch 1910: loss 0.03608887642621994\n",
      "batch 1911: loss 0.0012341553810983896\n",
      "batch 1912: loss 0.012127665802836418\n",
      "batch 1913: loss 0.0008025596616789699\n",
      "batch 1914: loss 0.044473111629486084\n",
      "batch 1915: loss 0.19070900976657867\n",
      "batch 1916: loss 0.058059677481651306\n",
      "batch 1917: loss 0.06016407534480095\n",
      "batch 1918: loss 0.17031890153884888\n",
      "batch 1919: loss 0.003145465161651373\n",
      "batch 1920: loss 0.04602023586630821\n",
      "batch 1921: loss 0.005300117656588554\n",
      "batch 1922: loss 0.10039707273244858\n",
      "batch 1923: loss 0.003891696222126484\n",
      "batch 1924: loss 0.004771492909640074\n",
      "batch 1925: loss 0.0005076860543340445\n",
      "batch 1926: loss 0.005044794175773859\n",
      "batch 1927: loss 0.05246293917298317\n",
      "batch 1928: loss 0.15327641367912292\n",
      "batch 1929: loss 0.008233756758272648\n",
      "batch 1930: loss 0.024610433727502823\n",
      "batch 1931: loss 0.015634143725037575\n",
      "batch 1932: loss 0.1446300595998764\n",
      "batch 1933: loss 0.006522627547383308\n",
      "batch 1934: loss 0.035831380635499954\n",
      "batch 1935: loss 0.105120949447155\n",
      "batch 1936: loss 0.007750518154352903\n",
      "batch 1937: loss 0.0036524173337966204\n",
      "batch 1938: loss 0.057262811809778214\n",
      "batch 1939: loss 0.021676253527402878\n",
      "batch 1940: loss 0.006842397153377533\n",
      "batch 1941: loss 0.056021399796009064\n",
      "batch 1942: loss 0.0030391672626137733\n",
      "batch 1943: loss 0.024132849648594856\n",
      "batch 1944: loss 0.07023981213569641\n",
      "batch 1945: loss 0.04427773505449295\n",
      "batch 1946: loss 0.03166322037577629\n",
      "batch 1947: loss 0.008246640674769878\n",
      "batch 1948: loss 0.009469715878367424\n",
      "batch 1949: loss 0.0022740685380995274\n",
      "batch 1950: loss 0.02540643885731697\n",
      "batch 1951: loss 0.08080877363681793\n",
      "batch 1952: loss 0.03282734006643295\n",
      "batch 1953: loss 0.0005885325372219086\n",
      "batch 1954: loss 0.005594345275312662\n",
      "batch 1955: loss 0.12337516248226166\n",
      "batch 1956: loss 0.003051369683817029\n",
      "batch 1957: loss 0.02133675292134285\n",
      "batch 1958: loss 0.040755171328783035\n",
      "batch 1959: loss 0.2401256114244461\n",
      "batch 1960: loss 0.053260743618011475\n",
      "batch 1961: loss 0.08630367368459702\n",
      "batch 1962: loss 0.01641263999044895\n",
      "batch 1963: loss 0.002041168510913849\n",
      "batch 1964: loss 0.05966951325535774\n",
      "batch 1965: loss 0.011370613239705563\n",
      "batch 1966: loss 0.013551097363233566\n",
      "batch 1967: loss 0.003803723957389593\n",
      "batch 1968: loss 0.008935253135859966\n",
      "batch 1969: loss 0.03178670257329941\n",
      "batch 1970: loss 0.08084729313850403\n",
      "batch 1971: loss 0.0042312429286539555\n",
      "batch 1972: loss 0.001490457565523684\n",
      "batch 1973: loss 0.0032796126324683428\n",
      "batch 1974: loss 0.02827964536845684\n",
      "batch 1975: loss 0.03648369386792183\n",
      "batch 1976: loss 0.00810003001242876\n",
      "batch 1977: loss 0.0020822721999138594\n",
      "batch 1978: loss 0.004425953142344952\n",
      "batch 1979: loss 0.0024214780423790216\n",
      "batch 1980: loss 0.015583392232656479\n",
      "batch 1981: loss 0.011414576321840286\n",
      "batch 1982: loss 0.006678026635199785\n",
      "batch 1983: loss 0.011800219304859638\n",
      "batch 1984: loss 0.006209646351635456\n",
      "batch 1985: loss 0.0031605649273842573\n",
      "batch 1986: loss 0.019231362268328667\n",
      "batch 1987: loss 0.0012176757445558906\n",
      "batch 1988: loss 0.0016994408797472715\n",
      "batch 1989: loss 0.007070648018270731\n",
      "batch 1990: loss 0.008927032351493835\n",
      "batch 1991: loss 0.011349399574100971\n",
      "batch 1992: loss 0.01252410002052784\n",
      "batch 1993: loss 0.06181967630982399\n",
      "batch 1994: loss 0.003880266798660159\n",
      "batch 1995: loss 0.04229022562503815\n",
      "batch 1996: loss 0.0015337916556745768\n",
      "batch 1997: loss 0.003745286725461483\n",
      "batch 1998: loss 0.0008755315793678164\n",
      "batch 1999: loss 0.0023364487569779158\n",
      "batch 2000: loss 0.03985045477747917\n",
      "batch 2001: loss 0.0007775129051879048\n",
      "batch 2002: loss 0.027838248759508133\n",
      "batch 2003: loss 0.0044913263991475105\n",
      "batch 2004: loss 0.10743029415607452\n",
      "batch 2005: loss 0.09966311603784561\n",
      "batch 2006: loss 0.08843342214822769\n",
      "batch 2007: loss 0.0005632197135128081\n",
      "batch 2008: loss 0.009884484112262726\n",
      "batch 2009: loss 0.10663224756717682\n",
      "batch 2010: loss 0.019712435081601143\n",
      "batch 2011: loss 0.003959164954721928\n",
      "batch 2012: loss 0.02261868678033352\n",
      "batch 2013: loss 0.03570651635527611\n",
      "batch 2014: loss 0.011546170338988304\n",
      "batch 2015: loss 0.09920606762170792\n",
      "batch 2016: loss 0.009625877253711224\n",
      "batch 2017: loss 0.02158120833337307\n",
      "batch 2018: loss 0.015512101352214813\n",
      "batch 2019: loss 0.002412577159702778\n",
      "batch 2020: loss 0.010269527323544025\n",
      "batch 2021: loss 0.017380366101861\n",
      "batch 2022: loss 0.08977574110031128\n",
      "batch 2023: loss 0.003470572642982006\n",
      "batch 2024: loss 0.12403473258018494\n",
      "batch 2025: loss 0.030939621850848198\n",
      "batch 2026: loss 0.0017168840859085321\n",
      "batch 2027: loss 0.0774722695350647\n",
      "batch 2028: loss 0.11426190286874771\n",
      "batch 2029: loss 0.04731447622179985\n",
      "batch 2030: loss 0.019891584292054176\n",
      "batch 2031: loss 0.03424742445349693\n",
      "batch 2032: loss 0.019945263862609863\n",
      "batch 2033: loss 0.03727862611413002\n",
      "batch 2034: loss 0.01962374895811081\n",
      "batch 2035: loss 0.047397222369909286\n",
      "batch 2036: loss 0.07271784543991089\n",
      "batch 2037: loss 0.06093278154730797\n",
      "batch 2038: loss 0.03152986615896225\n",
      "batch 2039: loss 0.02326733060181141\n",
      "batch 2040: loss 0.07557209581136703\n",
      "batch 2041: loss 0.05525273084640503\n",
      "batch 2042: loss 0.004627535585314035\n",
      "batch 2043: loss 0.004559006076306105\n",
      "batch 2044: loss 0.02292516827583313\n",
      "batch 2045: loss 0.0017708989325910807\n",
      "batch 2046: loss 0.1862170249223709\n",
      "batch 2047: loss 0.1429612785577774\n",
      "batch 2048: loss 0.014504484832286835\n",
      "batch 2049: loss 0.009982213377952576\n",
      "batch 2050: loss 0.03637881204485893\n",
      "batch 2051: loss 0.01702033169567585\n",
      "batch 2052: loss 0.028458088636398315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2053: loss 0.022838424891233444\n",
      "batch 2054: loss 0.02627965435385704\n",
      "batch 2055: loss 0.021937936544418335\n",
      "batch 2056: loss 0.0025188454892486334\n",
      "batch 2057: loss 0.01892087422311306\n",
      "batch 2058: loss 0.07251035422086716\n",
      "batch 2059: loss 0.019920503720641136\n",
      "batch 2060: loss 0.029184667393565178\n",
      "batch 2061: loss 0.008413152769207954\n",
      "batch 2062: loss 0.008840045891702175\n",
      "batch 2063: loss 0.007218245882540941\n",
      "batch 2064: loss 0.013470515608787537\n",
      "batch 2065: loss 0.048603422939777374\n",
      "batch 2066: loss 0.0008677875157445669\n",
      "batch 2067: loss 0.0035101573448628187\n",
      "batch 2068: loss 0.007494563702493906\n",
      "batch 2069: loss 0.003996262326836586\n",
      "batch 2070: loss 0.011094899848103523\n",
      "batch 2071: loss 0.031189756467938423\n",
      "batch 2072: loss 0.027334176003932953\n",
      "batch 2073: loss 0.001717303995974362\n",
      "batch 2074: loss 0.12078548222780228\n",
      "batch 2075: loss 0.004916015546768904\n",
      "batch 2076: loss 0.01688205637037754\n",
      "batch 2077: loss 0.00797176267951727\n",
      "batch 2078: loss 0.007826684042811394\n",
      "batch 2079: loss 0.004574338905513287\n",
      "batch 2080: loss 0.18255014717578888\n",
      "batch 2081: loss 0.008901766501367092\n",
      "batch 2082: loss 0.00788822490721941\n",
      "batch 2083: loss 0.006841366644948721\n",
      "batch 2084: loss 0.006005856674164534\n",
      "batch 2085: loss 0.009176489897072315\n",
      "batch 2086: loss 0.07978091388940811\n",
      "batch 2087: loss 0.0010312733938917518\n",
      "batch 2088: loss 0.061735913157463074\n",
      "batch 2089: loss 0.011132912710309029\n",
      "batch 2090: loss 0.021816572174429893\n",
      "batch 2091: loss 0.0893658697605133\n",
      "batch 2092: loss 0.014883405528962612\n",
      "batch 2093: loss 0.013326078653335571\n",
      "batch 2094: loss 0.10789171606302261\n",
      "batch 2095: loss 0.0008216685382649302\n",
      "batch 2096: loss 0.003468624781817198\n",
      "batch 2097: loss 0.10128386318683624\n",
      "batch 2098: loss 0.021079640835523605\n",
      "batch 2099: loss 0.0757746770977974\n",
      "batch 2100: loss 0.019564935937523842\n",
      "batch 2101: loss 0.10054707527160645\n",
      "batch 2102: loss 0.018095700070261955\n",
      "batch 2103: loss 0.006549043580889702\n",
      "batch 2104: loss 0.0170414000749588\n",
      "batch 2105: loss 0.05953417718410492\n",
      "batch 2106: loss 0.006291538942605257\n",
      "batch 2107: loss 0.020421268418431282\n",
      "batch 2108: loss 0.005147082731127739\n",
      "batch 2109: loss 0.016203144565224648\n",
      "batch 2110: loss 0.007549699395895004\n",
      "batch 2111: loss 0.05374130234122276\n",
      "batch 2112: loss 0.0021376602817326784\n",
      "batch 2113: loss 0.022662004455924034\n",
      "batch 2114: loss 0.0770375058054924\n",
      "batch 2115: loss 0.005955914035439491\n",
      "batch 2116: loss 0.004782497882843018\n",
      "batch 2117: loss 0.004618249833583832\n",
      "batch 2118: loss 0.004500104580074549\n",
      "batch 2119: loss 0.07602180540561676\n",
      "batch 2120: loss 0.0005548179615288973\n",
      "batch 2121: loss 0.0036080421414226294\n",
      "batch 2122: loss 0.031220005825161934\n",
      "batch 2123: loss 0.00024819117970764637\n",
      "batch 2124: loss 0.004064487759023905\n",
      "batch 2125: loss 0.010840165428817272\n",
      "batch 2126: loss 0.03003489412367344\n",
      "batch 2127: loss 0.11429528146982193\n",
      "batch 2128: loss 0.14773370325565338\n",
      "batch 2129: loss 0.05660409852862358\n",
      "batch 2130: loss 0.0005434663034975529\n",
      "batch 2131: loss 0.0012482916936278343\n",
      "batch 2132: loss 0.005256223026663065\n",
      "batch 2133: loss 0.02210593968629837\n",
      "batch 2134: loss 0.027578052133321762\n",
      "batch 2135: loss 0.0054358928464353085\n",
      "batch 2136: loss 0.006305869668722153\n",
      "batch 2137: loss 0.004662571009248495\n",
      "batch 2138: loss 0.02528204582631588\n",
      "batch 2139: loss 0.0010219609830528498\n",
      "batch 2140: loss 0.0020625400356948376\n",
      "batch 2141: loss 0.055106375366449356\n",
      "batch 2142: loss 0.024120450019836426\n",
      "batch 2143: loss 0.004845349118113518\n",
      "batch 2144: loss 0.009053434245288372\n",
      "batch 2145: loss 0.024880042299628258\n",
      "batch 2146: loss 0.056078433990478516\n",
      "batch 2147: loss 0.0018054067622870207\n",
      "batch 2148: loss 0.065982386469841\n",
      "batch 2149: loss 0.0022334312088787556\n",
      "batch 2150: loss 0.023452838882803917\n",
      "batch 2151: loss 0.017877493053674698\n",
      "batch 2152: loss 0.014772445894777775\n",
      "batch 2153: loss 0.01829027570784092\n",
      "batch 2154: loss 0.019558453932404518\n",
      "batch 2155: loss 0.0028877502772957087\n",
      "batch 2156: loss 0.016753429546952248\n",
      "batch 2157: loss 0.009519074112176895\n",
      "batch 2158: loss 0.013638441450893879\n",
      "batch 2159: loss 0.12125604599714279\n",
      "batch 2160: loss 0.0015122368931770325\n",
      "batch 2161: loss 0.007278973702341318\n",
      "batch 2162: loss 0.005312654189765453\n",
      "batch 2163: loss 0.0015381794655695558\n",
      "batch 2164: loss 0.0065013337880373\n",
      "batch 2165: loss 0.003910539206117392\n",
      "batch 2166: loss 0.0021477362606674433\n",
      "batch 2167: loss 0.013111071661114693\n",
      "batch 2168: loss 0.10697174072265625\n",
      "batch 2169: loss 0.005000721197575331\n",
      "batch 2170: loss 0.03454925864934921\n",
      "batch 2171: loss 0.09979705512523651\n",
      "batch 2172: loss 0.015156447887420654\n",
      "batch 2173: loss 0.0022275447845458984\n",
      "batch 2174: loss 0.011338473297655582\n",
      "batch 2175: loss 0.0838080570101738\n",
      "batch 2176: loss 0.04940028861165047\n",
      "batch 2177: loss 0.028829731047153473\n",
      "batch 2178: loss 0.003227359615266323\n",
      "batch 2179: loss 0.0012979238526895642\n",
      "batch 2180: loss 0.03991241753101349\n",
      "batch 2181: loss 0.012911087833344936\n",
      "batch 2182: loss 0.018236126750707626\n",
      "batch 2183: loss 0.004669870249927044\n",
      "batch 2184: loss 0.016681546345353127\n",
      "batch 2185: loss 0.003620937466621399\n",
      "batch 2186: loss 0.08526553213596344\n",
      "batch 2187: loss 0.02664341777563095\n",
      "batch 2188: loss 0.0014648903161287308\n",
      "batch 2189: loss 0.20520509779453278\n",
      "batch 2190: loss 0.01993859000504017\n",
      "batch 2191: loss 0.01320864912122488\n",
      "batch 2192: loss 0.015482264570891857\n",
      "batch 2193: loss 0.0004796548164449632\n",
      "batch 2194: loss 0.000891126983333379\n",
      "batch 2195: loss 0.05237396061420441\n",
      "batch 2196: loss 0.00542146060615778\n",
      "batch 2197: loss 0.003316519781947136\n",
      "batch 2198: loss 0.005423784255981445\n",
      "batch 2199: loss 0.024631459265947342\n",
      "batch 2200: loss 0.006286826450377703\n",
      "batch 2201: loss 0.2260063886642456\n",
      "batch 2202: loss 0.000575187848880887\n",
      "batch 2203: loss 0.14990703761577606\n",
      "batch 2204: loss 0.04538220167160034\n",
      "batch 2205: loss 0.0013864599168300629\n",
      "batch 2206: loss 0.031710315495729446\n",
      "batch 2207: loss 0.011316211894154549\n",
      "batch 2208: loss 0.017334984615445137\n",
      "batch 2209: loss 0.011196734383702278\n",
      "batch 2210: loss 0.0010295116808265448\n",
      "batch 2211: loss 0.0010046187089756131\n",
      "batch 2212: loss 0.001775539480149746\n",
      "batch 2213: loss 0.05566075071692467\n",
      "batch 2214: loss 0.011415567249059677\n",
      "batch 2215: loss 0.008014759980142117\n",
      "batch 2216: loss 0.07530097663402557\n",
      "batch 2217: loss 0.013935363851487637\n",
      "batch 2218: loss 0.027542175725102425\n",
      "batch 2219: loss 0.04602227732539177\n",
      "batch 2220: loss 0.011030767112970352\n",
      "batch 2221: loss 0.017865655943751335\n",
      "batch 2222: loss 0.007936163805425167\n",
      "batch 2223: loss 0.017211195081472397\n",
      "batch 2224: loss 0.04369005933403969\n",
      "batch 2225: loss 0.003270824206992984\n",
      "batch 2226: loss 0.006333411671221256\n",
      "batch 2227: loss 0.005787017289549112\n",
      "batch 2228: loss 0.06195331737399101\n",
      "batch 2229: loss 0.0028671969193965197\n",
      "batch 2230: loss 0.004495110362768173\n",
      "batch 2231: loss 0.002719585783779621\n",
      "batch 2232: loss 0.0002584719331935048\n",
      "batch 2233: loss 0.0007106441771611571\n",
      "batch 2234: loss 0.02633049339056015\n",
      "batch 2235: loss 0.009675148874521255\n",
      "batch 2236: loss 0.06557153910398483\n",
      "batch 2237: loss 0.048798635601997375\n",
      "batch 2238: loss 0.05886887386441231\n",
      "batch 2239: loss 0.014520696364343166\n",
      "batch 2240: loss 0.0016240612603724003\n",
      "batch 2241: loss 0.003317007329314947\n",
      "batch 2242: loss 0.00825794879347086\n",
      "batch 2243: loss 0.0045239562168717384\n",
      "batch 2244: loss 0.03156664967536926\n",
      "batch 2245: loss 0.005333891604095697\n",
      "batch 2246: loss 0.003138217143714428\n",
      "batch 2247: loss 0.1469118893146515\n",
      "batch 2248: loss 0.014608313329517841\n",
      "batch 2249: loss 0.028694843873381615\n",
      "batch 2250: loss 0.044234324246644974\n",
      "batch 2251: loss 0.007920078001916409\n",
      "batch 2252: loss 0.04900464788079262\n",
      "batch 2253: loss 0.1611974835395813\n",
      "batch 2254: loss 0.0025374104734510183\n",
      "batch 2255: loss 0.004027617163956165\n",
      "batch 2256: loss 0.060952868312597275\n",
      "batch 2257: loss 0.0007688170298933983\n",
      "batch 2258: loss 0.01695713959634304\n",
      "batch 2259: loss 0.00048591976519674063\n",
      "batch 2260: loss 0.006108887493610382\n",
      "batch 2261: loss 0.004887014627456665\n",
      "batch 2262: loss 0.0022395276464521885\n",
      "batch 2263: loss 0.005308162420988083\n",
      "batch 2264: loss 0.014374716207385063\n",
      "batch 2265: loss 0.0038232740480452776\n",
      "batch 2266: loss 0.0024338741786777973\n",
      "batch 2267: loss 0.003347760299220681\n",
      "batch 2268: loss 0.001977719133719802\n",
      "batch 2269: loss 0.052796535193920135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2270: loss 0.016272850334644318\n",
      "batch 2271: loss 0.004011811688542366\n",
      "batch 2272: loss 0.00690857321023941\n",
      "batch 2273: loss 0.04117128252983093\n",
      "batch 2274: loss 0.0006344480789266527\n",
      "batch 2275: loss 0.033867254853248596\n",
      "batch 2276: loss 0.01049667689949274\n",
      "batch 2277: loss 0.00987471453845501\n",
      "batch 2278: loss 0.004724624566733837\n",
      "batch 2279: loss 0.0023050052113831043\n",
      "batch 2280: loss 0.003683950752019882\n",
      "batch 2281: loss 0.0037346291355788708\n",
      "batch 2282: loss 0.05539705976843834\n",
      "batch 2283: loss 0.020525002852082253\n",
      "batch 2284: loss 0.008090890012681484\n",
      "batch 2285: loss 0.007225635927170515\n",
      "batch 2286: loss 0.00013200075773056597\n",
      "batch 2287: loss 0.00019982147205155343\n",
      "batch 2288: loss 0.0032887663692235947\n",
      "batch 2289: loss 0.008680442348122597\n",
      "batch 2290: loss 0.10537611693143845\n",
      "batch 2291: loss 0.0023278917651623487\n",
      "batch 2292: loss 0.00034857122227549553\n",
      "batch 2293: loss 0.004925613757222891\n",
      "batch 2294: loss 0.0022090754937380552\n",
      "batch 2295: loss 0.001219544792547822\n",
      "batch 2296: loss 0.029947904869914055\n",
      "batch 2297: loss 0.0020102073904126883\n",
      "batch 2298: loss 0.2210475504398346\n",
      "batch 2299: loss 0.0024652716238051653\n",
      "batch 2300: loss 0.0007260314305312932\n",
      "batch 2301: loss 0.011503342539072037\n",
      "batch 2302: loss 0.0014275218127295375\n",
      "batch 2303: loss 0.033957283943891525\n",
      "batch 2304: loss 0.021152125671505928\n",
      "batch 2305: loss 0.0015857145190238953\n",
      "batch 2306: loss 0.030193382874131203\n",
      "batch 2307: loss 0.006918071769177914\n",
      "batch 2308: loss 0.0030317504424601793\n",
      "batch 2309: loss 0.005212046205997467\n",
      "batch 2310: loss 0.004427592735737562\n",
      "batch 2311: loss 0.006502477452158928\n",
      "batch 2312: loss 0.09636487811803818\n",
      "batch 2313: loss 0.00042121915612369776\n",
      "batch 2314: loss 0.04607156664133072\n",
      "batch 2315: loss 0.14500951766967773\n",
      "batch 2316: loss 0.015074759721755981\n",
      "batch 2317: loss 0.004156296141445637\n",
      "batch 2318: loss 0.01026312168687582\n",
      "batch 2319: loss 0.10610565543174744\n",
      "batch 2320: loss 0.0007995485793799162\n",
      "batch 2321: loss 0.00038232706720009446\n",
      "batch 2322: loss 0.003966792020946741\n",
      "batch 2323: loss 0.014095569960772991\n",
      "batch 2324: loss 0.0015115512069314718\n",
      "batch 2325: loss 0.012073278427124023\n",
      "batch 2326: loss 0.0050560301169753075\n",
      "batch 2327: loss 0.006166145671159029\n",
      "batch 2328: loss 0.014598068781197071\n",
      "batch 2329: loss 0.02652810327708721\n",
      "batch 2330: loss 0.013013424351811409\n",
      "batch 2331: loss 0.05895233526825905\n",
      "batch 2332: loss 0.0767361968755722\n",
      "batch 2333: loss 0.07425740361213684\n",
      "batch 2334: loss 0.0022341187577694654\n",
      "batch 2335: loss 0.002863141242414713\n",
      "batch 2336: loss 0.03519653528928757\n",
      "batch 2337: loss 0.0008674966520629823\n",
      "batch 2338: loss 0.03420400619506836\n",
      "batch 2339: loss 0.036762870848178864\n",
      "batch 2340: loss 0.001980102388188243\n",
      "batch 2341: loss 0.009315386414527893\n",
      "batch 2342: loss 0.13309617340564728\n",
      "batch 2343: loss 0.0033701781649142504\n",
      "batch 2344: loss 0.0009842620929703116\n",
      "batch 2345: loss 0.059752557426691055\n",
      "batch 2346: loss 0.0044029816053807735\n",
      "batch 2347: loss 0.003240023273974657\n",
      "batch 2348: loss 0.0013657661620527506\n",
      "batch 2349: loss 0.009406131692230701\n",
      "batch 2350: loss 0.005900845862925053\n",
      "batch 2351: loss 0.0016786563210189342\n",
      "batch 2352: loss 0.008038005791604519\n",
      "batch 2353: loss 0.003190338145941496\n",
      "batch 2354: loss 0.001779551967047155\n",
      "batch 2355: loss 0.05892643705010414\n",
      "batch 2356: loss 0.003786916844546795\n",
      "batch 2357: loss 0.04153910651803017\n",
      "batch 2358: loss 0.002125931903719902\n",
      "batch 2359: loss 0.003520168596878648\n",
      "batch 2360: loss 0.042878732085227966\n",
      "batch 2361: loss 0.0033328263089060783\n",
      "batch 2362: loss 0.07319926470518112\n",
      "batch 2363: loss 0.0015494549879804254\n",
      "batch 2364: loss 0.0006473449757322669\n",
      "batch 2365: loss 0.006547019351273775\n",
      "batch 2366: loss 0.0019758963026106358\n",
      "batch 2367: loss 0.01361161470413208\n",
      "batch 2368: loss 0.05248737335205078\n",
      "batch 2369: loss 0.0014647498028352857\n",
      "batch 2370: loss 0.031044024974107742\n",
      "batch 2371: loss 0.051613107323646545\n",
      "batch 2372: loss 0.006024847272783518\n",
      "batch 2373: loss 0.022795910015702248\n",
      "batch 2374: loss 0.01187052670866251\n",
      "batch 2375: loss 0.005783733446151018\n",
      "batch 2376: loss 0.02301628515124321\n",
      "batch 2377: loss 0.039144206792116165\n",
      "batch 2378: loss 0.014374231919646263\n",
      "batch 2379: loss 0.030659841373562813\n",
      "batch 2380: loss 0.0002074432122753933\n",
      "batch 2381: loss 0.014694428071379662\n",
      "batch 2382: loss 0.002038171049207449\n",
      "batch 2383: loss 0.00045179869630374014\n",
      "batch 2384: loss 0.057103924453258514\n",
      "batch 2385: loss 0.04970891773700714\n",
      "batch 2386: loss 0.02102956920862198\n",
      "batch 2387: loss 0.0027294268365949392\n",
      "batch 2388: loss 0.018840571865439415\n",
      "batch 2389: loss 0.011249282397329807\n",
      "batch 2390: loss 0.00875314325094223\n",
      "batch 2391: loss 0.003687302116304636\n",
      "batch 2392: loss 0.007836755365133286\n",
      "batch 2393: loss 0.002683086320757866\n",
      "batch 2394: loss 0.034675415605306625\n",
      "batch 2395: loss 0.0008254463318735361\n",
      "batch 2396: loss 0.003911532461643219\n",
      "batch 2397: loss 0.012808073312044144\n",
      "batch 2398: loss 0.029824361205101013\n",
      "batch 2399: loss 0.15142822265625\n",
      "batch 2400: loss 0.010501893237233162\n",
      "batch 2401: loss 0.042547013610601425\n",
      "batch 2402: loss 0.0015005135210230947\n",
      "batch 2403: loss 0.0987827405333519\n",
      "batch 2404: loss 0.0015670733992010355\n",
      "batch 2405: loss 0.038811612874269485\n",
      "batch 2406: loss 0.003800506005063653\n",
      "batch 2407: loss 0.06103264167904854\n",
      "batch 2408: loss 0.0003096213040407747\n",
      "batch 2409: loss 0.06915326416492462\n",
      "batch 2410: loss 0.07489049434661865\n",
      "batch 2411: loss 0.0006904742913320661\n",
      "batch 2412: loss 0.14213019609451294\n",
      "batch 2413: loss 0.0009116323199123144\n",
      "batch 2414: loss 0.010715877637267113\n",
      "batch 2415: loss 0.0022933329455554485\n",
      "batch 2416: loss 0.03351237624883652\n",
      "batch 2417: loss 0.005726244300603867\n",
      "batch 2418: loss 0.012469008564949036\n",
      "batch 2419: loss 0.005946983117610216\n",
      "batch 2420: loss 0.03265203908085823\n",
      "batch 2421: loss 0.006387495435774326\n",
      "batch 2422: loss 0.0088338702917099\n",
      "batch 2423: loss 0.004125876817852259\n",
      "batch 2424: loss 0.0006957344594411552\n",
      "batch 2425: loss 0.018292276188731194\n",
      "batch 2426: loss 0.057716887444257736\n",
      "batch 2427: loss 0.00044396708835847676\n",
      "batch 2428: loss 0.0008703604689799249\n",
      "batch 2429: loss 0.006105483043938875\n",
      "batch 2430: loss 0.02197287417948246\n",
      "batch 2431: loss 0.0009691193699836731\n",
      "batch 2432: loss 0.0031947556417435408\n",
      "batch 2433: loss 0.03326332941651344\n",
      "batch 2434: loss 0.015417623333632946\n",
      "batch 2435: loss 0.20029868185520172\n",
      "batch 2436: loss 0.001752090989612043\n",
      "batch 2437: loss 0.006387942470610142\n",
      "batch 2438: loss 0.006417051423341036\n",
      "batch 2439: loss 0.12976479530334473\n",
      "batch 2440: loss 0.018632901832461357\n",
      "batch 2441: loss 0.05039244145154953\n",
      "batch 2442: loss 0.0015154842985793948\n",
      "batch 2443: loss 0.010933371260762215\n",
      "batch 2444: loss 0.005652233958244324\n",
      "batch 2445: loss 0.011431587859988213\n",
      "batch 2446: loss 0.0036372183822095394\n",
      "batch 2447: loss 0.001885565579868853\n",
      "batch 2448: loss 0.13795256614685059\n",
      "batch 2449: loss 0.0017759097972884774\n",
      "batch 2450: loss 0.0017272920813411474\n",
      "batch 2451: loss 0.02107783779501915\n",
      "batch 2452: loss 0.16599948704242706\n",
      "batch 2453: loss 0.0017563232686370611\n",
      "batch 2454: loss 0.017438260838389397\n",
      "batch 2455: loss 0.01497945748269558\n",
      "batch 2456: loss 0.007052673492580652\n",
      "batch 2457: loss 0.0009793273638933897\n",
      "batch 2458: loss 0.005724483635276556\n",
      "batch 2459: loss 0.0007192789344117045\n",
      "batch 2460: loss 0.01867212913930416\n",
      "batch 2461: loss 0.0030633381102234125\n",
      "batch 2462: loss 0.003604046069085598\n",
      "batch 2463: loss 0.02046322263777256\n",
      "batch 2464: loss 0.03856295719742775\n",
      "batch 2465: loss 0.001197376986965537\n",
      "batch 2466: loss 0.009399001486599445\n",
      "batch 2467: loss 0.006414239760488272\n",
      "batch 2468: loss 0.014601889066398144\n",
      "batch 2469: loss 0.0040825423784554005\n",
      "batch 2470: loss 0.008327123709022999\n",
      "batch 2471: loss 0.023690445348620415\n",
      "batch 2472: loss 0.003968119155615568\n",
      "batch 2473: loss 0.003988456912338734\n",
      "batch 2474: loss 0.028153352439403534\n",
      "batch 2475: loss 0.07813328504562378\n",
      "batch 2476: loss 0.0004566110437735915\n",
      "batch 2477: loss 0.010341807268559933\n",
      "batch 2478: loss 0.004279361572116613\n",
      "batch 2479: loss 0.005250001326203346\n",
      "batch 2480: loss 0.014940883032977581\n",
      "batch 2481: loss 0.005389912519603968\n",
      "batch 2482: loss 0.002047050278633833\n",
      "batch 2483: loss 0.011490623466670513\n",
      "batch 2484: loss 0.08528249710798264\n",
      "batch 2485: loss 0.0283752903342247\n",
      "batch 2486: loss 0.09855294227600098\n",
      "batch 2487: loss 0.002787628211081028\n",
      "batch 2488: loss 0.0080344770103693\n",
      "batch 2489: loss 0.00025436855503357947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2490: loss 0.002265653572976589\n",
      "batch 2491: loss 0.0007093881722539663\n",
      "batch 2492: loss 0.00022019294556230307\n",
      "batch 2493: loss 0.0010497593320906162\n",
      "batch 2494: loss 0.009356287308037281\n",
      "batch 2495: loss 0.1888231486082077\n",
      "batch 2496: loss 0.048527393490076065\n",
      "batch 2497: loss 0.0045191883109509945\n",
      "batch 2498: loss 0.0017634258838370442\n",
      "batch 2499: loss 0.006090426351875067\n",
      "batch 2500: loss 0.005687638185918331\n",
      "batch 2501: loss 0.023696303367614746\n",
      "batch 2502: loss 0.0011204174952581525\n",
      "batch 2503: loss 0.07726550847291946\n",
      "batch 2504: loss 0.021705443039536476\n",
      "batch 2505: loss 0.010921831242740154\n",
      "batch 2506: loss 0.003718433901667595\n",
      "batch 2507: loss 0.005833692383021116\n",
      "batch 2508: loss 0.0031865553464740515\n",
      "batch 2509: loss 0.00836956687271595\n",
      "batch 2510: loss 0.004038626328110695\n",
      "batch 2511: loss 0.0030000570695847273\n",
      "batch 2512: loss 0.21854975819587708\n",
      "batch 2513: loss 0.01748967356979847\n",
      "batch 2514: loss 0.018642352893948555\n",
      "batch 2515: loss 0.014543002471327782\n",
      "batch 2516: loss 0.014647220261394978\n",
      "batch 2517: loss 0.020517563447356224\n",
      "batch 2518: loss 0.005342250224202871\n",
      "batch 2519: loss 0.06253279745578766\n",
      "batch 2520: loss 0.005617039743810892\n",
      "batch 2521: loss 0.0036972707603126764\n",
      "batch 2522: loss 0.0025236925575882196\n",
      "batch 2523: loss 0.010903539136052132\n",
      "batch 2524: loss 0.004208762664347887\n",
      "batch 2525: loss 0.012491149827837944\n",
      "batch 2526: loss 0.0018237390322610736\n",
      "batch 2527: loss 0.006210667546838522\n",
      "batch 2528: loss 0.011332535184919834\n",
      "batch 2529: loss 0.04808482155203819\n",
      "batch 2530: loss 0.0016618329100310802\n",
      "batch 2531: loss 0.006585795897990465\n",
      "batch 2532: loss 0.001996441511437297\n",
      "batch 2533: loss 0.05973587930202484\n",
      "batch 2534: loss 0.002515173517167568\n",
      "batch 2535: loss 0.014855298213660717\n",
      "batch 2536: loss 0.003596444381400943\n",
      "batch 2537: loss 0.029624328017234802\n",
      "batch 2538: loss 0.005712416023015976\n",
      "batch 2539: loss 0.0012707057176157832\n",
      "batch 2540: loss 0.00487492885440588\n",
      "batch 2541: loss 0.011597181670367718\n",
      "batch 2542: loss 0.004866300616413355\n",
      "batch 2543: loss 0.0023364375811070204\n",
      "batch 2544: loss 0.0009465449838899076\n",
      "batch 2545: loss 0.002005735645070672\n",
      "batch 2546: loss 0.03989308699965477\n",
      "batch 2547: loss 0.06653912365436554\n",
      "batch 2548: loss 0.001782865496352315\n",
      "batch 2549: loss 0.0029241996817290783\n",
      "batch 2550: loss 0.07703664153814316\n",
      "batch 2551: loss 0.0458361841738224\n",
      "batch 2552: loss 0.0031180221121758223\n",
      "batch 2553: loss 0.004656011704355478\n",
      "batch 2554: loss 0.013111576437950134\n",
      "batch 2555: loss 0.04015825688838959\n",
      "batch 2556: loss 0.0068076495081186295\n",
      "batch 2557: loss 0.0018979584565386176\n",
      "batch 2558: loss 0.001593617140315473\n",
      "batch 2559: loss 0.013574592769145966\n",
      "batch 2560: loss 0.0005213926779106259\n",
      "batch 2561: loss 0.009824959561228752\n",
      "batch 2562: loss 0.031096812337636948\n",
      "batch 2563: loss 0.2726741135120392\n",
      "batch 2564: loss 0.001027434365823865\n",
      "batch 2565: loss 0.07045392692089081\n",
      "batch 2566: loss 0.04842071607708931\n",
      "batch 2567: loss 0.00278590084053576\n",
      "batch 2568: loss 0.0036665850784629583\n",
      "batch 2569: loss 0.0023914717603474855\n",
      "batch 2570: loss 0.07564910501241684\n",
      "batch 2571: loss 0.01681814342737198\n",
      "batch 2572: loss 0.012042737565934658\n",
      "batch 2573: loss 0.00830776710063219\n",
      "batch 2574: loss 0.0050185141153633595\n",
      "batch 2575: loss 0.0027017623651772738\n",
      "batch 2576: loss 0.04033350944519043\n",
      "batch 2577: loss 0.09252926707267761\n",
      "batch 2578: loss 0.011589013040065765\n",
      "batch 2579: loss 0.09760230034589767\n",
      "batch 2580: loss 0.004628111142665148\n",
      "batch 2581: loss 0.004538238048553467\n",
      "batch 2582: loss 0.024004992097616196\n",
      "batch 2583: loss 0.1369890719652176\n",
      "batch 2584: loss 0.002630349015817046\n",
      "batch 2585: loss 0.003916752990335226\n",
      "batch 2586: loss 0.00919219572097063\n",
      "batch 2587: loss 0.007781410589814186\n",
      "batch 2588: loss 0.051858559250831604\n",
      "batch 2589: loss 0.013228516094386578\n",
      "batch 2590: loss 0.010269967839121819\n",
      "batch 2591: loss 0.0021000769920647144\n",
      "batch 2592: loss 0.029131418094038963\n",
      "batch 2593: loss 0.001062398194335401\n",
      "batch 2594: loss 0.015237345360219479\n",
      "batch 2595: loss 0.007921752519905567\n",
      "batch 2596: loss 0.045400600880384445\n",
      "batch 2597: loss 0.0016044118674471974\n",
      "batch 2598: loss 0.017654772847890854\n",
      "batch 2599: loss 0.0005301183555275202\n",
      "batch 2600: loss 0.04634232819080353\n",
      "batch 2601: loss 0.0001677810214459896\n",
      "batch 2602: loss 0.0026420792564749718\n",
      "batch 2603: loss 0.022308973595499992\n",
      "batch 2604: loss 0.0015068033244460821\n",
      "batch 2605: loss 0.09334827214479446\n",
      "batch 2606: loss 0.02276553027331829\n",
      "batch 2607: loss 0.0039190552197396755\n",
      "batch 2608: loss 0.008723791688680649\n",
      "batch 2609: loss 0.0034490018151700497\n",
      "batch 2610: loss 0.10344124585390091\n",
      "batch 2611: loss 0.006169997155666351\n",
      "batch 2612: loss 0.008849645964801311\n",
      "batch 2613: loss 0.004597243387252092\n",
      "batch 2614: loss 0.027108084410429\n",
      "batch 2615: loss 0.0012020737631246448\n",
      "batch 2616: loss 0.005893888883292675\n",
      "batch 2617: loss 0.0983600988984108\n",
      "batch 2618: loss 0.0023957868106663227\n",
      "batch 2619: loss 0.06503820419311523\n",
      "batch 2620: loss 0.015569441951811314\n",
      "batch 2621: loss 0.000634938885923475\n",
      "batch 2622: loss 8.82285603438504e-05\n",
      "batch 2623: loss 0.11589612811803818\n",
      "batch 2624: loss 0.07883157581090927\n",
      "batch 2625: loss 0.0009986924706026912\n",
      "batch 2626: loss 0.00505863968282938\n",
      "batch 2627: loss 0.02524375356733799\n",
      "batch 2628: loss 0.006221678573638201\n",
      "batch 2629: loss 0.018441084772348404\n",
      "batch 2630: loss 0.038097359240055084\n",
      "batch 2631: loss 0.15835870802402496\n",
      "batch 2632: loss 0.022791368886828423\n",
      "batch 2633: loss 0.011098044924438\n",
      "batch 2634: loss 0.08559456467628479\n",
      "batch 2635: loss 0.03402823582291603\n",
      "batch 2636: loss 0.06670794636011124\n",
      "batch 2637: loss 0.005740858614444733\n",
      "batch 2638: loss 0.00042489616316743195\n",
      "batch 2639: loss 0.0019431322580203414\n",
      "batch 2640: loss 0.01284423191100359\n",
      "batch 2641: loss 0.01966226100921631\n",
      "batch 2642: loss 0.0011938554234802723\n",
      "batch 2643: loss 0.035084228962659836\n",
      "batch 2644: loss 0.006997575052082539\n",
      "batch 2645: loss 0.020846689119935036\n",
      "batch 2646: loss 0.020099548622965813\n",
      "batch 2647: loss 0.0016303330194205046\n",
      "batch 2648: loss 0.027447707951068878\n",
      "batch 2649: loss 0.001987360417842865\n",
      "batch 2650: loss 0.022887621074914932\n",
      "batch 2651: loss 0.023700665682554245\n",
      "batch 2652: loss 0.01621493324637413\n",
      "batch 2653: loss 0.037330932915210724\n",
      "batch 2654: loss 0.009696949273347855\n",
      "batch 2655: loss 0.004781085066497326\n",
      "batch 2656: loss 0.0031894093845039606\n",
      "batch 2657: loss 0.002287717303261161\n",
      "batch 2658: loss 0.0038881844375282526\n",
      "batch 2659: loss 0.014612799510359764\n",
      "batch 2660: loss 0.05629199370741844\n",
      "batch 2661: loss 0.0023316331207752228\n",
      "batch 2662: loss 0.008292303420603275\n",
      "batch 2663: loss 0.026703791692852974\n",
      "batch 2664: loss 0.02018110267817974\n",
      "batch 2665: loss 0.0005033397465012968\n",
      "batch 2666: loss 0.0006442794110625982\n",
      "batch 2667: loss 0.001959689427167177\n",
      "batch 2668: loss 0.0007344480254687369\n",
      "batch 2669: loss 0.028300516307353973\n",
      "batch 2670: loss 0.0005381383816711605\n",
      "batch 2671: loss 0.02763473242521286\n",
      "batch 2672: loss 0.0006713917828164995\n",
      "batch 2673: loss 0.002642460633069277\n",
      "batch 2674: loss 0.0002388804714428261\n",
      "batch 2675: loss 0.01931048557162285\n",
      "batch 2676: loss 0.0006056533893570304\n",
      "batch 2677: loss 0.014693149365484715\n",
      "batch 2678: loss 0.004735950846225023\n",
      "batch 2679: loss 0.0029498727526515722\n",
      "batch 2680: loss 0.002979219425469637\n",
      "batch 2681: loss 0.0005075803492218256\n",
      "batch 2682: loss 0.021064208820462227\n",
      "batch 2683: loss 0.0018216634634882212\n",
      "batch 2684: loss 0.007080657873302698\n",
      "batch 2685: loss 0.000655047653708607\n",
      "batch 2686: loss 0.030902022495865822\n",
      "batch 2687: loss 0.0024652022402733564\n",
      "batch 2688: loss 0.004014113452285528\n",
      "batch 2689: loss 0.0008496914524585009\n",
      "batch 2690: loss 0.00017625551845412701\n",
      "batch 2691: loss 0.0033061900176107883\n",
      "batch 2692: loss 0.008594461716711521\n",
      "batch 2693: loss 0.0015548474621027708\n",
      "batch 2694: loss 0.22259481251239777\n",
      "batch 2695: loss 0.0027565492782741785\n",
      "batch 2696: loss 0.0006253820611163974\n",
      "batch 2697: loss 0.007107528392225504\n",
      "batch 2698: loss 0.00047307583736255765\n",
      "batch 2699: loss 0.19876837730407715\n",
      "batch 2700: loss 0.001329987426288426\n",
      "batch 2701: loss 0.03504074364900589\n",
      "batch 2702: loss 0.01371975988149643\n",
      "batch 2703: loss 0.00011784960952354595\n",
      "batch 2704: loss 0.0002513625076971948\n",
      "batch 2705: loss 0.018925413489341736\n",
      "batch 2706: loss 0.017240606248378754\n",
      "batch 2707: loss 0.02604944445192814\n",
      "batch 2708: loss 0.011882327497005463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2709: loss 0.0008407295099459589\n",
      "batch 2710: loss 0.0007128735305741429\n",
      "batch 2711: loss 0.020960528403520584\n",
      "batch 2712: loss 0.1724560260772705\n",
      "batch 2713: loss 5.18120541528333e-05\n",
      "batch 2714: loss 0.00017876744095701724\n",
      "batch 2715: loss 0.00744590675458312\n",
      "batch 2716: loss 0.0020423305686563253\n",
      "batch 2717: loss 0.003380782902240753\n",
      "batch 2718: loss 0.004409232176840305\n",
      "batch 2719: loss 0.010127145797014236\n",
      "batch 2720: loss 0.0028537053149193525\n",
      "batch 2721: loss 0.013830305077135563\n",
      "batch 2722: loss 0.0015119792660698295\n",
      "batch 2723: loss 0.005112429615110159\n",
      "batch 2724: loss 0.00030780842644162476\n",
      "batch 2725: loss 0.0009794727666303515\n",
      "batch 2726: loss 0.009060586802661419\n",
      "batch 2727: loss 0.00038368385867215693\n",
      "batch 2728: loss 0.08747585117816925\n",
      "batch 2729: loss 0.0014513389905914664\n",
      "batch 2730: loss 0.002612937707453966\n",
      "batch 2731: loss 0.005818592384457588\n",
      "batch 2732: loss 0.0741332396864891\n",
      "batch 2733: loss 0.0015192372957244515\n",
      "batch 2734: loss 0.1081569641828537\n",
      "batch 2735: loss 0.0008096707169897854\n",
      "batch 2736: loss 0.000203559611691162\n",
      "batch 2737: loss 0.0012858649715781212\n",
      "batch 2738: loss 0.0020976609084755182\n",
      "batch 2739: loss 0.010944115929305553\n",
      "batch 2740: loss 0.0004455982125364244\n",
      "batch 2741: loss 0.016481375321745872\n",
      "batch 2742: loss 0.001561178476549685\n",
      "batch 2743: loss 0.007799467071890831\n",
      "batch 2744: loss 0.0547543428838253\n",
      "batch 2745: loss 0.0013174740597605705\n",
      "batch 2746: loss 0.060194820165634155\n",
      "batch 2747: loss 0.0019472518470138311\n",
      "batch 2748: loss 0.0007814114214852452\n",
      "batch 2749: loss 0.0012148894602432847\n",
      "batch 2750: loss 0.01136308629065752\n",
      "batch 2751: loss 0.0008377598132938147\n",
      "batch 2752: loss 0.0018240580102428794\n",
      "batch 2753: loss 0.04631735756993294\n",
      "batch 2754: loss 0.008034288883209229\n",
      "batch 2755: loss 0.004896895028650761\n",
      "batch 2756: loss 0.001643396564759314\n",
      "batch 2757: loss 0.005536096170544624\n",
      "batch 2758: loss 0.0011010706657543778\n",
      "batch 2759: loss 0.000762418145313859\n",
      "batch 2760: loss 0.0009719950030557811\n",
      "batch 2761: loss 0.29101303219795227\n",
      "batch 2762: loss 0.004723007790744305\n",
      "batch 2763: loss 0.002464612480252981\n",
      "batch 2764: loss 0.0008677345467731357\n",
      "batch 2765: loss 0.042467303574085236\n",
      "batch 2766: loss 0.03862976282835007\n",
      "batch 2767: loss 0.021894216537475586\n",
      "batch 2768: loss 0.010435521602630615\n",
      "batch 2769: loss 0.014749810099601746\n",
      "batch 2770: loss 0.06565118581056595\n",
      "batch 2771: loss 0.0006994445575401187\n",
      "batch 2772: loss 0.005574988201260567\n",
      "batch 2773: loss 0.03787659481167793\n",
      "batch 2774: loss 0.102532297372818\n",
      "batch 2775: loss 0.00691251503303647\n",
      "batch 2776: loss 0.0007757364073768258\n",
      "batch 2777: loss 0.0003323295677546412\n",
      "batch 2778: loss 0.007028292398899794\n",
      "batch 2779: loss 0.003259357064962387\n",
      "batch 2780: loss 0.013483520597219467\n",
      "batch 2781: loss 0.10101655125617981\n",
      "batch 2782: loss 0.005171865224838257\n",
      "batch 2783: loss 0.004494526889175177\n",
      "batch 2784: loss 0.05486815422773361\n",
      "batch 2785: loss 0.0009911038214340806\n",
      "batch 2786: loss 0.0017996837850660086\n",
      "batch 2787: loss 0.029603004455566406\n",
      "batch 2788: loss 0.024313688278198242\n",
      "batch 2789: loss 0.022287556901574135\n",
      "batch 2790: loss 0.004807095043361187\n",
      "batch 2791: loss 0.0005492503987625241\n",
      "batch 2792: loss 0.007882503792643547\n",
      "batch 2793: loss 0.0004080744693055749\n",
      "batch 2794: loss 0.004763795528560877\n",
      "batch 2795: loss 0.00212677801027894\n",
      "batch 2796: loss 0.06802863627672195\n",
      "batch 2797: loss 0.09664587676525116\n",
      "batch 2798: loss 0.0028775259852409363\n",
      "batch 2799: loss 0.039038922637701035\n",
      "batch 2800: loss 0.0009829268092289567\n",
      "batch 2801: loss 0.008979441598057747\n",
      "batch 2802: loss 0.07431890815496445\n",
      "batch 2803: loss 0.012739784084260464\n",
      "batch 2804: loss 0.0024963172618299723\n",
      "batch 2805: loss 0.020801177248358727\n",
      "batch 2806: loss 0.006153281312435865\n",
      "batch 2807: loss 0.08505430072546005\n",
      "batch 2808: loss 0.0018546158680692315\n",
      "batch 2809: loss 0.01098498422652483\n",
      "batch 2810: loss 0.08563109487295151\n",
      "batch 2811: loss 0.0069758412428200245\n",
      "batch 2812: loss 0.00114000728353858\n",
      "batch 2813: loss 0.00154474179726094\n",
      "batch 2814: loss 0.019794447347521782\n",
      "batch 2815: loss 0.005860103294253349\n",
      "batch 2816: loss 0.0006881870795041323\n",
      "batch 2817: loss 0.029562287032604218\n",
      "batch 2818: loss 0.0729450210928917\n",
      "batch 2819: loss 0.03037370927631855\n",
      "batch 2820: loss 0.010754624381661415\n",
      "batch 2821: loss 0.02160266786813736\n",
      "batch 2822: loss 0.14598019421100616\n",
      "batch 2823: loss 0.0010796100832521915\n",
      "batch 2824: loss 0.06751086562871933\n",
      "batch 2825: loss 0.0014401477528735995\n",
      "batch 2826: loss 0.00027070773649029434\n",
      "batch 2827: loss 0.07432715594768524\n",
      "batch 2828: loss 0.002650320762768388\n",
      "batch 2829: loss 0.004403907340019941\n",
      "batch 2830: loss 0.00722989859059453\n",
      "batch 2831: loss 0.007585455197840929\n",
      "batch 2832: loss 0.01901114359498024\n",
      "batch 2833: loss 0.007311681751161814\n",
      "batch 2834: loss 0.018240492790937424\n",
      "batch 2835: loss 0.03766559436917305\n",
      "batch 2836: loss 0.0025498084723949432\n",
      "batch 2837: loss 0.010246273130178452\n",
      "batch 2838: loss 0.020571622997522354\n",
      "batch 2839: loss 0.03433304652571678\n",
      "batch 2840: loss 0.0007306768675334752\n",
      "batch 2841: loss 0.04449896141886711\n",
      "batch 2842: loss 0.0015942483441904187\n",
      "batch 2843: loss 0.002086069667711854\n",
      "batch 2844: loss 0.07744597643613815\n",
      "batch 2845: loss 0.028574923053383827\n",
      "batch 2846: loss 0.0009105895878747106\n",
      "batch 2847: loss 0.09464649111032486\n",
      "batch 2848: loss 0.0007897206232883036\n",
      "batch 2849: loss 0.004927223548293114\n",
      "batch 2850: loss 0.02804703451693058\n",
      "batch 2851: loss 0.0007467152900062501\n",
      "batch 2852: loss 0.007574381772428751\n",
      "batch 2853: loss 0.0006686351844109595\n",
      "batch 2854: loss 0.07943353801965714\n",
      "batch 2855: loss 0.0027918785344809294\n",
      "batch 2856: loss 0.0012647515395656228\n",
      "batch 2857: loss 0.02144882082939148\n",
      "batch 2858: loss 0.00883751455694437\n",
      "batch 2859: loss 0.018429923802614212\n",
      "batch 2860: loss 0.004504832439124584\n",
      "batch 2861: loss 0.07124931365251541\n",
      "batch 2862: loss 0.011035881005227566\n",
      "batch 2863: loss 0.0008021894027478993\n",
      "batch 2864: loss 0.03701843321323395\n",
      "batch 2865: loss 0.002473709871992469\n",
      "batch 2866: loss 0.007614029571413994\n",
      "batch 2867: loss 0.25654831528663635\n",
      "batch 2868: loss 0.007402986288070679\n",
      "batch 2869: loss 0.017104119062423706\n",
      "batch 2870: loss 0.0014458739897236228\n",
      "batch 2871: loss 0.006003606133162975\n",
      "batch 2872: loss 0.00025298367836512625\n",
      "batch 2873: loss 0.027469661086797714\n",
      "batch 2874: loss 0.03582905977964401\n",
      "batch 2875: loss 0.0044886586256325245\n",
      "batch 2876: loss 0.03185185045003891\n",
      "batch 2877: loss 0.010182279162108898\n",
      "batch 2878: loss 0.003908757586032152\n",
      "batch 2879: loss 0.01892072521150112\n",
      "batch 2880: loss 0.0387844443321228\n",
      "batch 2881: loss 0.010177711956202984\n",
      "batch 2882: loss 0.0007227262831293046\n",
      "batch 2883: loss 0.0027528111822903156\n",
      "batch 2884: loss 0.007953867316246033\n",
      "batch 2885: loss 0.0015149784740060568\n",
      "batch 2886: loss 0.0019826225470751524\n",
      "batch 2887: loss 0.002069130539894104\n",
      "batch 2888: loss 0.04467201232910156\n",
      "batch 2889: loss 0.019802439957857132\n",
      "batch 2890: loss 0.004738418385386467\n",
      "batch 2891: loss 0.0007917947368696332\n",
      "batch 2892: loss 0.003743253881111741\n",
      "batch 2893: loss 0.002654313575476408\n",
      "batch 2894: loss 0.0018589855171740055\n",
      "batch 2895: loss 0.0853988379240036\n",
      "batch 2896: loss 0.009550366550683975\n",
      "batch 2897: loss 0.0018798626260831952\n",
      "batch 2898: loss 9.151722042588517e-05\n",
      "batch 2899: loss 0.00021381066471803933\n",
      "batch 2900: loss 0.00085483118891716\n",
      "batch 2901: loss 0.007563487160950899\n",
      "batch 2902: loss 0.024182483553886414\n",
      "batch 2903: loss 0.001656645443290472\n",
      "batch 2904: loss 0.0005401810049079359\n",
      "batch 2905: loss 0.0010613604681566358\n",
      "batch 2906: loss 0.007550136651843786\n",
      "batch 2907: loss 0.0034900768660008907\n",
      "batch 2908: loss 0.018365634605288506\n",
      "batch 2909: loss 0.004526801872998476\n",
      "batch 2910: loss 0.0019823843613266945\n",
      "batch 2911: loss 0.015513617545366287\n",
      "batch 2912: loss 0.0035520836245268583\n",
      "batch 2913: loss 0.002200610935688019\n",
      "batch 2914: loss 0.046802256256341934\n",
      "batch 2915: loss 0.010588073171675205\n",
      "batch 2916: loss 0.06795728206634521\n",
      "batch 2917: loss 0.24231477081775665\n",
      "batch 2918: loss 0.0015984427882358432\n",
      "batch 2919: loss 0.0010471713030710816\n",
      "batch 2920: loss 0.08080460131168365\n",
      "batch 2921: loss 0.023523032665252686\n",
      "batch 2922: loss 0.09459397196769714\n",
      "batch 2923: loss 0.02510944753885269\n",
      "batch 2924: loss 0.08978784084320068\n",
      "batch 2925: loss 0.016197457909584045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 2926: loss 0.0060954624786973\n",
      "batch 2927: loss 0.027320001274347305\n",
      "batch 2928: loss 0.02047056332230568\n",
      "batch 2929: loss 0.01878720335662365\n",
      "batch 2930: loss 0.004147423431277275\n",
      "batch 2931: loss 0.008376357145607471\n",
      "batch 2932: loss 0.01251107919961214\n",
      "batch 2933: loss 0.007606978062540293\n",
      "batch 2934: loss 0.04585292562842369\n",
      "batch 2935: loss 0.0019199512898921967\n",
      "batch 2936: loss 0.037273671478033066\n",
      "batch 2937: loss 0.0004321560263633728\n",
      "batch 2938: loss 0.009387953206896782\n",
      "batch 2939: loss 0.024556435644626617\n",
      "batch 2940: loss 0.007908185012638569\n",
      "batch 2941: loss 0.010921246372163296\n",
      "batch 2942: loss 0.1397300660610199\n",
      "batch 2943: loss 0.00037665374111384153\n",
      "batch 2944: loss 0.0005099170375615358\n",
      "batch 2945: loss 0.014835675247013569\n",
      "batch 2946: loss 0.0028166291303932667\n",
      "batch 2947: loss 0.0030987842474132776\n",
      "batch 2948: loss 0.013922680169343948\n",
      "batch 2949: loss 0.0010261089773848653\n",
      "batch 2950: loss 0.05637218430638313\n",
      "batch 2951: loss 0.05798017233610153\n",
      "batch 2952: loss 0.0008469962631352246\n",
      "batch 2953: loss 0.0007424201467074454\n",
      "batch 2954: loss 0.0009371681953780353\n",
      "batch 2955: loss 0.06378628313541412\n",
      "batch 2956: loss 0.0006723685655742884\n",
      "batch 2957: loss 0.003021244890987873\n",
      "batch 2958: loss 0.005764742381870747\n",
      "batch 2959: loss 0.0031669873278588057\n",
      "batch 2960: loss 0.003309626830741763\n",
      "batch 2961: loss 0.0010672651696950197\n",
      "batch 2962: loss 0.005731076933443546\n",
      "batch 2963: loss 0.00022995153267402202\n",
      "batch 2964: loss 0.0004546197014860809\n",
      "batch 2965: loss 0.04114268720149994\n",
      "batch 2966: loss 0.00023577854153700173\n",
      "batch 2967: loss 0.0007035413291305304\n",
      "batch 2968: loss 0.0004545436822809279\n",
      "batch 2969: loss 0.0020875371992588043\n",
      "batch 2970: loss 0.08622651547193527\n",
      "batch 2971: loss 0.0004559764638543129\n",
      "batch 2972: loss 0.020914101973176003\n",
      "batch 2973: loss 0.02619459293782711\n",
      "batch 2974: loss 0.005862039048224688\n",
      "batch 2975: loss 0.010048139840364456\n",
      "batch 2976: loss 0.007981925271451473\n",
      "batch 2977: loss 0.0149913365021348\n",
      "batch 2978: loss 0.0007094097090885043\n",
      "batch 2979: loss 0.0015167733654379845\n",
      "batch 2980: loss 0.00020012388995382935\n",
      "batch 2981: loss 0.009495064616203308\n",
      "batch 2982: loss 0.000672598194796592\n",
      "batch 2983: loss 0.012440788559615612\n",
      "batch 2984: loss 0.007830142043530941\n",
      "batch 2985: loss 0.006503390148282051\n",
      "batch 2986: loss 0.01342250406742096\n",
      "batch 2987: loss 0.008811995387077332\n",
      "batch 2988: loss 0.021931352093815804\n",
      "batch 2989: loss 0.0019071304704993963\n",
      "batch 2990: loss 0.000903116597328335\n",
      "batch 2991: loss 0.0006227973499335349\n",
      "batch 2992: loss 0.01397209893912077\n",
      "batch 2993: loss 0.02428949810564518\n",
      "batch 2994: loss 0.03190864622592926\n",
      "batch 2995: loss 0.1315918266773224\n",
      "batch 2996: loss 0.019363582134246826\n",
      "batch 2997: loss 0.0007082478259690106\n",
      "batch 2998: loss 0.0019406944047659636\n",
      "batch 2999: loss 0.005292094312608242\n",
      "batch 3000: loss 0.002842358546331525\n",
      "batch 3001: loss 0.00294276000931859\n",
      "batch 3002: loss 0.007842346094548702\n",
      "batch 3003: loss 0.01663631945848465\n",
      "batch 3004: loss 0.0031988173723220825\n",
      "batch 3005: loss 0.023205168545246124\n",
      "batch 3006: loss 0.028543628752231598\n",
      "batch 3007: loss 0.00026971782790496945\n",
      "batch 3008: loss 0.03167858347296715\n",
      "batch 3009: loss 0.010428039357066154\n",
      "batch 3010: loss 0.0014893502229824662\n",
      "batch 3011: loss 0.045305442065000534\n",
      "batch 3012: loss 0.02754490077495575\n",
      "batch 3013: loss 0.00455761281773448\n",
      "batch 3014: loss 0.01162432599812746\n",
      "batch 3015: loss 0.009203032590448856\n",
      "batch 3016: loss 0.0003736245562322438\n",
      "batch 3017: loss 0.0011973074870184064\n",
      "batch 3018: loss 0.0002756754111032933\n",
      "batch 3019: loss 0.002639426849782467\n",
      "batch 3020: loss 0.07412569224834442\n",
      "batch 3021: loss 0.034553155303001404\n",
      "batch 3022: loss 0.00028081994969397783\n",
      "batch 3023: loss 0.13835783302783966\n",
      "batch 3024: loss 0.3572840392589569\n",
      "batch 3025: loss 0.0001458874758100137\n",
      "batch 3026: loss 0.128147155046463\n",
      "batch 3027: loss 0.004938869271427393\n",
      "batch 3028: loss 0.005261159501969814\n",
      "batch 3029: loss 0.0008442099206149578\n",
      "batch 3030: loss 0.00801753904670477\n",
      "batch 3031: loss 0.008707738481462002\n",
      "batch 3032: loss 0.0016172757605090737\n",
      "batch 3033: loss 0.002609744668006897\n",
      "batch 3034: loss 0.19007571041584015\n",
      "batch 3035: loss 0.0028037058655172586\n",
      "batch 3036: loss 0.00111641816329211\n",
      "batch 3037: loss 0.000661661324556917\n",
      "batch 3038: loss 0.0035539332311600447\n",
      "batch 3039: loss 0.003353895153850317\n",
      "batch 3040: loss 0.04296974465250969\n",
      "batch 3041: loss 0.0006520944880321622\n",
      "batch 3042: loss 0.00096173892961815\n",
      "batch 3043: loss 0.007818951271474361\n",
      "batch 3044: loss 0.02179606445133686\n",
      "batch 3045: loss 0.0005689707468263805\n",
      "batch 3046: loss 0.03458765149116516\n",
      "batch 3047: loss 0.011728162877261639\n",
      "batch 3048: loss 0.00684750359505415\n",
      "batch 3049: loss 0.008100908249616623\n",
      "batch 3050: loss 0.062472980469465256\n",
      "batch 3051: loss 0.27116653323173523\n",
      "batch 3052: loss 0.0033199063036590815\n",
      "batch 3053: loss 0.10420814901590347\n",
      "batch 3054: loss 0.02081850729882717\n",
      "batch 3055: loss 0.008009908720850945\n",
      "batch 3056: loss 0.05156584084033966\n",
      "batch 3057: loss 0.004803675692528486\n",
      "batch 3058: loss 0.004249264020472765\n",
      "batch 3059: loss 0.028484800830483437\n",
      "batch 3060: loss 0.0032104630954563618\n",
      "batch 3061: loss 0.005194589961320162\n",
      "batch 3062: loss 0.00939403846859932\n",
      "batch 3063: loss 0.014643494971096516\n",
      "batch 3064: loss 0.007977788336575031\n",
      "batch 3065: loss 0.009397098794579506\n",
      "batch 3066: loss 0.01891803741455078\n",
      "batch 3067: loss 0.01182056125253439\n",
      "batch 3068: loss 0.0006335375364869833\n",
      "batch 3069: loss 0.002474291482940316\n",
      "batch 3070: loss 0.0002008460578508675\n",
      "batch 3071: loss 0.008292625658214092\n",
      "batch 3072: loss 0.0022445532958954573\n",
      "batch 3073: loss 0.003237765748053789\n",
      "batch 3074: loss 0.0003658159985207021\n",
      "batch 3075: loss 0.08336206525564194\n",
      "batch 3076: loss 0.003738808212801814\n",
      "batch 3077: loss 0.002362847328186035\n",
      "batch 3078: loss 0.0534566193819046\n",
      "batch 3079: loss 0.0010919466149061918\n",
      "batch 3080: loss 0.01622822880744934\n",
      "batch 3081: loss 0.0008482604171149433\n",
      "batch 3082: loss 0.007945849560201168\n",
      "batch 3083: loss 0.012648401781916618\n",
      "batch 3084: loss 0.002250968711450696\n",
      "batch 3085: loss 0.0004463319492060691\n",
      "batch 3086: loss 0.017424864694476128\n",
      "batch 3087: loss 0.008796135894954205\n",
      "batch 3088: loss 0.008089933544397354\n",
      "batch 3089: loss 0.014723770320415497\n",
      "batch 3090: loss 0.001528280321508646\n",
      "batch 3091: loss 0.005117048975080252\n",
      "batch 3092: loss 0.0009129098616540432\n",
      "batch 3093: loss 0.0037094878498464823\n",
      "batch 3094: loss 0.0018562471959739923\n",
      "batch 3095: loss 0.0014419109793379903\n",
      "batch 3096: loss 0.019572073593735695\n",
      "batch 3097: loss 0.05473869666457176\n",
      "batch 3098: loss 0.007796948775649071\n",
      "batch 3099: loss 0.0023484169505536556\n",
      "batch 3100: loss 0.00015619305486325175\n",
      "batch 3101: loss 0.0027414560317993164\n",
      "batch 3102: loss 0.00832325778901577\n",
      "batch 3103: loss 0.004604766145348549\n",
      "batch 3104: loss 0.0011770683340728283\n",
      "batch 3105: loss 0.0030863883439451456\n",
      "batch 3106: loss 0.024216430261731148\n",
      "batch 3107: loss 0.0009803271386772394\n",
      "batch 3108: loss 0.05709726735949516\n",
      "batch 3109: loss 0.017949266359210014\n",
      "batch 3110: loss 0.005180784501135349\n",
      "batch 3111: loss 0.017781764268875122\n",
      "batch 3112: loss 0.002513672923669219\n",
      "batch 3113: loss 0.003528937231749296\n",
      "batch 3114: loss 0.0007077537593431771\n",
      "batch 3115: loss 0.01771472580730915\n",
      "batch 3116: loss 0.0012377381790429354\n",
      "batch 3117: loss 0.004690734203904867\n",
      "batch 3118: loss 0.0006143212085589767\n",
      "batch 3119: loss 0.02558428980410099\n",
      "batch 3120: loss 8.515639638062567e-05\n",
      "batch 3121: loss 0.009002091363072395\n",
      "batch 3122: loss 0.0013295860262587667\n",
      "batch 3123: loss 0.00046746034058742225\n",
      "batch 3124: loss 0.001887087826617062\n",
      "batch 3125: loss 0.00033806735882535577\n",
      "batch 3126: loss 0.007617047056555748\n",
      "batch 3127: loss 0.004692372400313616\n",
      "batch 3128: loss 0.00044043714297004044\n",
      "batch 3129: loss 0.015225445851683617\n",
      "batch 3130: loss 0.007772376760840416\n",
      "batch 3131: loss 0.0008876862702891231\n",
      "batch 3132: loss 0.00031669437885284424\n",
      "batch 3133: loss 0.0024264391977339983\n",
      "batch 3134: loss 0.002034860895946622\n",
      "batch 3135: loss 0.010550949722528458\n",
      "batch 3136: loss 0.005453277844935656\n",
      "batch 3137: loss 0.00040255318162962794\n",
      "batch 3138: loss 0.007156464271247387\n",
      "batch 3139: loss 0.010560763068497181\n",
      "batch 3140: loss 0.0006637385813519359\n",
      "batch 3141: loss 0.0003391200734768063\n",
      "batch 3142: loss 0.0001393498241668567\n",
      "batch 3143: loss 0.0006826558383181691\n",
      "batch 3144: loss 0.00017905334243550897\n",
      "batch 3145: loss 0.0016924884403124452\n",
      "batch 3146: loss 0.0005267420783638954\n",
      "batch 3147: loss 0.005482794716954231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3148: loss 0.0012557449517771602\n",
      "batch 3149: loss 0.0038080550730228424\n",
      "batch 3150: loss 0.0004588932788465172\n",
      "batch 3151: loss 0.005147973075509071\n",
      "batch 3152: loss 0.00012045906623825431\n",
      "batch 3153: loss 0.002571948105469346\n",
      "batch 3154: loss 0.006914218887686729\n",
      "batch 3155: loss 0.010913841426372528\n",
      "batch 3156: loss 0.0003474804980214685\n",
      "batch 3157: loss 0.0015898046549409628\n",
      "batch 3158: loss 0.0004859677283093333\n",
      "batch 3159: loss 0.0007372012478299439\n",
      "batch 3160: loss 0.0008743978105485439\n",
      "batch 3161: loss 3.235278927604668e-05\n",
      "batch 3162: loss 0.039413128048181534\n",
      "batch 3163: loss 0.00011054350761696696\n",
      "batch 3164: loss 0.00017339982150588185\n",
      "batch 3165: loss 0.0002128884952981025\n",
      "batch 3166: loss 0.0061394828371703625\n",
      "batch 3167: loss 0.0001849398686317727\n",
      "batch 3168: loss 0.044202182441949844\n",
      "batch 3169: loss 0.0014766664244234562\n",
      "batch 3170: loss 0.0014778152108192444\n",
      "batch 3171: loss 0.002892959164455533\n",
      "batch 3172: loss 0.000502966286148876\n",
      "batch 3173: loss 0.0427522175014019\n",
      "batch 3174: loss 0.10727804154157639\n",
      "batch 3175: loss 0.060076601803302765\n",
      "batch 3176: loss 0.006949108559638262\n",
      "batch 3177: loss 0.0008553385268896818\n",
      "batch 3178: loss 0.0014482256956398487\n",
      "batch 3179: loss 0.04382859170436859\n",
      "batch 3180: loss 7.11473694536835e-05\n",
      "batch 3181: loss 0.03518368676304817\n",
      "batch 3182: loss 0.0004794730048161\n",
      "batch 3183: loss 0.039743512868881226\n",
      "batch 3184: loss 0.000884795852471143\n",
      "batch 3185: loss 0.0009319856762886047\n",
      "batch 3186: loss 0.10517005622386932\n",
      "batch 3187: loss 0.00011818177881650627\n",
      "batch 3188: loss 0.00015820424596313387\n",
      "batch 3189: loss 0.0045560807920992374\n",
      "batch 3190: loss 0.04331202805042267\n",
      "batch 3191: loss 0.000513519742526114\n",
      "batch 3192: loss 0.048354197293519974\n",
      "batch 3193: loss 0.024224450811743736\n",
      "batch 3194: loss 0.0006456575938500464\n",
      "batch 3195: loss 0.010740958154201508\n",
      "batch 3196: loss 0.0007319990545511246\n",
      "batch 3197: loss 0.08820738643407822\n",
      "batch 3198: loss 0.031283799558877945\n",
      "batch 3199: loss 0.03709480166435242\n",
      "batch 3200: loss 0.0006677197525277734\n",
      "batch 3201: loss 0.0031505771912634373\n",
      "batch 3202: loss 0.0005125165334902704\n",
      "batch 3203: loss 0.001084301620721817\n",
      "batch 3204: loss 0.006738410331308842\n",
      "batch 3205: loss 0.00614846870303154\n",
      "batch 3206: loss 0.0008906741859391332\n",
      "batch 3207: loss 0.005849660374224186\n",
      "batch 3208: loss 0.0008703374187462032\n",
      "batch 3209: loss 0.012739041820168495\n",
      "batch 3210: loss 0.0024696029722690582\n",
      "batch 3211: loss 0.021332774311304092\n",
      "batch 3212: loss 0.0036715019959956408\n",
      "batch 3213: loss 0.0526079460978508\n",
      "batch 3214: loss 0.012405218556523323\n",
      "batch 3215: loss 0.00628442270681262\n",
      "batch 3216: loss 0.0035989414900541306\n",
      "batch 3217: loss 0.03807847946882248\n",
      "batch 3218: loss 0.00017088458116631955\n",
      "batch 3219: loss 0.0005356479086913168\n",
      "batch 3220: loss 0.0003436366096138954\n",
      "batch 3221: loss 0.04570353031158447\n",
      "batch 3222: loss 0.0592908076941967\n",
      "batch 3223: loss 0.00758651178330183\n",
      "batch 3224: loss 0.0002482804120518267\n",
      "batch 3225: loss 0.10719915479421616\n",
      "batch 3226: loss 0.004026194103062153\n",
      "batch 3227: loss 0.002206469187512994\n",
      "batch 3228: loss 0.015570629388093948\n",
      "batch 3229: loss 0.0017698759911581874\n",
      "batch 3230: loss 0.08159509301185608\n",
      "batch 3231: loss 0.008876976557075977\n",
      "batch 3232: loss 0.008927283808588982\n",
      "batch 3233: loss 0.0632232055068016\n",
      "batch 3234: loss 0.008747268468141556\n",
      "batch 3235: loss 0.01528411079198122\n",
      "batch 3236: loss 0.01757119782269001\n",
      "batch 3237: loss 0.1390884965658188\n",
      "batch 3238: loss 0.0028300744015723467\n",
      "batch 3239: loss 0.08466452360153198\n",
      "batch 3240: loss 0.1026124581694603\n",
      "batch 3241: loss 0.0010823484044522047\n",
      "batch 3242: loss 0.0009795440128073096\n",
      "batch 3243: loss 0.0017023991094902158\n",
      "batch 3244: loss 0.04692627862095833\n",
      "batch 3245: loss 0.001451897551305592\n",
      "batch 3246: loss 0.001510182861238718\n",
      "batch 3247: loss 0.006484282668679953\n",
      "batch 3248: loss 0.04744019731879234\n",
      "batch 3249: loss 0.04530134052038193\n",
      "batch 3250: loss 0.007856471464037895\n",
      "batch 3251: loss 0.0013346181949600577\n",
      "batch 3252: loss 0.0015759923262521625\n",
      "batch 3253: loss 0.13177214562892914\n",
      "batch 3254: loss 0.0012479264987632632\n",
      "batch 3255: loss 0.13279032707214355\n",
      "batch 3256: loss 0.10187608748674393\n",
      "batch 3257: loss 0.0020815895404666662\n",
      "batch 3258: loss 0.02354312874376774\n",
      "batch 3259: loss 0.00872317235916853\n",
      "batch 3260: loss 0.0960170179605484\n",
      "batch 3261: loss 0.018811941146850586\n",
      "batch 3262: loss 0.16221904754638672\n",
      "batch 3263: loss 0.002136558061465621\n",
      "batch 3264: loss 0.005757047329097986\n",
      "batch 3265: loss 0.013522164896130562\n",
      "batch 3266: loss 0.003062959760427475\n",
      "batch 3267: loss 0.021099932491779327\n",
      "batch 3268: loss 0.0030693220905959606\n",
      "batch 3269: loss 0.022325245663523674\n",
      "batch 3270: loss 0.09085822850465775\n",
      "batch 3271: loss 0.0006603971123695374\n",
      "batch 3272: loss 0.0019211280159652233\n",
      "batch 3273: loss 0.000492625986225903\n",
      "batch 3274: loss 0.09718813002109528\n",
      "batch 3275: loss 0.02112867869436741\n",
      "batch 3276: loss 0.00438849488273263\n",
      "batch 3277: loss 0.00570340733975172\n",
      "batch 3278: loss 0.00972818024456501\n",
      "batch 3279: loss 0.014517366886138916\n",
      "batch 3280: loss 0.0025244527496397495\n",
      "batch 3281: loss 0.00048249063547700644\n",
      "batch 3282: loss 0.03037918172776699\n",
      "batch 3283: loss 0.006453750655055046\n",
      "batch 3284: loss 0.006375898141413927\n",
      "batch 3285: loss 0.00419248640537262\n",
      "batch 3286: loss 0.006889980752021074\n",
      "batch 3287: loss 0.0033557673450559378\n",
      "batch 3288: loss 0.029073676094412804\n",
      "batch 3289: loss 0.00292216707020998\n",
      "batch 3290: loss 0.009287582710385323\n",
      "batch 3291: loss 0.06839632987976074\n",
      "batch 3292: loss 0.002029625466093421\n",
      "batch 3293: loss 0.001254888717085123\n",
      "batch 3294: loss 0.003980961162596941\n",
      "batch 3295: loss 0.0032457776833325624\n",
      "batch 3296: loss 0.0002501346170902252\n",
      "batch 3297: loss 0.0029754796996712685\n",
      "batch 3298: loss 0.017798785120248795\n",
      "batch 3299: loss 0.047138359397649765\n",
      "batch 3300: loss 0.00022318909759633243\n",
      "batch 3301: loss 0.013180725276470184\n",
      "batch 3302: loss 0.04273940995335579\n",
      "batch 3303: loss 0.007553725503385067\n",
      "batch 3304: loss 0.07343024015426636\n",
      "batch 3305: loss 0.00031013524858281016\n",
      "batch 3306: loss 0.02821449190378189\n",
      "batch 3307: loss 0.01185371819883585\n",
      "batch 3308: loss 0.0009012094815261662\n",
      "batch 3309: loss 0.057794008404016495\n",
      "batch 3310: loss 0.04543813690543175\n",
      "batch 3311: loss 0.0005477432277984917\n",
      "batch 3312: loss 0.01840445213019848\n",
      "batch 3313: loss 0.000134496163809672\n",
      "batch 3314: loss 0.00019483230425976217\n",
      "batch 3315: loss 0.011677827686071396\n",
      "batch 3316: loss 0.011782843619585037\n",
      "batch 3317: loss 0.0016020139446482062\n",
      "batch 3318: loss 0.0490385964512825\n",
      "batch 3319: loss 0.0031878971494734287\n",
      "batch 3320: loss 0.03034069947898388\n",
      "batch 3321: loss 0.007100437302142382\n",
      "batch 3322: loss 0.0027836551889777184\n",
      "batch 3323: loss 0.2884473502635956\n",
      "batch 3324: loss 0.0047459471970796585\n",
      "batch 3325: loss 0.014829645864665508\n",
      "batch 3326: loss 0.0014924141578376293\n",
      "batch 3327: loss 0.0027310214936733246\n",
      "batch 3328: loss 0.0010325739858672023\n",
      "batch 3329: loss 0.011310277506709099\n",
      "batch 3330: loss 0.01699127070605755\n",
      "batch 3331: loss 0.0010785390622913837\n",
      "batch 3332: loss 0.0008946107118390501\n",
      "batch 3333: loss 0.002136775990948081\n",
      "batch 3334: loss 0.030997270718216896\n",
      "batch 3335: loss 0.0880003497004509\n",
      "batch 3336: loss 0.024233225733041763\n",
      "batch 3337: loss 0.003970488905906677\n",
      "batch 3338: loss 0.006124552339315414\n",
      "batch 3339: loss 0.0014684052439406514\n",
      "batch 3340: loss 0.3198229670524597\n",
      "batch 3341: loss 0.0007682534633204341\n",
      "batch 3342: loss 0.00030116664129309356\n",
      "batch 3343: loss 0.004375736694782972\n",
      "batch 3344: loss 0.005173126235604286\n",
      "batch 3345: loss 0.018121564760804176\n",
      "batch 3346: loss 0.013920441269874573\n",
      "batch 3347: loss 0.011021946556866169\n",
      "batch 3348: loss 0.015499570406973362\n",
      "batch 3349: loss 0.015721874311566353\n",
      "batch 3350: loss 0.0788891613483429\n",
      "batch 3351: loss 0.06389815360307693\n",
      "batch 3352: loss 0.005507478956133127\n",
      "batch 3353: loss 0.0011551433708518744\n",
      "batch 3354: loss 0.0007102492381818593\n",
      "batch 3355: loss 0.005541636608541012\n",
      "batch 3356: loss 0.0006520631141029298\n",
      "batch 3357: loss 0.03855810686945915\n",
      "batch 3358: loss 0.003584135789424181\n",
      "batch 3359: loss 0.04992806166410446\n",
      "batch 3360: loss 0.05141166225075722\n",
      "batch 3361: loss 0.06951615959405899\n",
      "batch 3362: loss 0.001539651770144701\n",
      "batch 3363: loss 0.0013609224697574973\n",
      "batch 3364: loss 0.015087750740349293\n",
      "batch 3365: loss 0.0025631545577198267\n",
      "batch 3366: loss 0.0019739631097763777\n",
      "batch 3367: loss 0.04029005765914917\n",
      "batch 3368: loss 0.005700510460883379\n",
      "batch 3369: loss 0.001318105380050838\n",
      "batch 3370: loss 0.002076922683045268\n",
      "batch 3371: loss 0.08206246048212051\n",
      "batch 3372: loss 0.04776517301797867\n",
      "batch 3373: loss 0.002820682944729924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3374: loss 0.004413672257214785\n",
      "batch 3375: loss 0.005406038835644722\n",
      "batch 3376: loss 0.010690433904528618\n",
      "batch 3377: loss 0.014270661398768425\n",
      "batch 3378: loss 0.00026171928038820624\n",
      "batch 3379: loss 0.0007980431546457112\n",
      "batch 3380: loss 0.0376865416765213\n",
      "batch 3381: loss 0.0011086906306445599\n",
      "batch 3382: loss 0.01292184367775917\n",
      "batch 3383: loss 0.021821463480591774\n",
      "batch 3384: loss 0.0015413622604683042\n",
      "batch 3385: loss 0.0016879307804629207\n",
      "batch 3386: loss 0.0037654053885489702\n",
      "batch 3387: loss 0.018078161403536797\n",
      "batch 3388: loss 0.08819754421710968\n",
      "batch 3389: loss 0.04411225765943527\n",
      "batch 3390: loss 0.012382256798446178\n",
      "batch 3391: loss 0.01231129840016365\n",
      "batch 3392: loss 0.02728193625807762\n",
      "batch 3393: loss 0.0034507568925619125\n",
      "batch 3394: loss 0.010379256680607796\n",
      "batch 3395: loss 0.059616997838020325\n",
      "batch 3396: loss 0.033897221088409424\n",
      "batch 3397: loss 0.11012812703847885\n",
      "batch 3398: loss 0.008805635385215282\n",
      "batch 3399: loss 0.0032836152240633965\n",
      "batch 3400: loss 0.01623392105102539\n",
      "batch 3401: loss 0.00461726076900959\n",
      "batch 3402: loss 0.0008336548344232142\n",
      "batch 3403: loss 0.032608311623334885\n",
      "batch 3404: loss 0.03920556604862213\n",
      "batch 3405: loss 0.0004573497863020748\n",
      "batch 3406: loss 0.0023780763149261475\n",
      "batch 3407: loss 0.00017290239338763058\n",
      "batch 3408: loss 0.05155608430504799\n",
      "batch 3409: loss 0.10940235108137131\n",
      "batch 3410: loss 0.047137293964624405\n",
      "batch 3411: loss 0.00041824945947155356\n",
      "batch 3412: loss 0.002351560164242983\n",
      "batch 3413: loss 0.00036964676110073924\n",
      "batch 3414: loss 0.026750922203063965\n",
      "batch 3415: loss 0.0016722625587135553\n",
      "batch 3416: loss 0.0008909533498808742\n",
      "batch 3417: loss 0.019721021875739098\n",
      "batch 3418: loss 0.07553450763225555\n",
      "batch 3419: loss 0.002826533280313015\n",
      "batch 3420: loss 0.0027242230717092752\n",
      "batch 3421: loss 0.0016138267237693071\n",
      "batch 3422: loss 0.0010675097582861781\n",
      "batch 3423: loss 0.04095439240336418\n",
      "batch 3424: loss 0.001200315309688449\n",
      "batch 3425: loss 0.002973557449877262\n",
      "batch 3426: loss 0.0005594171234406531\n",
      "batch 3427: loss 0.0005833581672050059\n",
      "batch 3428: loss 0.028513215482234955\n",
      "batch 3429: loss 0.06560230255126953\n",
      "batch 3430: loss 0.018365228548645973\n",
      "batch 3431: loss 0.002608434297144413\n",
      "batch 3432: loss 0.002072169678285718\n",
      "batch 3433: loss 0.0005953015061095357\n",
      "batch 3434: loss 0.008897317573428154\n",
      "batch 3435: loss 0.0003737600054591894\n",
      "batch 3436: loss 0.05867563560605049\n",
      "batch 3437: loss 0.008753343485295773\n",
      "batch 3438: loss 0.01316971704363823\n",
      "batch 3439: loss 0.0021565246861428022\n",
      "batch 3440: loss 0.004620520863682032\n",
      "batch 3441: loss 0.0024335619527846575\n",
      "batch 3442: loss 0.0018019065028056502\n",
      "batch 3443: loss 0.007813170552253723\n",
      "batch 3444: loss 0.00035571539774537086\n",
      "batch 3445: loss 0.003684696275740862\n",
      "batch 3446: loss 0.03947531431913376\n",
      "batch 3447: loss 0.0016932650469243526\n",
      "batch 3448: loss 0.002397360047325492\n",
      "batch 3449: loss 0.008091876283288002\n",
      "batch 3450: loss 0.011345885694026947\n",
      "batch 3451: loss 0.0113372802734375\n",
      "batch 3452: loss 0.024779926985502243\n",
      "batch 3453: loss 0.013280129991471767\n",
      "batch 3454: loss 0.00013703620061278343\n",
      "batch 3455: loss 0.015407643280923367\n",
      "batch 3456: loss 0.0019014307763427496\n",
      "batch 3457: loss 0.0009140385664068162\n",
      "batch 3458: loss 0.0140000581741333\n",
      "batch 3459: loss 0.005145950708538294\n",
      "batch 3460: loss 0.004247185308486223\n",
      "batch 3461: loss 0.00313784321770072\n",
      "batch 3462: loss 0.018287433311343193\n",
      "batch 3463: loss 0.00160345493350178\n",
      "batch 3464: loss 0.0005621224408969283\n",
      "batch 3465: loss 0.013411185704171658\n",
      "batch 3466: loss 0.00014240876771509647\n",
      "batch 3467: loss 0.0012971274554729462\n",
      "batch 3468: loss 0.0040346370078623295\n",
      "batch 3469: loss 0.03973780944943428\n",
      "batch 3470: loss 0.024519991129636765\n",
      "batch 3471: loss 0.0030515664257109165\n",
      "batch 3472: loss 0.00044928849092684686\n",
      "batch 3473: loss 0.0001269041677005589\n",
      "batch 3474: loss 0.05310489237308502\n",
      "batch 3475: loss 0.00043433744576759636\n",
      "batch 3476: loss 0.0001084860268747434\n",
      "batch 3477: loss 0.001439556130208075\n",
      "batch 3478: loss 0.017039410769939423\n",
      "batch 3479: loss 0.0031377295963466167\n",
      "batch 3480: loss 0.004818187095224857\n",
      "batch 3481: loss 0.0037951883859932423\n",
      "batch 3482: loss 0.000890150258783251\n",
      "batch 3483: loss 0.02257360704243183\n",
      "batch 3484: loss 0.08251528441905975\n",
      "batch 3485: loss 0.020237522199749947\n",
      "batch 3486: loss 0.0015158198075369\n",
      "batch 3487: loss 0.003971228376030922\n",
      "batch 3488: loss 0.04903320223093033\n",
      "batch 3489: loss 0.006075891200453043\n",
      "batch 3490: loss 0.005535988602787256\n",
      "batch 3491: loss 0.3396945893764496\n",
      "batch 3492: loss 0.0009893289534375072\n",
      "batch 3493: loss 0.044799938797950745\n",
      "batch 3494: loss 0.0005076975794509053\n",
      "batch 3495: loss 0.0002494454674888402\n",
      "batch 3496: loss 0.07279722392559052\n",
      "batch 3497: loss 0.025404376909136772\n",
      "batch 3498: loss 0.01100183930248022\n",
      "batch 3499: loss 0.15628835558891296\n",
      "batch 3500: loss 0.00856835674494505\n",
      "batch 3501: loss 0.04980459436774254\n",
      "batch 3502: loss 0.0008175733964890242\n",
      "batch 3503: loss 0.0007455947925336659\n",
      "batch 3504: loss 0.0008072893251664937\n",
      "batch 3505: loss 0.010287674143910408\n",
      "batch 3506: loss 0.023899752646684647\n",
      "batch 3507: loss 0.0002451636246405542\n",
      "batch 3508: loss 0.0019506647950038314\n",
      "batch 3509: loss 0.005500870756804943\n",
      "batch 3510: loss 0.0023348939139395952\n",
      "batch 3511: loss 0.0034872826654464006\n",
      "batch 3512: loss 0.005239003337919712\n",
      "batch 3513: loss 0.03416433185338974\n",
      "batch 3514: loss 0.0457104817032814\n",
      "batch 3515: loss 0.007908541709184647\n",
      "batch 3516: loss 0.0005302340141497552\n",
      "batch 3517: loss 0.000816828163806349\n",
      "batch 3518: loss 0.007928434759378433\n",
      "batch 3519: loss 0.0016703653382137418\n",
      "batch 3520: loss 0.014747876673936844\n",
      "batch 3521: loss 0.0038558568339794874\n",
      "batch 3522: loss 0.00039079494308680296\n",
      "batch 3523: loss 0.02730620838701725\n",
      "batch 3524: loss 0.06743718683719635\n",
      "batch 3525: loss 0.0005752324359491467\n",
      "batch 3526: loss 0.00026928153238259256\n",
      "batch 3527: loss 0.0035462307278066874\n",
      "batch 3528: loss 0.0001253143564099446\n",
      "batch 3529: loss 0.0006557235610671341\n",
      "batch 3530: loss 0.015351623296737671\n",
      "batch 3531: loss 0.05622894689440727\n",
      "batch 3532: loss 0.10144046694040298\n",
      "batch 3533: loss 0.011383827775716782\n",
      "batch 3534: loss 0.028814498335123062\n",
      "batch 3535: loss 0.0006041477899998426\n",
      "batch 3536: loss 0.0010841876501217484\n",
      "batch 3537: loss 0.0002109367778757587\n",
      "batch 3538: loss 0.0018489507492631674\n",
      "batch 3539: loss 0.0004108342691324651\n",
      "batch 3540: loss 0.0014052912592887878\n",
      "batch 3541: loss 0.004715711809694767\n",
      "batch 3542: loss 9.669644350651652e-05\n",
      "batch 3543: loss 0.005854359827935696\n",
      "batch 3544: loss 0.0028639626689255238\n",
      "batch 3545: loss 0.005633352790027857\n",
      "batch 3546: loss 0.02043665573000908\n",
      "batch 3547: loss 0.0007333325920626521\n",
      "batch 3548: loss 0.005923080258071423\n",
      "batch 3549: loss 0.07498842477798462\n",
      "batch 3550: loss 0.0012860138667747378\n",
      "batch 3551: loss 0.0064160567708313465\n",
      "batch 3552: loss 0.01411553006619215\n",
      "batch 3553: loss 0.044382501393556595\n",
      "batch 3554: loss 0.004881464876234531\n",
      "batch 3555: loss 0.0031376539263874292\n",
      "batch 3556: loss 0.014798702672123909\n",
      "batch 3557: loss 0.0018254691967740655\n",
      "batch 3558: loss 0.0007745035691186786\n",
      "batch 3559: loss 0.0004978514625690877\n",
      "batch 3560: loss 0.0007013911381363869\n",
      "batch 3561: loss 0.005093549843877554\n",
      "batch 3562: loss 0.0031536670867353678\n",
      "batch 3563: loss 0.0005390803562477231\n",
      "batch 3564: loss 0.0027008503675460815\n",
      "batch 3565: loss 0.04986349120736122\n",
      "batch 3566: loss 0.05273102596402168\n",
      "batch 3567: loss 0.004708828404545784\n",
      "batch 3568: loss 0.0060906438156962395\n",
      "batch 3569: loss 3.751217809622176e-05\n",
      "batch 3570: loss 0.0032189181074500084\n",
      "batch 3571: loss 0.025735821574926376\n",
      "batch 3572: loss 0.00026450722361914814\n",
      "batch 3573: loss 0.00027847985620610416\n",
      "batch 3574: loss 0.13998040556907654\n",
      "batch 3575: loss 0.08605347573757172\n",
      "batch 3576: loss 0.008231086656451225\n",
      "batch 3577: loss 0.00040892220567911863\n",
      "batch 3578: loss 0.14537616074085236\n",
      "batch 3579: loss 0.01796666532754898\n",
      "batch 3580: loss 0.001012090127915144\n",
      "batch 3581: loss 0.04825068265199661\n",
      "batch 3582: loss 0.0017259320011362433\n",
      "batch 3583: loss 0.004801857750862837\n",
      "batch 3584: loss 0.011257405392825603\n",
      "batch 3585: loss 0.014111101627349854\n",
      "batch 3586: loss 0.10900655388832092\n",
      "batch 3587: loss 0.00022390800586435944\n",
      "batch 3588: loss 0.1321224719285965\n",
      "batch 3589: loss 0.03688453882932663\n",
      "batch 3590: loss 0.0015537583967670798\n",
      "batch 3591: loss 0.0007636526715941727\n",
      "batch 3592: loss 0.0018078509019687772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3593: loss 0.0021010059863328934\n",
      "batch 3594: loss 0.05655304715037346\n",
      "batch 3595: loss 0.0026339865289628506\n",
      "batch 3596: loss 0.0008414335316047072\n",
      "batch 3597: loss 0.0003539471363183111\n",
      "batch 3598: loss 0.006978300865739584\n",
      "batch 3599: loss 0.0035001083742827177\n",
      "batch 3600: loss 0.002494028303772211\n",
      "batch 3601: loss 0.08547590672969818\n",
      "batch 3602: loss 0.03396935015916824\n",
      "batch 3603: loss 0.011041020974516869\n",
      "batch 3604: loss 0.02387191914021969\n",
      "batch 3605: loss 0.02396511659026146\n",
      "batch 3606: loss 0.03162747249007225\n",
      "batch 3607: loss 0.01154734380543232\n",
      "batch 3608: loss 0.0003075836575590074\n",
      "batch 3609: loss 0.008577217347919941\n",
      "batch 3610: loss 0.005710927303880453\n",
      "batch 3611: loss 0.03751424700021744\n",
      "batch 3612: loss 0.051220331341028214\n",
      "batch 3613: loss 0.02788025327026844\n",
      "batch 3614: loss 0.0008548146579414606\n",
      "batch 3615: loss 0.02738010324537754\n",
      "batch 3616: loss 0.0046152835711836815\n",
      "batch 3617: loss 0.001076526241376996\n",
      "batch 3618: loss 0.000529996061231941\n",
      "batch 3619: loss 0.02369762770831585\n",
      "batch 3620: loss 0.00035222183214500546\n",
      "batch 3621: loss 0.0024863022845238447\n",
      "batch 3622: loss 0.0013239687541499734\n",
      "batch 3623: loss 0.030508587136864662\n",
      "batch 3624: loss 0.01406056061387062\n",
      "batch 3625: loss 0.0030992692336440086\n",
      "batch 3626: loss 0.026103395968675613\n",
      "batch 3627: loss 0.0749061182141304\n",
      "batch 3628: loss 0.0032383655197918415\n",
      "batch 3629: loss 0.0015689225401729345\n",
      "batch 3630: loss 0.010491225868463516\n",
      "batch 3631: loss 0.00023335914011113346\n",
      "batch 3632: loss 0.01421902235597372\n",
      "batch 3633: loss 0.009934090077877045\n",
      "batch 3634: loss 0.00037737429374828935\n",
      "batch 3635: loss 0.0059192972257733345\n",
      "batch 3636: loss 0.011203872039914131\n",
      "batch 3637: loss 0.00012920786684844643\n",
      "batch 3638: loss 0.0011293067364022136\n",
      "batch 3639: loss 0.005749939475208521\n",
      "batch 3640: loss 0.07645491510629654\n",
      "batch 3641: loss 0.0013225276488810778\n",
      "batch 3642: loss 0.009337875992059708\n",
      "batch 3643: loss 0.001889393082819879\n",
      "batch 3644: loss 0.0007051674765534699\n",
      "batch 3645: loss 0.00277101737447083\n",
      "batch 3646: loss 0.016162816435098648\n",
      "batch 3647: loss 0.0004834392457269132\n",
      "batch 3648: loss 0.00017586549802217633\n",
      "batch 3649: loss 0.0006230964208953083\n",
      "batch 3650: loss 0.002307032933458686\n",
      "batch 3651: loss 0.013267065398395061\n",
      "batch 3652: loss 0.0002918245445471257\n",
      "batch 3653: loss 0.15306536853313446\n",
      "batch 3654: loss 0.0007431108970195055\n",
      "batch 3655: loss 0.02914014831185341\n",
      "batch 3656: loss 0.0008954517543315887\n",
      "batch 3657: loss 0.002134673995897174\n",
      "batch 3658: loss 0.00022477064339909703\n",
      "batch 3659: loss 0.00350314867682755\n",
      "batch 3660: loss 0.01815376989543438\n",
      "batch 3661: loss 0.01165689341723919\n",
      "batch 3662: loss 0.0019366985652595758\n",
      "batch 3663: loss 0.000518892309628427\n",
      "batch 3664: loss 0.008393692784011364\n",
      "batch 3665: loss 0.0013674965593963861\n",
      "batch 3666: loss 0.0020713433623313904\n",
      "batch 3667: loss 0.0015590086113661528\n",
      "batch 3668: loss 0.020822571590542793\n",
      "batch 3669: loss 0.0006226932164281607\n",
      "batch 3670: loss 0.0004008790710940957\n",
      "batch 3671: loss 0.011234872974455357\n",
      "batch 3672: loss 0.0013284323504194617\n",
      "batch 3673: loss 0.00034811015939339995\n",
      "batch 3674: loss 0.03907793387770653\n",
      "batch 3675: loss 0.0028252655174583197\n",
      "batch 3676: loss 0.003212848212569952\n",
      "batch 3677: loss 0.004420673009008169\n",
      "batch 3678: loss 0.006839436013251543\n",
      "batch 3679: loss 1.7160511561087333e-05\n",
      "batch 3680: loss 0.0003300771059002727\n",
      "batch 3681: loss 0.00206386880017817\n",
      "batch 3682: loss 0.15098415315151215\n",
      "batch 3683: loss 0.0008725005900487304\n",
      "batch 3684: loss 0.027129584923386574\n",
      "batch 3685: loss 0.0037700836546719074\n",
      "batch 3686: loss 0.0005289995460771024\n",
      "batch 3687: loss 0.055628400295972824\n",
      "batch 3688: loss 0.012881661765277386\n",
      "batch 3689: loss 0.001390135264955461\n",
      "batch 3690: loss 0.00021565990755334496\n",
      "batch 3691: loss 0.0006550624966621399\n",
      "batch 3692: loss 0.004193529020994902\n",
      "batch 3693: loss 0.08723432570695877\n",
      "batch 3694: loss 0.005698512773960829\n",
      "batch 3695: loss 0.01455951202660799\n",
      "batch 3696: loss 0.003823340404778719\n",
      "batch 3697: loss 0.09487680345773697\n",
      "batch 3698: loss 0.02209128998219967\n",
      "batch 3699: loss 0.0056501696817576885\n",
      "batch 3700: loss 0.05763357877731323\n",
      "batch 3701: loss 0.0013353204121813178\n",
      "batch 3702: loss 0.006469848565757275\n",
      "batch 3703: loss 0.1609111875295639\n",
      "batch 3704: loss 0.0014439435908570886\n",
      "batch 3705: loss 0.0027643826324492693\n",
      "batch 3706: loss 0.0006273832987062633\n",
      "batch 3707: loss 0.10796277970075607\n",
      "batch 3708: loss 0.05491219460964203\n",
      "batch 3709: loss 0.009733447805047035\n",
      "batch 3710: loss 0.03520312160253525\n",
      "batch 3711: loss 0.015170945785939693\n",
      "batch 3712: loss 0.02500041015446186\n",
      "batch 3713: loss 0.00019422742479946464\n",
      "batch 3714: loss 0.023509357124567032\n",
      "batch 3715: loss 0.003750645788386464\n",
      "batch 3716: loss 0.005585445556789637\n",
      "batch 3717: loss 0.006186855491250753\n",
      "batch 3718: loss 0.0668981596827507\n",
      "batch 3719: loss 0.0066685667261481285\n",
      "batch 3720: loss 0.02843100018799305\n",
      "batch 3721: loss 0.005744614638388157\n",
      "batch 3722: loss 0.06379086524248123\n",
      "batch 3723: loss 0.000564996269531548\n",
      "batch 3724: loss 0.06060847267508507\n",
      "batch 3725: loss 0.00023047042486723512\n",
      "batch 3726: loss 0.0030944400932639837\n",
      "batch 3727: loss 0.0074348412454128265\n",
      "batch 3728: loss 0.03049289621412754\n",
      "batch 3729: loss 0.004607085138559341\n",
      "batch 3730: loss 0.03828975558280945\n",
      "batch 3731: loss 0.10455073416233063\n",
      "batch 3732: loss 0.07887260615825653\n",
      "batch 3733: loss 0.0028567176777869463\n",
      "batch 3734: loss 0.005076877307146788\n",
      "batch 3735: loss 0.0005763000226579607\n",
      "batch 3736: loss 0.0010487644467502832\n",
      "batch 3737: loss 0.005887629464268684\n",
      "batch 3738: loss 0.0019187646685168147\n",
      "batch 3739: loss 0.03556918725371361\n",
      "batch 3740: loss 0.0038891977164894342\n",
      "batch 3741: loss 0.0036578329745680094\n",
      "batch 3742: loss 0.00757124088704586\n",
      "batch 3743: loss 0.021811621263623238\n",
      "batch 3744: loss 0.04456155747175217\n",
      "batch 3745: loss 0.005628532264381647\n",
      "batch 3746: loss 0.003193197539076209\n",
      "batch 3747: loss 0.005834505893290043\n",
      "batch 3748: loss 0.000434477929957211\n",
      "batch 3749: loss 0.00018534311675466597\n",
      "batch 3750: loss 0.0006262677488848567\n",
      "batch 3751: loss 0.021685926243662834\n",
      "batch 3752: loss 0.0011059952666983008\n",
      "batch 3753: loss 0.0006257357308641076\n",
      "batch 3754: loss 0.00016451548435725272\n",
      "batch 3755: loss 0.06027444824576378\n",
      "batch 3756: loss 0.0005336747854016721\n",
      "batch 3757: loss 0.0033537778072059155\n",
      "batch 3758: loss 0.0020931512117385864\n",
      "batch 3759: loss 0.005672120489180088\n",
      "batch 3760: loss 0.001931274775415659\n",
      "batch 3761: loss 0.0019735663663595915\n",
      "batch 3762: loss 0.0033777980133891106\n",
      "batch 3763: loss 0.013044153340160847\n",
      "batch 3764: loss 0.00043732699123211205\n",
      "batch 3765: loss 0.0014493560884147882\n",
      "batch 3766: loss 0.01929248310625553\n",
      "batch 3767: loss 0.0007433835999108851\n",
      "batch 3768: loss 0.004344118293374777\n",
      "batch 3769: loss 0.002357742516323924\n",
      "batch 3770: loss 0.0010178129887208343\n",
      "batch 3771: loss 0.025155620649456978\n",
      "batch 3772: loss 0.030621284618973732\n",
      "batch 3773: loss 0.13474096357822418\n",
      "batch 3774: loss 0.009020315483212471\n",
      "batch 3775: loss 0.07543472945690155\n",
      "batch 3776: loss 0.012017335742712021\n",
      "batch 3777: loss 0.01644272729754448\n",
      "batch 3778: loss 0.007555938325822353\n",
      "batch 3779: loss 0.0018536525312811136\n",
      "batch 3780: loss 0.024031182751059532\n",
      "batch 3781: loss 0.007140904664993286\n",
      "batch 3782: loss 0.009517291560769081\n",
      "batch 3783: loss 0.003865208476781845\n",
      "batch 3784: loss 0.03915191441774368\n",
      "batch 3785: loss 0.001443963497877121\n",
      "batch 3786: loss 0.0030951686203479767\n",
      "batch 3787: loss 0.0027342820540070534\n",
      "batch 3788: loss 0.03602633997797966\n",
      "batch 3789: loss 0.005804764572530985\n",
      "batch 3790: loss 0.004040224477648735\n",
      "batch 3791: loss 0.0023156460374593735\n",
      "batch 3792: loss 0.0001016327369143255\n",
      "batch 3793: loss 0.004811184946447611\n",
      "batch 3794: loss 0.0023738271556794643\n",
      "batch 3795: loss 0.00044517184142023325\n",
      "batch 3796: loss 0.004426857922226191\n",
      "batch 3797: loss 7.639270188519731e-05\n",
      "batch 3798: loss 0.009271509014070034\n",
      "batch 3799: loss 0.0007995188934728503\n",
      "batch 3800: loss 0.013903934508562088\n",
      "batch 3801: loss 0.0013333968818187714\n",
      "batch 3802: loss 0.01894928514957428\n",
      "batch 3803: loss 0.000289925723336637\n",
      "batch 3804: loss 0.0029429467394948006\n",
      "batch 3805: loss 0.02395014464855194\n",
      "batch 3806: loss 0.000405688100727275\n",
      "batch 3807: loss 0.0034350967034697533\n",
      "batch 3808: loss 0.0005529736517928541\n",
      "batch 3809: loss 0.0018502017483115196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 3810: loss 0.0005432068719528615\n",
      "batch 3811: loss 0.00632281182333827\n",
      "batch 3812: loss 0.0004385382926557213\n",
      "batch 3813: loss 0.0016415471909567714\n",
      "batch 3814: loss 0.00047964489203877747\n",
      "batch 3815: loss 0.0012862649746239185\n",
      "batch 3816: loss 0.001083501847460866\n",
      "batch 3817: loss 0.0001757167628966272\n",
      "batch 3818: loss 0.0009980364702641964\n",
      "batch 3819: loss 0.12003922462463379\n",
      "batch 3820: loss 0.0010693259537220001\n",
      "batch 3821: loss 0.0007497642654925585\n",
      "batch 3822: loss 0.0018446905305609107\n",
      "batch 3823: loss 0.0008813255117274821\n",
      "batch 3824: loss 0.00031357762054540217\n",
      "batch 3825: loss 0.0001505924155935645\n",
      "batch 3826: loss 0.037067607045173645\n",
      "batch 3827: loss 0.001126953400671482\n",
      "batch 3828: loss 0.00018330216698814183\n",
      "batch 3829: loss 0.08091722428798676\n",
      "batch 3830: loss 0.042991165071725845\n",
      "batch 3831: loss 0.017506826668977737\n",
      "batch 3832: loss 0.005834705196321011\n",
      "batch 3833: loss 0.0407717302441597\n",
      "batch 3834: loss 0.008255853317677975\n",
      "batch 3835: loss 0.041178327053785324\n",
      "batch 3836: loss 0.1613157093524933\n",
      "batch 3837: loss 0.3085683286190033\n",
      "batch 3838: loss 0.0008344596717506647\n",
      "batch 3839: loss 0.006765037775039673\n",
      "batch 3840: loss 0.009861075319349766\n",
      "batch 3841: loss 0.0014308387180790305\n",
      "batch 3842: loss 0.02261333353817463\n",
      "batch 3843: loss 0.013901323080062866\n",
      "batch 3844: loss 0.15836717188358307\n",
      "batch 3845: loss 0.08149044960737228\n",
      "batch 3846: loss 0.006012179888784885\n",
      "batch 3847: loss 0.007950490340590477\n",
      "batch 3848: loss 0.006015386898070574\n",
      "batch 3849: loss 0.005621378310024738\n",
      "batch 3850: loss 0.03942325711250305\n",
      "batch 3851: loss 0.0550764799118042\n",
      "batch 3852: loss 0.0006326021393761039\n",
      "batch 3853: loss 0.005349442362785339\n",
      "batch 3854: loss 0.0008190806256607175\n",
      "batch 3855: loss 0.05985327064990997\n",
      "batch 3856: loss 0.017389606684446335\n",
      "batch 3857: loss 0.018770761787891388\n",
      "batch 3858: loss 0.0039631095714867115\n",
      "batch 3859: loss 0.005840744823217392\n",
      "batch 3860: loss 0.007029717788100243\n",
      "batch 3861: loss 0.007125936448574066\n",
      "batch 3862: loss 0.0015413423534482718\n",
      "batch 3863: loss 0.0342584028840065\n",
      "batch 3864: loss 0.006269253324717283\n",
      "batch 3865: loss 0.006940479855984449\n",
      "batch 3866: loss 0.007322823163121939\n",
      "batch 3867: loss 0.11480762809515\n",
      "batch 3868: loss 0.00038439943455159664\n",
      "batch 3869: loss 0.007657866925001144\n",
      "batch 3870: loss 0.008604755625128746\n",
      "batch 3871: loss 0.008463730104267597\n",
      "batch 3872: loss 0.1273404061794281\n",
      "batch 3873: loss 0.001187699381262064\n",
      "batch 3874: loss 0.00220690225251019\n",
      "batch 3875: loss 0.009183987975120544\n",
      "batch 3876: loss 0.16496430337429047\n",
      "batch 3877: loss 0.00038826483068987727\n",
      "batch 3878: loss 0.0012660521315410733\n",
      "batch 3879: loss 0.0032082372345030308\n",
      "batch 3880: loss 0.025727877393364906\n",
      "batch 3881: loss 0.0011757181491702795\n",
      "batch 3882: loss 0.003950035665184259\n",
      "batch 3883: loss 0.006182881072163582\n",
      "batch 3884: loss 0.012042403221130371\n",
      "batch 3885: loss 0.0004651813942473382\n",
      "batch 3886: loss 0.014742642641067505\n",
      "batch 3887: loss 0.018926100805401802\n",
      "batch 3888: loss 0.03279063105583191\n",
      "batch 3889: loss 0.0028808042407035828\n",
      "batch 3890: loss 0.04249275475740433\n",
      "batch 3891: loss 0.0026568882167339325\n",
      "batch 3892: loss 0.003998336382210255\n",
      "batch 3893: loss 0.0015416234964504838\n",
      "batch 3894: loss 0.0006729270098730922\n",
      "batch 3895: loss 0.0008870186284184456\n",
      "batch 3896: loss 0.024356182664632797\n",
      "batch 3897: loss 0.046480704098939896\n",
      "batch 3898: loss 0.00829725805670023\n",
      "batch 3899: loss 0.031173912808299065\n",
      "batch 3900: loss 0.00034072803100571036\n",
      "batch 3901: loss 0.010289076715707779\n",
      "batch 3902: loss 0.0764644593000412\n",
      "batch 3903: loss 0.020296448841691017\n",
      "batch 3904: loss 0.005861422512680292\n",
      "batch 3905: loss 0.11583748459815979\n",
      "batch 3906: loss 0.0017994936788454652\n",
      "batch 3907: loss 0.01564565859735012\n",
      "batch 3908: loss 0.0030857189558446407\n",
      "batch 3909: loss 0.011301293037831783\n",
      "batch 3910: loss 0.0033793433103710413\n",
      "batch 3911: loss 0.0012964218622073531\n",
      "batch 3912: loss 0.021718589588999748\n",
      "batch 3913: loss 0.12468791753053665\n",
      "batch 3914: loss 0.09509687125682831\n",
      "batch 3915: loss 0.04760903865098953\n",
      "batch 3916: loss 0.01031637005507946\n",
      "batch 3917: loss 0.0009693011525087059\n",
      "batch 3918: loss 0.0006611738353967667\n",
      "batch 3919: loss 0.0158607829362154\n",
      "batch 3920: loss 0.016699323430657387\n",
      "batch 3921: loss 0.003793382318690419\n",
      "batch 3922: loss 0.032709963619709015\n",
      "batch 3923: loss 0.0029622106812894344\n",
      "batch 3924: loss 0.004533163271844387\n",
      "batch 3925: loss 0.002399288583546877\n",
      "batch 3926: loss 0.0009979107417166233\n",
      "batch 3927: loss 0.002431041793897748\n",
      "batch 3928: loss 0.005414268933236599\n",
      "batch 3929: loss 0.021169334650039673\n",
      "batch 3930: loss 0.09303927421569824\n",
      "batch 3931: loss 0.026599828153848648\n",
      "batch 3932: loss 0.0013868134701624513\n",
      "batch 3933: loss 0.007117004599422216\n",
      "batch 3934: loss 0.003022722201421857\n",
      "batch 3935: loss 0.0026128035970032215\n",
      "batch 3936: loss 0.00654569361358881\n",
      "batch 3937: loss 0.0032585507724434137\n",
      "batch 3938: loss 0.08032767474651337\n",
      "batch 3939: loss 0.0029720570892095566\n",
      "batch 3940: loss 0.00246576895006001\n",
      "batch 3941: loss 0.060807693749666214\n",
      "batch 3942: loss 0.00781982857733965\n",
      "batch 3943: loss 0.00035078570363111794\n",
      "batch 3944: loss 0.010123903863132\n",
      "batch 3945: loss 0.020182188600301743\n",
      "batch 3946: loss 0.0030313122551888227\n",
      "batch 3947: loss 0.026224004104733467\n",
      "batch 3948: loss 0.0005216769059188664\n",
      "batch 3949: loss 0.0002893407072406262\n",
      "batch 3950: loss 0.0012075966224074364\n",
      "batch 3951: loss 0.07155002653598785\n",
      "batch 3952: loss 0.013482975773513317\n",
      "batch 3953: loss 0.0007750754011794925\n",
      "batch 3954: loss 0.005444975569844246\n",
      "batch 3955: loss 0.0028876890428364277\n",
      "batch 3956: loss 0.007337015587836504\n",
      "batch 3957: loss 0.12015596032142639\n",
      "batch 3958: loss 0.006749731954187155\n",
      "batch 3959: loss 0.014961330220103264\n",
      "batch 3960: loss 0.006519756279885769\n",
      "batch 3961: loss 0.003299552947282791\n",
      "batch 3962: loss 0.012671027332544327\n",
      "batch 3963: loss 0.0008605970069766045\n",
      "batch 3964: loss 7.25537320249714e-05\n",
      "batch 3965: loss 0.010563748888671398\n",
      "batch 3966: loss 0.0002340152277611196\n",
      "batch 3967: loss 0.09716049581766129\n",
      "batch 3968: loss 0.027738265693187714\n",
      "batch 3969: loss 0.000835439539514482\n",
      "batch 3970: loss 0.0338742695748806\n",
      "batch 3971: loss 0.03533310815691948\n",
      "batch 3972: loss 0.0019548756536096334\n",
      "batch 3973: loss 0.047704506665468216\n",
      "batch 3974: loss 0.16026799380779266\n",
      "batch 3975: loss 0.005220297258347273\n",
      "batch 3976: loss 7.132090104278177e-05\n",
      "batch 3977: loss 0.046370189636945724\n",
      "batch 3978: loss 0.016375338658690453\n",
      "batch 3979: loss 0.012251172214746475\n",
      "batch 3980: loss 0.002217057393863797\n",
      "batch 3981: loss 0.005966295022517443\n",
      "batch 3982: loss 0.0020850650034844875\n",
      "batch 3983: loss 0.043927934020757675\n",
      "batch 3984: loss 0.0020606431644409895\n",
      "batch 3985: loss 0.00047069447464309633\n",
      "batch 3986: loss 0.004329816438257694\n",
      "batch 3987: loss 0.0006109647802077234\n",
      "batch 3988: loss 0.0001964435650734231\n",
      "batch 3989: loss 0.0012119775637984276\n",
      "batch 3990: loss 0.005828244611620903\n",
      "batch 3991: loss 0.00016590037557762116\n",
      "batch 3992: loss 0.010844755917787552\n",
      "batch 3993: loss 0.0012040699366480112\n",
      "batch 3994: loss 0.003264583880081773\n",
      "batch 3995: loss 0.0020143985748291016\n",
      "batch 3996: loss 0.00012853693624492735\n",
      "batch 3997: loss 0.007903004996478558\n",
      "batch 3998: loss 0.05556651949882507\n",
      "batch 3999: loss 0.013039331883192062\n",
      "batch 4000: loss 0.0020634408574551344\n",
      "batch 4001: loss 0.06410012394189835\n",
      "batch 4002: loss 0.04623093083500862\n",
      "batch 4003: loss 0.0030121589079499245\n",
      "batch 4004: loss 0.040186990052461624\n",
      "batch 4005: loss 0.003377774963155389\n",
      "batch 4006: loss 0.012171449139714241\n",
      "batch 4007: loss 0.0014467199798673391\n",
      "batch 4008: loss 0.00472725136205554\n",
      "batch 4009: loss 0.01169754471629858\n",
      "batch 4010: loss 0.004464212339371443\n",
      "batch 4011: loss 0.006524435244500637\n",
      "batch 4012: loss 0.005190371535718441\n",
      "batch 4013: loss 0.0011742771603167057\n",
      "batch 4014: loss 0.0038313933182507753\n",
      "batch 4015: loss 0.0010448792017996311\n",
      "batch 4016: loss 0.08273628354072571\n",
      "batch 4017: loss 0.003455071710050106\n",
      "batch 4018: loss 0.029935019090771675\n",
      "batch 4019: loss 0.024673214182257652\n",
      "batch 4020: loss 0.0048758722841739655\n",
      "batch 4021: loss 0.008075529709458351\n",
      "batch 4022: loss 0.008198176510632038\n",
      "batch 4023: loss 0.007896552793681622\n",
      "batch 4024: loss 0.007985124364495277\n",
      "batch 4025: loss 0.00014697820006404072\n",
      "batch 4026: loss 0.0010256909299641848\n",
      "batch 4027: loss 0.08580482006072998\n",
      "batch 4028: loss 0.0014436693163588643\n",
      "batch 4029: loss 0.0006743251578882337\n",
      "batch 4030: loss 0.0006144384969957173\n",
      "batch 4031: loss 0.002542827744036913\n",
      "batch 4032: loss 0.004629218485206366\n",
      "batch 4033: loss 0.07042282819747925\n",
      "batch 4034: loss 0.00042764938552863896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4035: loss 0.053899697959423065\n",
      "batch 4036: loss 0.0028811448719352484\n",
      "batch 4037: loss 0.17350663244724274\n",
      "batch 4038: loss 0.0005698350141756237\n",
      "batch 4039: loss 0.01272361259907484\n",
      "batch 4040: loss 0.0006807405152358115\n",
      "batch 4041: loss 0.00012563828204292804\n",
      "batch 4042: loss 0.0026169398333877325\n",
      "batch 4043: loss 0.0006700382800772786\n",
      "batch 4044: loss 0.006110553629696369\n",
      "batch 4045: loss 0.009787586517632008\n",
      "batch 4046: loss 0.0574147067964077\n",
      "batch 4047: loss 0.007923167198896408\n",
      "batch 4048: loss 0.005003827624022961\n",
      "batch 4049: loss 0.01898040436208248\n",
      "batch 4050: loss 0.00010888429824262857\n",
      "batch 4051: loss 0.016512153670191765\n",
      "batch 4052: loss 0.0031374781392514706\n",
      "batch 4053: loss 0.00041253812378272414\n",
      "batch 4054: loss 0.17275220155715942\n",
      "batch 4055: loss 0.0024490596260875463\n",
      "batch 4056: loss 0.0005864700651727617\n",
      "batch 4057: loss 0.011837005615234375\n",
      "batch 4058: loss 0.00544837536290288\n",
      "batch 4059: loss 0.006083163898438215\n",
      "batch 4060: loss 0.1276850551366806\n",
      "batch 4061: loss 0.001624568714760244\n",
      "batch 4062: loss 0.044357042759656906\n",
      "batch 4063: loss 0.00384341599419713\n",
      "batch 4064: loss 0.0009020093129947782\n",
      "batch 4065: loss 0.002387586748227477\n",
      "batch 4066: loss 0.0009428052580915391\n",
      "batch 4067: loss 0.014395451173186302\n",
      "batch 4068: loss 0.11978141218423843\n",
      "batch 4069: loss 0.04813903570175171\n",
      "batch 4070: loss 0.031501416116952896\n",
      "batch 4071: loss 0.026543017476797104\n",
      "batch 4072: loss 0.003738635452464223\n",
      "batch 4073: loss 0.0004966748529113829\n",
      "batch 4074: loss 0.0015639878110960126\n",
      "batch 4075: loss 0.08955245465040207\n",
      "batch 4076: loss 0.029006117954850197\n",
      "batch 4077: loss 0.0032202647998929024\n",
      "batch 4078: loss 0.1268738955259323\n",
      "batch 4079: loss 0.002571757882833481\n",
      "batch 4080: loss 0.00028774759266525507\n",
      "batch 4081: loss 0.01206684485077858\n",
      "batch 4082: loss 0.00036365771666169167\n",
      "batch 4083: loss 0.0007276080432347953\n",
      "batch 4084: loss 0.00503425532951951\n",
      "batch 4085: loss 0.03514793515205383\n",
      "batch 4086: loss 0.009017719887197018\n",
      "batch 4087: loss 0.03517044335603714\n",
      "batch 4088: loss 0.0012100854655727744\n",
      "batch 4089: loss 0.059054888784885406\n",
      "batch 4090: loss 0.004842827096581459\n",
      "batch 4091: loss 0.014989614486694336\n",
      "batch 4092: loss 0.0024906157050281763\n",
      "batch 4093: loss 0.003837126074358821\n",
      "batch 4094: loss 0.026902539655566216\n",
      "batch 4095: loss 0.013052430003881454\n",
      "batch 4096: loss 0.0044862935319542885\n",
      "batch 4097: loss 0.014441508799791336\n",
      "batch 4098: loss 0.0007176048820838332\n",
      "batch 4099: loss 0.016901111230254173\n",
      "batch 4100: loss 0.011424371041357517\n",
      "batch 4101: loss 0.0002874319616239518\n",
      "batch 4102: loss 0.0028800044674426317\n",
      "batch 4103: loss 0.0551055409014225\n",
      "batch 4104: loss 0.00031202746322378516\n",
      "batch 4105: loss 0.0006550575490109622\n",
      "batch 4106: loss 0.0016993052558973432\n",
      "batch 4107: loss 0.0034363442100584507\n",
      "batch 4108: loss 0.00701371394097805\n",
      "batch 4109: loss 0.004928811453282833\n",
      "batch 4110: loss 0.0009221791988238692\n",
      "batch 4111: loss 0.01262167189270258\n",
      "batch 4112: loss 0.00010360097076045349\n",
      "batch 4113: loss 0.0474218986928463\n",
      "batch 4114: loss 0.004531606566160917\n",
      "batch 4115: loss 0.009447320364415646\n",
      "batch 4116: loss 0.015492577105760574\n",
      "batch 4117: loss 0.0001768306246958673\n",
      "batch 4118: loss 0.003295918693765998\n",
      "batch 4119: loss 0.0015773872146382928\n",
      "batch 4120: loss 0.033817827701568604\n",
      "batch 4121: loss 0.002396198222413659\n",
      "batch 4122: loss 0.0032066276762634516\n",
      "batch 4123: loss 0.0009592550923116505\n",
      "batch 4124: loss 0.0005530249327421188\n",
      "batch 4125: loss 0.013835771009325981\n",
      "batch 4126: loss 0.0011524397414177656\n",
      "batch 4127: loss 0.0008929993491619825\n",
      "batch 4128: loss 0.0009957823203876615\n",
      "batch 4129: loss 0.016497038304805756\n",
      "batch 4130: loss 0.0019562833476811647\n",
      "batch 4131: loss 0.0020887188147753477\n",
      "batch 4132: loss 0.003035806119441986\n",
      "batch 4133: loss 0.004740413744002581\n",
      "batch 4134: loss 0.0028828883077949286\n",
      "batch 4135: loss 0.05086711421608925\n",
      "batch 4136: loss 0.0026739095337688923\n",
      "batch 4137: loss 0.00862607266753912\n",
      "batch 4138: loss 0.0001533291069790721\n",
      "batch 4139: loss 0.0018737380160018802\n",
      "batch 4140: loss 6.545419455505908e-05\n",
      "batch 4141: loss 0.00010640377149684355\n",
      "batch 4142: loss 0.010298904962837696\n",
      "batch 4143: loss 0.0007148136501200497\n",
      "batch 4144: loss 0.0012223509838804603\n",
      "batch 4145: loss 0.006675899960100651\n",
      "batch 4146: loss 0.003866876009851694\n",
      "batch 4147: loss 0.001564068952575326\n",
      "batch 4148: loss 0.00012256280751898885\n",
      "batch 4149: loss 0.02807033061981201\n",
      "batch 4150: loss 0.0005843117833137512\n",
      "batch 4151: loss 0.0004230121849104762\n",
      "batch 4152: loss 0.004870352800935507\n",
      "batch 4153: loss 0.04705201834440231\n",
      "batch 4154: loss 0.030865205451846123\n",
      "batch 4155: loss 0.0016378372674807906\n",
      "batch 4156: loss 0.0015599936014041305\n",
      "batch 4157: loss 0.001976648112758994\n",
      "batch 4158: loss 8.845496631693095e-05\n",
      "batch 4159: loss 0.007113246712833643\n",
      "batch 4160: loss 0.00031645476701669395\n",
      "batch 4161: loss 0.007365374360233545\n",
      "batch 4162: loss 0.10834194719791412\n",
      "batch 4163: loss 0.0193487536162138\n",
      "batch 4164: loss 0.003698148066177964\n",
      "batch 4165: loss 0.0034463228657841682\n",
      "batch 4166: loss 0.0007504518143832684\n",
      "batch 4167: loss 0.031791456043720245\n",
      "batch 4168: loss 0.0006457996787503362\n",
      "batch 4169: loss 0.00044283835450187325\n",
      "batch 4170: loss 8.976381650427356e-05\n",
      "batch 4171: loss 0.0014277601148933172\n",
      "batch 4172: loss 0.027417723089456558\n",
      "batch 4173: loss 0.0007988215074874461\n",
      "batch 4174: loss 0.0029755046125501394\n",
      "batch 4175: loss 0.0010884514776989818\n",
      "batch 4176: loss 0.0005023746634833515\n",
      "batch 4177: loss 0.0003518681915011257\n",
      "batch 4178: loss 0.01924281008541584\n",
      "batch 4179: loss 0.014296137727797031\n",
      "batch 4180: loss 0.040091801434755325\n",
      "batch 4181: loss 0.0022125046234577894\n",
      "batch 4182: loss 0.00020278786541894078\n",
      "batch 4183: loss 0.002633120631799102\n",
      "batch 4184: loss 0.0017875886987894773\n",
      "batch 4185: loss 0.0017291115364059806\n",
      "batch 4186: loss 0.0003079802554566413\n",
      "batch 4187: loss 0.007487764116376638\n",
      "batch 4188: loss 0.002790725091472268\n",
      "batch 4189: loss 0.000175842345925048\n",
      "batch 4190: loss 0.001061464543454349\n",
      "batch 4191: loss 0.0005082082352600992\n",
      "batch 4192: loss 0.028252005577087402\n",
      "batch 4193: loss 0.005025258753448725\n",
      "batch 4194: loss 0.00033407803857699037\n",
      "batch 4195: loss 0.04543038830161095\n",
      "batch 4196: loss 0.0011097349924966693\n",
      "batch 4197: loss 0.03248140588402748\n",
      "batch 4198: loss 0.0012188045075163245\n",
      "batch 4199: loss 0.0011177501874044538\n",
      "batch 4200: loss 0.013370972126722336\n",
      "batch 4201: loss 0.0010764552280306816\n",
      "batch 4202: loss 0.19005483388900757\n",
      "batch 4203: loss 0.0006738161901012063\n",
      "batch 4204: loss 0.00015521141176577657\n",
      "batch 4205: loss 0.00014110488700680435\n",
      "batch 4206: loss 0.0011240250896662474\n",
      "batch 4207: loss 0.006201563868671656\n",
      "batch 4208: loss 0.0002964485320262611\n",
      "batch 4209: loss 0.0006580570479854941\n",
      "batch 4210: loss 0.13599854707717896\n",
      "batch 4211: loss 0.004025106318295002\n",
      "batch 4212: loss 0.0010486090322956443\n",
      "batch 4213: loss 0.0015877305995672941\n",
      "batch 4214: loss 0.0003663284005597234\n",
      "batch 4215: loss 0.0004067904083058238\n",
      "batch 4216: loss 3.807736356975511e-05\n",
      "batch 4217: loss 0.015671715140342712\n",
      "batch 4218: loss 0.0189058855175972\n",
      "batch 4219: loss 0.01334819570183754\n",
      "batch 4220: loss 0.010110562667250633\n",
      "batch 4221: loss 0.08432913571596146\n",
      "batch 4222: loss 0.0015473543899133801\n",
      "batch 4223: loss 0.0006025778711773455\n",
      "batch 4224: loss 0.005353036802262068\n",
      "batch 4225: loss 0.0028219011146575212\n",
      "batch 4226: loss 0.012466685846447945\n",
      "batch 4227: loss 0.002471429295837879\n",
      "batch 4228: loss 0.004687736742198467\n",
      "batch 4229: loss 0.0008598797721788287\n",
      "batch 4230: loss 0.0003238798526581377\n",
      "batch 4231: loss 0.01564646326005459\n",
      "batch 4232: loss 3.23015519825276e-05\n",
      "batch 4233: loss 0.0013759792782366276\n",
      "batch 4234: loss 0.002384215360507369\n",
      "batch 4235: loss 0.049010299146175385\n",
      "batch 4236: loss 0.0001630727929295972\n",
      "batch 4237: loss 0.0011554755037650466\n",
      "batch 4238: loss 0.002151702530682087\n",
      "batch 4239: loss 0.0032791465055197477\n",
      "batch 4240: loss 0.004540216643363237\n",
      "batch 4241: loss 0.03119775280356407\n",
      "batch 4242: loss 0.011156981810927391\n",
      "batch 4243: loss 0.0017993366345763206\n",
      "batch 4244: loss 0.00011350495333317667\n",
      "batch 4245: loss 0.04833865910768509\n",
      "batch 4246: loss 0.0006873459205962718\n",
      "batch 4247: loss 0.0003335557703394443\n",
      "batch 4248: loss 0.0008221163880079985\n",
      "batch 4249: loss 0.03595949709415436\n",
      "batch 4250: loss 0.007400364615023136\n",
      "batch 4251: loss 0.11134202778339386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4252: loss 0.016892289742827415\n",
      "batch 4253: loss 0.007373171858489513\n",
      "batch 4254: loss 0.0010740772122517228\n",
      "batch 4255: loss 0.0001505676336819306\n",
      "batch 4256: loss 0.0005029315943829715\n",
      "batch 4257: loss 0.0014318764442577958\n",
      "batch 4258: loss 1.5878100384725258e-05\n",
      "batch 4259: loss 0.006194074172526598\n",
      "batch 4260: loss 0.002606170019134879\n",
      "batch 4261: loss 0.0011637782445177436\n",
      "batch 4262: loss 0.03324061632156372\n",
      "batch 4263: loss 0.0014114141231402755\n",
      "batch 4264: loss 0.0017034667544066906\n",
      "batch 4265: loss 0.00034892818075604737\n",
      "batch 4266: loss 0.00024512893287464976\n",
      "batch 4267: loss 0.0017158165574073792\n",
      "batch 4268: loss 0.10863453894853592\n",
      "batch 4269: loss 0.0041705346666276455\n",
      "batch 4270: loss 0.0005240631871856749\n",
      "batch 4271: loss 0.013009601272642612\n",
      "batch 4272: loss 0.007578890770673752\n",
      "batch 4273: loss 0.00010145755368284881\n",
      "batch 4274: loss 0.0006847133045084774\n",
      "batch 4275: loss 0.0009228931157849729\n",
      "batch 4276: loss 0.06837866455316544\n",
      "batch 4277: loss 0.014485948719084263\n",
      "batch 4278: loss 0.005213363096117973\n",
      "batch 4279: loss 0.00237751891836524\n",
      "batch 4280: loss 0.010563431307673454\n",
      "batch 4281: loss 0.0024151718243956566\n",
      "batch 4282: loss 0.04849150404334068\n",
      "batch 4283: loss 0.06504441052675247\n",
      "batch 4284: loss 0.0031293241772800684\n",
      "batch 4285: loss 0.000716563721653074\n",
      "batch 4286: loss 0.023449357599020004\n",
      "batch 4287: loss 0.009132868610322475\n",
      "batch 4288: loss 0.05588068068027496\n",
      "batch 4289: loss 0.0013090063584968448\n",
      "batch 4290: loss 0.001030346262268722\n",
      "batch 4291: loss 0.0001370428071822971\n",
      "batch 4292: loss 0.0006399991107173264\n",
      "batch 4293: loss 0.03566518798470497\n",
      "batch 4294: loss 0.006464774254709482\n",
      "batch 4295: loss 0.0009507971699349582\n",
      "batch 4296: loss 0.00013787239731755108\n",
      "batch 4297: loss 0.033102501183748245\n",
      "batch 4298: loss 0.0004950867150910199\n",
      "batch 4299: loss 0.06430964916944504\n",
      "batch 4300: loss 0.01146458275616169\n",
      "batch 4301: loss 0.0005406023701652884\n",
      "batch 4302: loss 0.0015376753872260451\n",
      "batch 4303: loss 0.05023352801799774\n",
      "batch 4304: loss 0.003508903319016099\n",
      "batch 4305: loss 0.0007596798241138458\n",
      "batch 4306: loss 0.00040706139407120645\n",
      "batch 4307: loss 0.0009942668257281184\n",
      "batch 4308: loss 0.0017537944950163364\n",
      "batch 4309: loss 0.0015659163473173976\n",
      "batch 4310: loss 0.016321411356329918\n",
      "batch 4311: loss 0.000715246656909585\n",
      "batch 4312: loss 0.002526903757825494\n",
      "batch 4313: loss 0.0031427566427737474\n",
      "batch 4314: loss 0.04806516692042351\n",
      "batch 4315: loss 0.026185786351561546\n",
      "batch 4316: loss 0.005099565256386995\n",
      "batch 4317: loss 0.0572231262922287\n",
      "batch 4318: loss 0.00021322397515177727\n",
      "batch 4319: loss 0.000828872318379581\n",
      "batch 4320: loss 0.0006341804401017725\n",
      "batch 4321: loss 0.009574783965945244\n",
      "batch 4322: loss 0.0007139691733755171\n",
      "batch 4323: loss 0.03640863671898842\n",
      "batch 4324: loss 2.3770879124640487e-05\n",
      "batch 4325: loss 0.0006044502952136099\n",
      "batch 4326: loss 0.030796360224485397\n",
      "batch 4327: loss 0.0012298652436584234\n",
      "batch 4328: loss 0.0015761905815452337\n",
      "batch 4329: loss 0.04160470515489578\n",
      "batch 4330: loss 0.021750696003437042\n",
      "batch 4331: loss 0.00042944742017425597\n",
      "batch 4332: loss 0.001925938529893756\n",
      "batch 4333: loss 0.001387881115078926\n",
      "batch 4334: loss 0.00018285762052983046\n",
      "batch 4335: loss 0.057385772466659546\n",
      "batch 4336: loss 0.002097816439345479\n",
      "batch 4337: loss 0.00023285842326004058\n",
      "batch 4338: loss 0.012722235172986984\n",
      "batch 4339: loss 0.005290319677442312\n",
      "batch 4340: loss 0.0029322216287255287\n",
      "batch 4341: loss 0.0464969202876091\n",
      "batch 4342: loss 0.11570657789707184\n",
      "batch 4343: loss 0.0008679840248078108\n",
      "batch 4344: loss 0.13147304952144623\n",
      "batch 4345: loss 0.002344672102481127\n",
      "batch 4346: loss 0.13189132511615753\n",
      "batch 4347: loss 0.05147620663046837\n",
      "batch 4348: loss 0.003471566364169121\n",
      "batch 4349: loss 0.054120104759931564\n",
      "batch 4350: loss 0.020204905420541763\n",
      "batch 4351: loss 0.012850796803832054\n",
      "batch 4352: loss 0.004397647455334663\n",
      "batch 4353: loss 0.01232279371470213\n",
      "batch 4354: loss 0.005174464546144009\n",
      "batch 4355: loss 0.023101462051272392\n",
      "batch 4356: loss 0.0008870632736943662\n",
      "batch 4357: loss 0.04920629784464836\n",
      "batch 4358: loss 0.0016011811094358563\n",
      "batch 4359: loss 0.00466941948980093\n",
      "batch 4360: loss 0.0013518199557438493\n",
      "batch 4361: loss 0.0017172854859381914\n",
      "batch 4362: loss 0.001918477239087224\n",
      "batch 4363: loss 0.00030578067526221275\n",
      "batch 4364: loss 0.0172504223883152\n",
      "batch 4365: loss 0.008754870854318142\n",
      "batch 4366: loss 0.00997630599886179\n",
      "batch 4367: loss 0.004079554229974747\n",
      "batch 4368: loss 0.004337513819336891\n",
      "batch 4369: loss 0.0011407112469896674\n",
      "batch 4370: loss 0.0022582118399441242\n",
      "batch 4371: loss 0.04746454581618309\n",
      "batch 4372: loss 0.0057299998588860035\n",
      "batch 4373: loss 0.1033199355006218\n",
      "batch 4374: loss 0.0006210307474248111\n",
      "batch 4375: loss 0.014860717579722404\n",
      "batch 4376: loss 0.002583688125014305\n",
      "batch 4377: loss 0.001815586700104177\n",
      "batch 4378: loss 0.006395028904080391\n",
      "batch 4379: loss 0.004837621469050646\n",
      "batch 4380: loss 0.0010845380602404475\n",
      "batch 4381: loss 0.00034247315488755703\n",
      "batch 4382: loss 0.0009190276614390314\n",
      "batch 4383: loss 0.00010239789116894826\n",
      "batch 4384: loss 0.00010005451622419059\n",
      "batch 4385: loss 0.0003432895173318684\n",
      "batch 4386: loss 0.0008913566707633436\n",
      "batch 4387: loss 0.00013309477071743459\n",
      "batch 4388: loss 0.0008309097611345351\n",
      "batch 4389: loss 0.006149962544441223\n",
      "batch 4390: loss 0.005911299027502537\n",
      "batch 4391: loss 0.007688180543482304\n",
      "batch 4392: loss 0.032899606972932816\n",
      "batch 4393: loss 0.008883739821612835\n",
      "batch 4394: loss 0.0007238846737891436\n",
      "batch 4395: loss 0.003821899415925145\n",
      "batch 4396: loss 0.0017960498807951808\n",
      "batch 4397: loss 0.011174138635396957\n",
      "batch 4398: loss 0.00012229011917952448\n",
      "batch 4399: loss 0.0001149840172729455\n",
      "batch 4400: loss 0.0004407032101880759\n",
      "batch 4401: loss 4.69517799501773e-05\n",
      "batch 4402: loss 0.0037644999101758003\n",
      "batch 4403: loss 8.435819472651929e-05\n",
      "batch 4404: loss 0.0027409337926656008\n",
      "batch 4405: loss 0.0030971833039075136\n",
      "batch 4406: loss 0.002508822362869978\n",
      "batch 4407: loss 0.020409824326634407\n",
      "batch 4408: loss 0.0005117232794873416\n",
      "batch 4409: loss 0.0002806298725772649\n",
      "batch 4410: loss 0.0014273880515247583\n",
      "batch 4411: loss 0.000893019197974354\n",
      "batch 4412: loss 0.042196840047836304\n",
      "batch 4413: loss 0.0006941065657883883\n",
      "batch 4414: loss 0.0011349058477208018\n",
      "batch 4415: loss 0.05822283774614334\n",
      "batch 4416: loss 0.0002697211457416415\n",
      "batch 4417: loss 0.0019463679054751992\n",
      "batch 4418: loss 0.003232404589653015\n",
      "batch 4419: loss 0.00013827564544044435\n",
      "batch 4420: loss 0.008792047388851643\n",
      "batch 4421: loss 0.06493230909109116\n",
      "batch 4422: loss 0.0010036386083811522\n",
      "batch 4423: loss 0.015330672264099121\n",
      "batch 4424: loss 0.00018654951418284327\n",
      "batch 4425: loss 0.0036076903343200684\n",
      "batch 4426: loss 0.03409308195114136\n",
      "batch 4427: loss 0.017846327275037766\n",
      "batch 4428: loss 0.009519227780401707\n",
      "batch 4429: loss 0.0006274708430282772\n",
      "batch 4430: loss 0.010923339053988457\n",
      "batch 4431: loss 9.670305735198781e-05\n",
      "batch 4432: loss 0.00019404073827899992\n",
      "batch 4433: loss 0.0016883837524801493\n",
      "batch 4434: loss 0.0029357396997511387\n",
      "batch 4435: loss 0.001609001075848937\n",
      "batch 4436: loss 0.0037323732394725084\n",
      "batch 4437: loss 1.1601192454691045e-05\n",
      "batch 4438: loss 0.0023549033794552088\n",
      "batch 4439: loss 0.03737245500087738\n",
      "batch 4440: loss 0.00038044145912863314\n",
      "batch 4441: loss 0.0022834832780063152\n",
      "batch 4442: loss 0.004377561621367931\n",
      "batch 4443: loss 0.03945872560143471\n",
      "batch 4444: loss 0.05945691466331482\n",
      "batch 4445: loss 0.0004265884344931692\n",
      "batch 4446: loss 0.002390245907008648\n",
      "batch 4447: loss 0.025430981069803238\n",
      "batch 4448: loss 0.001419832231476903\n",
      "batch 4449: loss 9.924970072461292e-05\n",
      "batch 4450: loss 0.07370983064174652\n",
      "batch 4451: loss 0.04627466946840286\n",
      "batch 4452: loss 0.028654996305704117\n",
      "batch 4453: loss 0.003289199434220791\n",
      "batch 4454: loss 0.03648075833916664\n",
      "batch 4455: loss 0.0003459254512563348\n",
      "batch 4456: loss 0.0003273833426646888\n",
      "batch 4457: loss 0.000435986730735749\n",
      "batch 4458: loss 0.010072031058371067\n",
      "batch 4459: loss 0.001453069387935102\n",
      "batch 4460: loss 9.861509897746146e-05\n",
      "batch 4461: loss 0.027061574161052704\n",
      "batch 4462: loss 0.0005277518066577613\n",
      "batch 4463: loss 0.00019417457224335521\n",
      "batch 4464: loss 0.042014531791210175\n",
      "batch 4465: loss 0.010898834094405174\n",
      "batch 4466: loss 0.00042772540473379195\n",
      "batch 4467: loss 0.002055614022538066\n",
      "batch 4468: loss 0.003157940926030278\n",
      "batch 4469: loss 0.0005488388705998659\n",
      "batch 4470: loss 0.0011749331606552005\n",
      "batch 4471: loss 0.002793332561850548\n",
      "batch 4472: loss 0.0031168325804173946\n",
      "batch 4473: loss 0.015104051679372787\n",
      "batch 4474: loss 0.006378197576850653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4475: loss 0.00291068316437304\n",
      "batch 4476: loss 0.00036945671308785677\n",
      "batch 4477: loss 0.0034916680306196213\n",
      "batch 4478: loss 0.06844568252563477\n",
      "batch 4479: loss 0.001495848293416202\n",
      "batch 4480: loss 0.0011160612339153886\n",
      "batch 4481: loss 0.001159866456873715\n",
      "batch 4482: loss 0.007874124683439732\n",
      "batch 4483: loss 0.011451870203018188\n",
      "batch 4484: loss 0.0012533006956800818\n",
      "batch 4485: loss 0.006497124210000038\n",
      "batch 4486: loss 9.513970326224808e-06\n",
      "batch 4487: loss 0.001592053915373981\n",
      "batch 4488: loss 0.009063022211194038\n",
      "batch 4489: loss 0.0002161110460292548\n",
      "batch 4490: loss 0.01104262936860323\n",
      "batch 4491: loss 0.0226153414696455\n",
      "batch 4492: loss 0.027798978611826897\n",
      "batch 4493: loss 0.0004679462290368974\n",
      "batch 4494: loss 0.00033350122976116836\n",
      "batch 4495: loss 0.002261407906189561\n",
      "batch 4496: loss 0.003239713842049241\n",
      "batch 4497: loss 0.14189015328884125\n",
      "batch 4498: loss 0.006251491140574217\n",
      "batch 4499: loss 0.0002893191995099187\n",
      "batch 4500: loss 0.0244341678917408\n",
      "batch 4501: loss 0.0014703173656016588\n",
      "batch 4502: loss 0.00030282416264526546\n",
      "batch 4503: loss 0.0014351701829582453\n",
      "batch 4504: loss 0.0005767049733549356\n",
      "batch 4505: loss 0.003555784234777093\n",
      "batch 4506: loss 0.002927042543888092\n",
      "batch 4507: loss 0.05737931653857231\n",
      "batch 4508: loss 0.0004328782088123262\n",
      "batch 4509: loss 0.0070119937881827354\n",
      "batch 4510: loss 0.007149705663323402\n",
      "batch 4511: loss 4.8247416998492554e-05\n",
      "batch 4512: loss 0.0020637877751141787\n",
      "batch 4513: loss 0.00019590317970141768\n",
      "batch 4514: loss 0.0004641468694899231\n",
      "batch 4515: loss 0.0002598022692836821\n",
      "batch 4516: loss 0.043322913348674774\n",
      "batch 4517: loss 0.001044264412485063\n",
      "batch 4518: loss 0.00014216253475751728\n",
      "batch 4519: loss 0.0008031760808080435\n",
      "batch 4520: loss 0.0013917085016146302\n",
      "batch 4521: loss 0.008370008319616318\n",
      "batch 4522: loss 0.003947053570300341\n",
      "batch 4523: loss 9.376143862027675e-05\n",
      "batch 4524: loss 0.08098609000444412\n",
      "batch 4525: loss 5.6437653256580234e-05\n",
      "batch 4526: loss 0.014991732314229012\n",
      "batch 4527: loss 0.0008624577894806862\n",
      "batch 4528: loss 0.00039890583138912916\n",
      "batch 4529: loss 0.019173195585608482\n",
      "batch 4530: loss 0.00010489825945114717\n",
      "batch 4531: loss 0.0002931681228801608\n",
      "batch 4532: loss 0.0021950399968773127\n",
      "batch 4533: loss 0.010994235053658485\n",
      "batch 4534: loss 0.0007635385845787823\n",
      "batch 4535: loss 0.034083470702171326\n",
      "batch 4536: loss 3.404834569664672e-05\n",
      "batch 4537: loss 0.004689653869718313\n",
      "batch 4538: loss 0.00013528943236451596\n",
      "batch 4539: loss 0.01644783467054367\n",
      "batch 4540: loss 0.06447667628526688\n",
      "batch 4541: loss 3.335260771564208e-05\n",
      "batch 4542: loss 0.002045863773673773\n",
      "batch 4543: loss 5.470078394864686e-05\n",
      "batch 4544: loss 0.0008915598737075925\n",
      "batch 4545: loss 0.0034334161318838596\n",
      "batch 4546: loss 3.2597370591247454e-05\n",
      "batch 4547: loss 0.002738787094131112\n",
      "batch 4548: loss 0.0013654042268171906\n",
      "batch 4549: loss 0.004520313814282417\n",
      "batch 4550: loss 0.003997691906988621\n",
      "batch 4551: loss 0.017938487231731415\n",
      "batch 4552: loss 0.0065918248146772385\n",
      "batch 4553: loss 0.060642752796411514\n",
      "batch 4554: loss 0.0013719054404646158\n",
      "batch 4555: loss 0.00013964563549961895\n",
      "batch 4556: loss 0.07884901762008667\n",
      "batch 4557: loss 0.06404360383749008\n",
      "batch 4558: loss 0.003205409971997142\n",
      "batch 4559: loss 0.001979454420506954\n",
      "batch 4560: loss 0.00010814063716679811\n",
      "batch 4561: loss 0.0059671467170119286\n",
      "batch 4562: loss 0.003664326388388872\n",
      "batch 4563: loss 0.007703127805143595\n",
      "batch 4564: loss 0.013931282795965672\n",
      "batch 4565: loss 0.0017366161337122321\n",
      "batch 4566: loss 0.0001230139605468139\n",
      "batch 4567: loss 0.09977688640356064\n",
      "batch 4568: loss 0.007238985504955053\n",
      "batch 4569: loss 0.011721674352884293\n",
      "batch 4570: loss 0.010662516579031944\n",
      "batch 4571: loss 0.001198343699797988\n",
      "batch 4572: loss 0.002152117434889078\n",
      "batch 4573: loss 9.306734864367172e-05\n",
      "batch 4574: loss 0.00021715715411119163\n",
      "batch 4575: loss 0.053578805178403854\n",
      "batch 4576: loss 0.001291349995881319\n",
      "batch 4577: loss 9.523555490886793e-05\n",
      "batch 4578: loss 0.001699725165963173\n",
      "batch 4579: loss 0.00014260377793107182\n",
      "batch 4580: loss 0.002160335425287485\n",
      "batch 4581: loss 0.13483744859695435\n",
      "batch 4582: loss 0.19905591011047363\n",
      "batch 4583: loss 0.0176447331905365\n",
      "batch 4584: loss 0.0005726642557419837\n",
      "batch 4585: loss 0.004037254955619574\n",
      "batch 4586: loss 0.004674247931689024\n",
      "batch 4587: loss 0.0008096623932942748\n",
      "batch 4588: loss 0.0013083815574645996\n",
      "batch 4589: loss 0.049203503876924515\n",
      "batch 4590: loss 0.0006091155228205025\n",
      "batch 4591: loss 0.0016058183973655105\n",
      "batch 4592: loss 0.005041624419391155\n",
      "batch 4593: loss 0.0066150794737041\n",
      "batch 4594: loss 0.012415776029229164\n",
      "batch 4595: loss 0.002000979380682111\n",
      "batch 4596: loss 0.17034472525119781\n",
      "batch 4597: loss 0.11845597624778748\n",
      "batch 4598: loss 0.00017268590454477817\n",
      "batch 4599: loss 0.007064092438668013\n",
      "batch 4600: loss 0.0054436963982880116\n",
      "batch 4601: loss 0.019905375316739082\n",
      "batch 4602: loss 0.002644721418619156\n",
      "batch 4603: loss 0.002157678361982107\n",
      "batch 4604: loss 0.00230394396930933\n",
      "batch 4605: loss 0.0029539659153670073\n",
      "batch 4606: loss 0.16537009179592133\n",
      "batch 4607: loss 0.006494469009339809\n",
      "batch 4608: loss 0.0011253240518271923\n",
      "batch 4609: loss 0.002167418599128723\n",
      "batch 4610: loss 0.0021068574860692024\n",
      "batch 4611: loss 0.07197276502847672\n",
      "batch 4612: loss 0.06252090632915497\n",
      "batch 4613: loss 0.0027155252173542976\n",
      "batch 4614: loss 0.0019835925195366144\n",
      "batch 4615: loss 0.008923276327550411\n",
      "batch 4616: loss 0.005482330918312073\n",
      "batch 4617: loss 0.03939204663038254\n",
      "batch 4618: loss 0.06476287543773651\n",
      "batch 4619: loss 0.009745655581355095\n",
      "batch 4620: loss 0.001244251150637865\n",
      "batch 4621: loss 0.02494749426841736\n",
      "batch 4622: loss 0.0034668382722884417\n",
      "batch 4623: loss 0.0010589625453576446\n",
      "batch 4624: loss 0.0509655736386776\n",
      "batch 4625: loss 0.00299741979688406\n",
      "batch 4626: loss 0.00887194462120533\n",
      "batch 4627: loss 0.001061486080288887\n",
      "batch 4628: loss 0.0025135257747024298\n",
      "batch 4629: loss 0.005027408711612225\n",
      "batch 4630: loss 0.007412286009639502\n",
      "batch 4631: loss 0.004180258605629206\n",
      "batch 4632: loss 0.000912690011318773\n",
      "batch 4633: loss 0.004891159012913704\n",
      "batch 4634: loss 0.0023708841763436794\n",
      "batch 4635: loss 0.0009460144792683423\n",
      "batch 4636: loss 0.012584070675075054\n",
      "batch 4637: loss 0.04332420602440834\n",
      "batch 4638: loss 0.023217611014842987\n",
      "batch 4639: loss 0.003934410400688648\n",
      "batch 4640: loss 0.0068222275003790855\n",
      "batch 4641: loss 0.0027776032220572233\n",
      "batch 4642: loss 0.0034328987821936607\n",
      "batch 4643: loss 0.0003060252929572016\n",
      "batch 4644: loss 0.02061879076063633\n",
      "batch 4645: loss 0.0013776995474472642\n",
      "batch 4646: loss 0.00016081039211712778\n",
      "batch 4647: loss 0.003257249714806676\n",
      "batch 4648: loss 0.0002977309050038457\n",
      "batch 4649: loss 0.088566854596138\n",
      "batch 4650: loss 0.0002140486176358536\n",
      "batch 4651: loss 0.001969998236745596\n",
      "batch 4652: loss 0.0004489380808081478\n",
      "batch 4653: loss 0.0014066629810258746\n",
      "batch 4654: loss 0.008304954506456852\n",
      "batch 4655: loss 0.015420515090227127\n",
      "batch 4656: loss 0.004658079240471125\n",
      "batch 4657: loss 0.0025188883300870657\n",
      "batch 4658: loss 0.008668587543070316\n",
      "batch 4659: loss 0.012158188968896866\n",
      "batch 4660: loss 0.006043466739356518\n",
      "batch 4661: loss 0.006413436960428953\n",
      "batch 4662: loss 0.0005063193966634572\n",
      "batch 4663: loss 7.473349251085892e-05\n",
      "batch 4664: loss 0.0008323640795424581\n",
      "batch 4665: loss 2.8487374947872013e-05\n",
      "batch 4666: loss 0.0017913331976160407\n",
      "batch 4667: loss 0.0001877542381407693\n",
      "batch 4668: loss 7.087469566613436e-05\n",
      "batch 4669: loss 5.894793866900727e-05\n",
      "batch 4670: loss 0.036921340972185135\n",
      "batch 4671: loss 0.0005421657115221024\n",
      "batch 4672: loss 0.000388215237762779\n",
      "batch 4673: loss 0.009637218900024891\n",
      "batch 4674: loss 0.00011582685692701489\n",
      "batch 4675: loss 0.03458153456449509\n",
      "batch 4676: loss 0.0014218370197340846\n",
      "batch 4677: loss 0.0017106785671785474\n",
      "batch 4678: loss 0.007208064664155245\n",
      "batch 4679: loss 0.0015393114881590009\n",
      "batch 4680: loss 0.00022695866937283427\n",
      "batch 4681: loss 0.0021913214586675167\n",
      "batch 4682: loss 2.824278999469243e-05\n",
      "batch 4683: loss 0.00034112302819266915\n",
      "batch 4684: loss 1.559220254421234e-05\n",
      "batch 4685: loss 0.0013885669177398086\n",
      "batch 4686: loss 0.013427830301225185\n",
      "batch 4687: loss 0.060071151703596115\n",
      "batch 4688: loss 0.0934482291340828\n",
      "batch 4689: loss 0.00010860667680390179\n",
      "batch 4690: loss 0.0003834144736174494\n",
      "batch 4691: loss 0.012458576820790768\n",
      "batch 4692: loss 0.0015563743654638529\n",
      "batch 4693: loss 5.9724658058257774e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4694: loss 0.04748386889696121\n",
      "batch 4695: loss 0.0019270210759714246\n",
      "batch 4696: loss 0.030352706089615822\n",
      "batch 4697: loss 0.000726903963368386\n",
      "batch 4698: loss 0.00614794110879302\n",
      "batch 4699: loss 0.0060668527148664\n",
      "batch 4700: loss 0.048792604357004166\n",
      "batch 4701: loss 0.024798370897769928\n",
      "batch 4702: loss 0.00017646040942054242\n",
      "batch 4703: loss 0.0015166973462328315\n",
      "batch 4704: loss 0.0009246761910617352\n",
      "batch 4705: loss 0.07970436662435532\n",
      "batch 4706: loss 0.042062170803546906\n",
      "batch 4707: loss 0.13230538368225098\n",
      "batch 4708: loss 0.006189718842506409\n",
      "batch 4709: loss 0.0018394963117316365\n",
      "batch 4710: loss 0.040395013988018036\n",
      "batch 4711: loss 0.002758677816018462\n",
      "batch 4712: loss 0.0050758845172822475\n",
      "batch 4713: loss 0.01393998134881258\n",
      "batch 4714: loss 0.014015798456966877\n",
      "batch 4715: loss 0.08031567186117172\n",
      "batch 4716: loss 0.006569435354322195\n",
      "batch 4717: loss 0.06615858525037766\n",
      "batch 4718: loss 0.0017207494238391519\n",
      "batch 4719: loss 0.0007152548059821129\n",
      "batch 4720: loss 0.04472954571247101\n",
      "batch 4721: loss 0.0007913931040093303\n",
      "batch 4722: loss 0.1119908019900322\n",
      "batch 4723: loss 0.0067401002161204815\n",
      "batch 4724: loss 0.002133128931745887\n",
      "batch 4725: loss 0.010180537588894367\n",
      "batch 4726: loss 0.012661568820476532\n",
      "batch 4727: loss 0.022285034880042076\n",
      "batch 4728: loss 0.0011184259783476591\n",
      "batch 4729: loss 0.00048358633648604155\n",
      "batch 4730: loss 0.025983933359384537\n",
      "batch 4731: loss 0.04371665418148041\n",
      "batch 4732: loss 0.0020186372566968203\n",
      "batch 4733: loss 0.0003654029278550297\n",
      "batch 4734: loss 0.01012298185378313\n",
      "batch 4735: loss 0.0283819492906332\n",
      "batch 4736: loss 0.01719064824283123\n",
      "batch 4737: loss 0.0006237921770662069\n",
      "batch 4738: loss 0.0006582718924619257\n",
      "batch 4739: loss 0.06669192761182785\n",
      "batch 4740: loss 0.06274092942476273\n",
      "batch 4741: loss 0.00060605991166085\n",
      "batch 4742: loss 0.009013011120259762\n",
      "batch 4743: loss 0.00027992919785901904\n",
      "batch 4744: loss 0.0463881641626358\n",
      "batch 4745: loss 0.0669192373752594\n",
      "batch 4746: loss 0.17878654599189758\n",
      "batch 4747: loss 0.06604000180959702\n",
      "batch 4748: loss 0.039278481155633926\n",
      "batch 4749: loss 0.0005575943505391479\n",
      "batch 4750: loss 0.00022813532268628478\n",
      "batch 4751: loss 0.0023876396007835865\n",
      "batch 4752: loss 0.024662863463163376\n",
      "batch 4753: loss 0.006233832333236933\n",
      "batch 4754: loss 0.0011529304319992661\n",
      "batch 4755: loss 0.0031431366223841906\n",
      "batch 4756: loss 0.000930020643863827\n",
      "batch 4757: loss 0.001002701697871089\n",
      "batch 4758: loss 0.015458394773304462\n",
      "batch 4759: loss 0.0005594055401161313\n",
      "batch 4760: loss 0.056458644568920135\n",
      "batch 4761: loss 0.0195026658475399\n",
      "batch 4762: loss 0.02060406468808651\n",
      "batch 4763: loss 0.004782850854098797\n",
      "batch 4764: loss 0.01896718516945839\n",
      "batch 4765: loss 0.0272077489644289\n",
      "batch 4766: loss 0.0005356181063689291\n",
      "batch 4767: loss 0.0003324502322357148\n",
      "batch 4768: loss 0.0030889466870576143\n",
      "batch 4769: loss 0.05815909802913666\n",
      "batch 4770: loss 0.07391276955604553\n",
      "batch 4771: loss 0.000628860667347908\n",
      "batch 4772: loss 0.00035353554994799197\n",
      "batch 4773: loss 0.0067497133277356625\n",
      "batch 4774: loss 0.013609368354082108\n",
      "batch 4775: loss 0.003336245659738779\n",
      "batch 4776: loss 0.00036513517261482775\n",
      "batch 4777: loss 0.0004857908934354782\n",
      "batch 4778: loss 0.004990377463400364\n",
      "batch 4779: loss 0.0004916278412565589\n",
      "batch 4780: loss 0.0003656557237263769\n",
      "batch 4781: loss 0.0017439271323382854\n",
      "batch 4782: loss 0.08603460341691971\n",
      "batch 4783: loss 0.001766309724189341\n",
      "batch 4784: loss 0.00032733543775975704\n",
      "batch 4785: loss 0.000905783730559051\n",
      "batch 4786: loss 0.0003663035458885133\n",
      "batch 4787: loss 0.00046579117770306766\n",
      "batch 4788: loss 0.0031334226951003075\n",
      "batch 4789: loss 0.033872075378894806\n",
      "batch 4790: loss 0.00017013927572406828\n",
      "batch 4791: loss 0.011611617170274258\n",
      "batch 4792: loss 0.023595014587044716\n",
      "batch 4793: loss 0.024942468851804733\n",
      "batch 4794: loss 0.005253098905086517\n",
      "batch 4795: loss 0.0027707847766578197\n",
      "batch 4796: loss 0.0009224237292073667\n",
      "batch 4797: loss 0.0026655972469598055\n",
      "batch 4798: loss 0.0008279038011096418\n",
      "batch 4799: loss 8.365584653802216e-05\n",
      "batch 4800: loss 0.01380930282175541\n",
      "batch 4801: loss 0.0005121265421621501\n",
      "batch 4802: loss 0.0006620893254876137\n",
      "batch 4803: loss 0.004195091780275106\n",
      "batch 4804: loss 0.0005099897971376777\n",
      "batch 4805: loss 0.18358691036701202\n",
      "batch 4806: loss 0.0012129377573728561\n",
      "batch 4807: loss 0.003315609646961093\n",
      "batch 4808: loss 0.026883069425821304\n",
      "batch 4809: loss 0.056439246982336044\n",
      "batch 4810: loss 0.004900762345641851\n",
      "batch 4811: loss 0.0026982477866113186\n",
      "batch 4812: loss 0.001505284570157528\n",
      "batch 4813: loss 0.00442808261141181\n",
      "batch 4814: loss 0.006192073691636324\n",
      "batch 4815: loss 0.007543351035565138\n",
      "batch 4816: loss 0.00024612873676232994\n",
      "batch 4817: loss 0.0021929030772298574\n",
      "batch 4818: loss 0.12567372620105743\n",
      "batch 4819: loss 0.0025075103621929884\n",
      "batch 4820: loss 0.0038604175206273794\n",
      "batch 4821: loss 0.006974686868488789\n",
      "batch 4822: loss 0.010378017090260983\n",
      "batch 4823: loss 0.0006125876097939909\n",
      "batch 4824: loss 0.00041387503733858466\n",
      "batch 4825: loss 6.824541924288496e-05\n",
      "batch 4826: loss 0.0010968499118462205\n",
      "batch 4827: loss 0.0035956804640591145\n",
      "batch 4828: loss 0.015464477241039276\n",
      "batch 4829: loss 0.005957295186817646\n",
      "batch 4830: loss 0.01249895803630352\n",
      "batch 4831: loss 0.0011326217791065574\n",
      "batch 4832: loss 0.00848322082310915\n",
      "batch 4833: loss 0.003131079487502575\n",
      "batch 4834: loss 0.007075295317918062\n",
      "batch 4835: loss 0.00016025015793275088\n",
      "batch 4836: loss 0.0012695621699094772\n",
      "batch 4837: loss 0.005106993950903416\n",
      "batch 4838: loss 0.018369784578680992\n",
      "batch 4839: loss 0.00038587686140090227\n",
      "batch 4840: loss 0.013736259192228317\n",
      "batch 4841: loss 0.00013142400712240487\n",
      "batch 4842: loss 0.06090106442570686\n",
      "batch 4843: loss 0.0035385871306061745\n",
      "batch 4844: loss 0.000577648461330682\n",
      "batch 4845: loss 0.0252784825861454\n",
      "batch 4846: loss 0.0028989464044570923\n",
      "batch 4847: loss 8.117860124912113e-05\n",
      "batch 4848: loss 0.00020581377611961216\n",
      "batch 4849: loss 0.00021643663058057427\n",
      "batch 4850: loss 0.0008248761878348887\n",
      "batch 4851: loss 0.07951001077890396\n",
      "batch 4852: loss 0.0001673000951996073\n",
      "batch 4853: loss 0.0005303166690282524\n",
      "batch 4854: loss 0.0002569614734966308\n",
      "batch 4855: loss 0.009211261756718159\n",
      "batch 4856: loss 0.010638561099767685\n",
      "batch 4857: loss 0.0036891677882522345\n",
      "batch 4858: loss 0.00010216156806563959\n",
      "batch 4859: loss 0.003381352871656418\n",
      "batch 4860: loss 0.029869910329580307\n",
      "batch 4861: loss 0.012900914996862411\n",
      "batch 4862: loss 0.009640821255743504\n",
      "batch 4863: loss 0.0018151637632399797\n",
      "batch 4864: loss 0.00026938397786580026\n",
      "batch 4865: loss 0.0009534080745652318\n",
      "batch 4866: loss 0.0001170134128187783\n",
      "batch 4867: loss 0.012144516222178936\n",
      "batch 4868: loss 0.00786319375038147\n",
      "batch 4869: loss 0.0003246004052925855\n",
      "batch 4870: loss 0.009366863407194614\n",
      "batch 4871: loss 0.002760958392173052\n",
      "batch 4872: loss 0.00020412645244505256\n",
      "batch 4873: loss 0.000540911394637078\n",
      "batch 4874: loss 0.02934870682656765\n",
      "batch 4875: loss 0.005606108345091343\n",
      "batch 4876: loss 7.479795021936297e-05\n",
      "batch 4877: loss 0.02123246341943741\n",
      "batch 4878: loss 0.0007468590047210455\n",
      "batch 4879: loss 9.337969095213339e-05\n",
      "batch 4880: loss 0.0006275287014432251\n",
      "batch 4881: loss 0.0003294275957159698\n",
      "batch 4882: loss 0.028073750436306\n",
      "batch 4883: loss 0.0006840192363597453\n",
      "batch 4884: loss 0.00015270276344381273\n",
      "batch 4885: loss 0.005619477480649948\n",
      "batch 4886: loss 0.0010093715973198414\n",
      "batch 4887: loss 0.049375228583812714\n",
      "batch 4888: loss 0.02982158586382866\n",
      "batch 4889: loss 0.004127279855310917\n",
      "batch 4890: loss 0.0026236395351588726\n",
      "batch 4891: loss 0.00038703856989741325\n",
      "batch 4892: loss 3.317742812214419e-05\n",
      "batch 4893: loss 0.00029688645736314356\n",
      "batch 4894: loss 0.00019478934700600803\n",
      "batch 4895: loss 0.00010711272625485435\n",
      "batch 4896: loss 0.00019838537264149636\n",
      "batch 4897: loss 0.03349271044135094\n",
      "batch 4898: loss 0.004169601481407881\n",
      "batch 4899: loss 0.005666399374604225\n",
      "batch 4900: loss 2.809901343425736e-05\n",
      "batch 4901: loss 0.017071671783924103\n",
      "batch 4902: loss 0.029086174443364143\n",
      "batch 4903: loss 0.004389929585158825\n",
      "batch 4904: loss 0.0018473528325557709\n",
      "batch 4905: loss 9.106936340685934e-05\n",
      "batch 4906: loss 0.0012283084215596318\n",
      "batch 4907: loss 0.0011458838125690818\n",
      "batch 4908: loss 0.006065206602215767\n",
      "batch 4909: loss 0.09981749206781387\n",
      "batch 4910: loss 0.009401308372616768\n",
      "batch 4911: loss 0.0006508633377961814\n",
      "batch 4912: loss 0.003771063406020403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 4913: loss 0.03587350249290466\n",
      "batch 4914: loss 0.08328612148761749\n",
      "batch 4915: loss 0.0001504635001765564\n",
      "batch 4916: loss 0.00014549581101164222\n",
      "batch 4917: loss 0.01590087078511715\n",
      "batch 4918: loss 0.0002635949931573123\n",
      "batch 4919: loss 0.046280283480882645\n",
      "batch 4920: loss 4.209977123537101e-05\n",
      "batch 4921: loss 0.01817293092608452\n",
      "batch 4922: loss 0.0005536991520784795\n",
      "batch 4923: loss 0.0025315056554973125\n",
      "batch 4924: loss 8.480438555125147e-05\n",
      "batch 4925: loss 0.0007419491303153336\n",
      "batch 4926: loss 0.02795286290347576\n",
      "batch 4927: loss 0.012103780172765255\n",
      "batch 4928: loss 4.2547624616418034e-05\n",
      "batch 4929: loss 0.029090842232108116\n",
      "batch 4930: loss 0.01778128370642662\n",
      "batch 4931: loss 0.0012579759350046515\n",
      "batch 4932: loss 0.003768123686313629\n",
      "batch 4933: loss 0.002496314002200961\n",
      "batch 4934: loss 0.0005444363923743367\n",
      "batch 4935: loss 0.0004622331471182406\n",
      "batch 4936: loss 0.003233854193240404\n",
      "batch 4937: loss 0.0065131401643157005\n",
      "batch 4938: loss 0.07206465303897858\n",
      "batch 4939: loss 0.0012103845365345478\n",
      "batch 4940: loss 0.006884698290377855\n",
      "batch 4941: loss 0.005193214863538742\n",
      "batch 4942: loss 7.601590914418921e-05\n",
      "batch 4943: loss 0.08472048491239548\n",
      "batch 4944: loss 0.0050323219038546085\n",
      "batch 4945: loss 0.0030800141394138336\n",
      "batch 4946: loss 0.0015920191071927547\n",
      "batch 4947: loss 0.03468964621424675\n",
      "batch 4948: loss 0.001618606154806912\n",
      "batch 4949: loss 0.018098553642630577\n",
      "batch 4950: loss 0.0004657201352529228\n",
      "batch 4951: loss 0.002539375564083457\n",
      "batch 4952: loss 0.00022130183060653508\n",
      "batch 4953: loss 0.00028902338817715645\n",
      "batch 4954: loss 0.13365769386291504\n",
      "batch 4955: loss 0.04103931412100792\n",
      "batch 4956: loss 0.001803321298211813\n",
      "batch 4957: loss 0.002958953846246004\n",
      "batch 4958: loss 0.06902331858873367\n",
      "batch 4959: loss 0.0002015963546000421\n",
      "batch 4960: loss 0.00045162028982304037\n",
      "batch 4961: loss 0.0017421671655029058\n",
      "batch 4962: loss 0.0014627831988036633\n",
      "batch 4963: loss 0.009465706534683704\n",
      "batch 4964: loss 0.0007103300886228681\n",
      "batch 4965: loss 0.006998711731284857\n",
      "batch 4966: loss 0.0002475863147992641\n",
      "batch 4967: loss 0.052649762481451035\n",
      "batch 4968: loss 0.005828232504427433\n",
      "batch 4969: loss 0.0016255732625722885\n",
      "batch 4970: loss 0.004708847962319851\n",
      "batch 4971: loss 0.0027335267513990402\n",
      "batch 4972: loss 0.05313049256801605\n",
      "batch 4973: loss 0.003685571951791644\n",
      "batch 4974: loss 8.88284484972246e-05\n",
      "batch 4975: loss 0.00030227555544115603\n",
      "batch 4976: loss 0.023969313129782677\n",
      "batch 4977: loss 0.006864439696073532\n",
      "batch 4978: loss 0.016006840392947197\n",
      "batch 4979: loss 0.001932352315634489\n",
      "batch 4980: loss 0.00011145573080284521\n",
      "batch 4981: loss 0.02039356343448162\n",
      "batch 4982: loss 0.04853696748614311\n",
      "batch 4983: loss 0.06655445694923401\n",
      "batch 4984: loss 0.0006956369034014642\n",
      "batch 4985: loss 0.004751713015139103\n",
      "batch 4986: loss 0.014932416379451752\n",
      "batch 4987: loss 0.004075977019965649\n",
      "batch 4988: loss 0.0012377564562484622\n",
      "batch 4989: loss 0.20036746561527252\n",
      "batch 4990: loss 0.04188462719321251\n",
      "batch 4991: loss 0.020284583792090416\n",
      "batch 4992: loss 5.1585644541773945e-05\n",
      "batch 4993: loss 0.01014724001288414\n",
      "batch 4994: loss 0.0026041704695671797\n",
      "batch 4995: loss 0.0005436316132545471\n",
      "batch 4996: loss 0.03002137318253517\n",
      "batch 4997: loss 0.0017030747840180993\n",
      "batch 4998: loss 0.00015220369095914066\n",
      "batch 4999: loss 0.0036720868665724993\n",
      "batch 5000: loss 0.002892854856327176\n",
      "batch 5001: loss 0.050269171595573425\n",
      "batch 5002: loss 0.00323267700150609\n",
      "batch 5003: loss 0.001022463315166533\n",
      "batch 5004: loss 0.0009269518777728081\n",
      "batch 5005: loss 0.00101944908965379\n",
      "batch 5006: loss 9.442908776691183e-06\n",
      "batch 5007: loss 0.03943316265940666\n",
      "batch 5008: loss 0.0032689999788999557\n",
      "batch 5009: loss 0.0006810776540078223\n",
      "batch 5010: loss 0.003766158828511834\n",
      "batch 5011: loss 0.0005641253665089607\n",
      "batch 5012: loss 0.010748935863375664\n",
      "batch 5013: loss 0.001677297754213214\n",
      "batch 5014: loss 3.1384370231535286e-05\n",
      "batch 5015: loss 0.0002988926717080176\n",
      "batch 5016: loss 0.000260697997873649\n",
      "batch 5017: loss 0.001542661339044571\n",
      "batch 5018: loss 0.08915160596370697\n",
      "batch 5019: loss 0.00028874247800558805\n",
      "batch 5020: loss 0.04262121021747589\n",
      "batch 5021: loss 0.0001770305825630203\n",
      "batch 5022: loss 0.0001148237133747898\n",
      "batch 5023: loss 0.0015917134005576372\n",
      "batch 5024: loss 0.000611852272413671\n",
      "batch 5025: loss 0.003945833537727594\n",
      "batch 5026: loss 0.006366472225636244\n",
      "batch 5027: loss 0.0004944983520545065\n",
      "batch 5028: loss 0.004988799337297678\n",
      "batch 5029: loss 0.0006982083432376385\n",
      "batch 5030: loss 0.00017422446398995817\n",
      "batch 5031: loss 0.0004902710788883269\n",
      "batch 5032: loss 0.019050337374210358\n",
      "batch 5033: loss 0.000702420889865607\n",
      "batch 5034: loss 0.026254186406731606\n",
      "batch 5035: loss 0.010847391560673714\n",
      "batch 5036: loss 0.04420081526041031\n",
      "batch 5037: loss 0.09868637472391129\n",
      "batch 5038: loss 0.08822725713253021\n",
      "batch 5039: loss 3.028539322258439e-05\n",
      "batch 5040: loss 0.0005999502609483898\n",
      "batch 5041: loss 0.0015626311069354415\n",
      "batch 5042: loss 0.0387120395898819\n",
      "batch 5043: loss 0.06792592257261276\n",
      "batch 5044: loss 0.001121103297919035\n",
      "batch 5045: loss 0.001664732932113111\n",
      "batch 5046: loss 0.0002464195713400841\n",
      "batch 5047: loss 0.007878029718995094\n",
      "batch 5048: loss 0.009539535269141197\n",
      "batch 5049: loss 0.0010493361623957753\n",
      "batch 5050: loss 0.005587471649050713\n",
      "batch 5051: loss 0.002036763122305274\n",
      "batch 5052: loss 0.0038239217828959227\n",
      "batch 5053: loss 0.00215256679803133\n",
      "batch 5054: loss 0.022696439176797867\n",
      "batch 5055: loss 0.0068062893114984035\n",
      "batch 5056: loss 0.0072784400545060635\n",
      "batch 5057: loss 0.0003493760304991156\n",
      "batch 5058: loss 0.061551813036203384\n",
      "batch 5059: loss 0.011392399668693542\n",
      "batch 5060: loss 0.08212976902723312\n",
      "batch 5061: loss 0.0008558392291888595\n",
      "batch 5062: loss 0.00045353060704655945\n",
      "batch 5063: loss 0.0005038256058469415\n",
      "batch 5064: loss 0.05106484889984131\n",
      "batch 5065: loss 0.06210635229945183\n",
      "batch 5066: loss 0.0012793042697012424\n",
      "batch 5067: loss 0.008797994814813137\n",
      "batch 5068: loss 0.0025133590679615736\n",
      "batch 5069: loss 0.001579583389684558\n",
      "batch 5070: loss 0.0003556674637366086\n",
      "batch 5071: loss 0.0012346113799139857\n",
      "batch 5072: loss 0.019562672823667526\n",
      "batch 5073: loss 0.00044775157584808767\n",
      "batch 5074: loss 0.008672387339174747\n",
      "batch 5075: loss 0.0009648193372413516\n",
      "batch 5076: loss 0.002570236334577203\n",
      "batch 5077: loss 0.0016828078078106046\n",
      "batch 5078: loss 0.0003634065797086805\n",
      "batch 5079: loss 0.0006080215098336339\n",
      "batch 5080: loss 0.003405648982152343\n",
      "batch 5081: loss 0.1543087214231491\n",
      "batch 5082: loss 0.02779558300971985\n",
      "batch 5083: loss 0.034265950322151184\n",
      "batch 5084: loss 0.0012123759370297194\n",
      "batch 5085: loss 0.0007441488560289145\n",
      "batch 5086: loss 0.040690891444683075\n",
      "batch 5087: loss 0.0003718992229551077\n",
      "batch 5088: loss 0.010391023010015488\n",
      "batch 5089: loss 0.03311651572585106\n",
      "batch 5090: loss 0.012381438165903091\n",
      "batch 5091: loss 0.007094876375049353\n",
      "batch 5092: loss 0.001978084444999695\n",
      "batch 5093: loss 0.004072623327374458\n",
      "batch 5094: loss 0.011563131585717201\n",
      "batch 5095: loss 0.0055788420140743256\n",
      "batch 5096: loss 0.0029299308080226183\n",
      "batch 5097: loss 0.00392845319584012\n",
      "batch 5098: loss 0.006486852187663317\n",
      "batch 5099: loss 0.007679005153477192\n",
      "batch 5100: loss 0.05666004866361618\n",
      "batch 5101: loss 0.002444255631417036\n",
      "batch 5102: loss 0.0006979489116929471\n",
      "batch 5103: loss 0.0026977728120982647\n",
      "batch 5104: loss 0.0005749019328504801\n",
      "batch 5105: loss 0.030173026025295258\n",
      "batch 5106: loss 0.0005161109729669988\n",
      "batch 5107: loss 0.011840214021503925\n",
      "batch 5108: loss 0.0007378177251666784\n",
      "batch 5109: loss 0.0010747663909569383\n",
      "batch 5110: loss 0.00016068643890321255\n",
      "batch 5111: loss 0.00929183792322874\n",
      "batch 5112: loss 3.0814222554909065e-05\n",
      "batch 5113: loss 0.00014008358994033188\n",
      "batch 5114: loss 0.027837028726935387\n",
      "batch 5115: loss 0.03634411096572876\n",
      "batch 5116: loss 0.002075841883197427\n",
      "batch 5117: loss 0.00453501520678401\n",
      "batch 5118: loss 0.0003058004949707538\n",
      "batch 5119: loss 0.05256454274058342\n",
      "batch 5120: loss 0.0022440359462052584\n",
      "batch 5121: loss 0.0017125607701018453\n",
      "batch 5122: loss 0.00010435785952722654\n",
      "batch 5123: loss 0.005201906897127628\n",
      "batch 5124: loss 0.0009481545421294868\n",
      "batch 5125: loss 0.0015013843076303601\n",
      "batch 5126: loss 9.75954535533674e-05\n",
      "batch 5127: loss 0.0016922621289268136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5128: loss 0.004163709934800863\n",
      "batch 5129: loss 0.00011467003059806302\n",
      "batch 5130: loss 0.0034042447805404663\n",
      "batch 5131: loss 0.008666347712278366\n",
      "batch 5132: loss 0.0001654178195167333\n",
      "batch 5133: loss 0.016009874641895294\n",
      "batch 5134: loss 0.0009282160317525268\n",
      "batch 5135: loss 0.004585246555507183\n",
      "batch 5136: loss 0.008343799039721489\n",
      "batch 5137: loss 0.0021016502287238836\n",
      "batch 5138: loss 0.001288068015128374\n",
      "batch 5139: loss 0.006056306883692741\n",
      "batch 5140: loss 0.00734728155657649\n",
      "batch 5141: loss 0.012216807343065739\n",
      "batch 5142: loss 0.03995482996106148\n",
      "batch 5143: loss 0.023248597979545593\n",
      "batch 5144: loss 0.0006557813612744212\n",
      "batch 5145: loss 0.016943156719207764\n",
      "batch 5146: loss 0.014763446524739265\n",
      "batch 5147: loss 0.014392619021236897\n",
      "batch 5148: loss 0.008620684035122395\n",
      "batch 5149: loss 0.008358822204172611\n",
      "batch 5150: loss 0.12240779399871826\n",
      "batch 5151: loss 0.0013547036796808243\n",
      "batch 5152: loss 0.048595551401376724\n",
      "batch 5153: loss 0.002798187779262662\n",
      "batch 5154: loss 0.00015583939966745675\n",
      "batch 5155: loss 0.012046219781041145\n",
      "batch 5156: loss 0.0019139539217576385\n",
      "batch 5157: loss 0.000992339919321239\n",
      "batch 5158: loss 8.106292807497084e-05\n",
      "batch 5159: loss 6.70786948830937e-06\n",
      "batch 5160: loss 0.0019045654917135835\n",
      "batch 5161: loss 0.05150674283504486\n",
      "batch 5162: loss 0.0005075174849480391\n",
      "batch 5163: loss 0.00044071144657209516\n",
      "batch 5164: loss 0.0006265834090299904\n",
      "batch 5165: loss 0.00035222506267018616\n",
      "batch 5166: loss 0.00031761324498802423\n",
      "batch 5167: loss 0.0058220187202095985\n",
      "batch 5168: loss 0.0003897405695170164\n",
      "batch 5169: loss 0.00025303656002506614\n",
      "batch 5170: loss 0.06100289523601532\n",
      "batch 5171: loss 0.0004580323293339461\n",
      "batch 5172: loss 0.008567309938371181\n",
      "batch 5173: loss 0.010054437443614006\n",
      "batch 5174: loss 0.0024363333359360695\n",
      "batch 5175: loss 0.007575024385005236\n",
      "batch 5176: loss 0.007637800183147192\n",
      "batch 5177: loss 0.0026254886761307716\n",
      "batch 5178: loss 0.000377684977138415\n",
      "batch 5179: loss 0.03801163658499718\n",
      "batch 5180: loss 0.00014445139095187187\n",
      "batch 5181: loss 0.0022241652477532625\n",
      "batch 5182: loss 0.0015369979664683342\n",
      "batch 5183: loss 0.0008862253744155169\n",
      "batch 5184: loss 0.002652285387739539\n",
      "batch 5185: loss 0.0001282097218791023\n",
      "batch 5186: loss 0.00011037163494620472\n",
      "batch 5187: loss 0.0008456245413981378\n",
      "batch 5188: loss 0.050047509372234344\n",
      "batch 5189: loss 0.00012270327715668827\n",
      "batch 5190: loss 0.013965873047709465\n",
      "batch 5191: loss 0.00020760185725521296\n",
      "batch 5192: loss 0.00016076410247478634\n",
      "batch 5193: loss 0.002963574370369315\n",
      "batch 5194: loss 0.00010694250522647053\n",
      "batch 5195: loss 0.0022647283039987087\n",
      "batch 5196: loss 0.0005263967323116958\n",
      "batch 5197: loss 0.00023879452783148736\n",
      "batch 5198: loss 0.0023068790324032307\n",
      "batch 5199: loss 0.00023092985793482512\n",
      "batch 5200: loss 0.00036155563429929316\n",
      "batch 5201: loss 0.007102221250534058\n",
      "batch 5202: loss 0.00014477860531769693\n",
      "batch 5203: loss 0.17120280861854553\n",
      "batch 5204: loss 0.0015578814782202244\n",
      "batch 5205: loss 0.011897561140358448\n",
      "batch 5206: loss 0.012077164836227894\n",
      "batch 5207: loss 0.0007264841697178781\n",
      "batch 5208: loss 0.032410379499197006\n",
      "batch 5209: loss 0.08861306309700012\n",
      "batch 5210: loss 0.00042219419265165925\n",
      "batch 5211: loss 0.00010235326044494286\n",
      "batch 5212: loss 0.0009772732155397534\n",
      "batch 5213: loss 0.07702353596687317\n",
      "batch 5214: loss 0.009518829174339771\n",
      "batch 5215: loss 0.00019968098786193877\n",
      "batch 5216: loss 0.002015548525378108\n",
      "batch 5217: loss 0.010880248621106148\n",
      "batch 5218: loss 0.00010222270793747157\n",
      "batch 5219: loss 0.014950891025364399\n",
      "batch 5220: loss 0.005994725041091442\n",
      "batch 5221: loss 0.00045398506335914135\n",
      "batch 5222: loss 0.10586394369602203\n",
      "batch 5223: loss 0.00044811840052716434\n",
      "batch 5224: loss 0.00026770003023557365\n",
      "batch 5225: loss 0.00014898114022798836\n",
      "batch 5226: loss 0.0018808689201250672\n",
      "batch 5227: loss 0.012319390662014484\n",
      "batch 5228: loss 0.0035724288318306208\n",
      "batch 5229: loss 0.03282023221254349\n",
      "batch 5230: loss 0.0004865643277298659\n",
      "batch 5231: loss 0.0006247523706406355\n",
      "batch 5232: loss 0.014094446785748005\n",
      "batch 5233: loss 0.03461913391947746\n",
      "batch 5234: loss 0.015144298784434795\n",
      "batch 5235: loss 0.00033377556246705353\n",
      "batch 5236: loss 0.0008428085129708052\n",
      "batch 5237: loss 0.0027669924311339855\n",
      "batch 5238: loss 0.00249398872256279\n",
      "batch 5239: loss 0.0007917368202470243\n",
      "batch 5240: loss 0.0002494339132681489\n",
      "batch 5241: loss 0.0007188509334810078\n",
      "batch 5242: loss 0.008426278829574585\n",
      "batch 5243: loss 0.00023100255930330604\n",
      "batch 5244: loss 0.0034423251636326313\n",
      "batch 5245: loss 0.13002917170524597\n",
      "batch 5246: loss 0.008845224045217037\n",
      "batch 5247: loss 0.08928278833627701\n",
      "batch 5248: loss 0.0036833041813224554\n",
      "batch 5249: loss 0.04764365032315254\n",
      "batch 5250: loss 0.00013707421021535993\n",
      "batch 5251: loss 0.015543886460363865\n",
      "batch 5252: loss 0.009608877822756767\n",
      "batch 5253: loss 8.64982939674519e-05\n",
      "batch 5254: loss 0.00016946169489528984\n",
      "batch 5255: loss 0.000509829493239522\n",
      "batch 5256: loss 0.07743223756551743\n",
      "batch 5257: loss 5.4680949688190594e-05\n",
      "batch 5258: loss 0.0006553632556460798\n",
      "batch 5259: loss 0.03175659477710724\n",
      "batch 5260: loss 0.008474694564938545\n",
      "batch 5261: loss 0.00012901453010272235\n",
      "batch 5262: loss 0.003329071681946516\n",
      "batch 5263: loss 0.00042587946518324316\n",
      "batch 5264: loss 0.009922337718307972\n",
      "batch 5265: loss 0.0005792036536149681\n",
      "batch 5266: loss 0.0006007203483022749\n",
      "batch 5267: loss 0.01164404395967722\n",
      "batch 5268: loss 0.07042572647333145\n",
      "batch 5269: loss 0.0001222686405526474\n",
      "batch 5270: loss 0.010393863543868065\n",
      "batch 5271: loss 0.043098997324705124\n",
      "batch 5272: loss 0.004197883419692516\n",
      "batch 5273: loss 0.0005767164402641356\n",
      "batch 5274: loss 0.0007290524081327021\n",
      "batch 5275: loss 0.00027533993124961853\n",
      "batch 5276: loss 0.0008482356206513941\n",
      "batch 5277: loss 0.015558017417788506\n",
      "batch 5278: loss 0.1335849016904831\n",
      "batch 5279: loss 0.00536669185385108\n",
      "batch 5280: loss 0.003068178193643689\n",
      "batch 5281: loss 0.0012147440575063229\n",
      "batch 5282: loss 0.0009432496153749526\n",
      "batch 5283: loss 0.04302937537431717\n",
      "batch 5284: loss 0.00011389165592845529\n",
      "batch 5285: loss 0.000803995702881366\n",
      "batch 5286: loss 0.01975107751786709\n",
      "batch 5287: loss 0.007731383666396141\n",
      "batch 5288: loss 0.0017845791298896074\n",
      "batch 5289: loss 0.0004965690895915031\n",
      "batch 5290: loss 0.16356010735034943\n",
      "batch 5291: loss 0.0260093342512846\n",
      "batch 5292: loss 6.348596070893109e-05\n",
      "batch 5293: loss 0.00015934949624352157\n",
      "batch 5294: loss 0.0007755084079690278\n",
      "batch 5295: loss 0.00038658909033983946\n",
      "batch 5296: loss 0.010854661464691162\n",
      "batch 5297: loss 0.0072120800614356995\n",
      "batch 5298: loss 0.009667186066508293\n",
      "batch 5299: loss 0.0017305908259004354\n",
      "batch 5300: loss 0.0019138219067826867\n",
      "batch 5301: loss 0.001656356267631054\n",
      "batch 5302: loss 1.1508647730806842e-05\n",
      "batch 5303: loss 0.00862821377813816\n",
      "batch 5304: loss 0.0004573481564875692\n",
      "batch 5305: loss 0.00013182227849029005\n",
      "batch 5306: loss 0.020218128338456154\n",
      "batch 5307: loss 0.0024972015526145697\n",
      "batch 5308: loss 0.01544605940580368\n",
      "batch 5309: loss 0.04845922067761421\n",
      "batch 5310: loss 0.0007143046823330224\n",
      "batch 5311: loss 0.0010869376128539443\n",
      "batch 5312: loss 0.0014745824737474322\n",
      "batch 5313: loss 0.024357371032238007\n",
      "batch 5314: loss 4.710051143774763e-05\n",
      "batch 5315: loss 0.004149081185460091\n",
      "batch 5316: loss 0.015498541295528412\n",
      "batch 5317: loss 0.0022821661550551653\n",
      "batch 5318: loss 0.02908945456147194\n",
      "batch 5319: loss 0.0035373312421143055\n",
      "batch 5320: loss 0.0014564982848241925\n",
      "batch 5321: loss 0.01209261640906334\n",
      "batch 5322: loss 0.00045035601942799985\n",
      "batch 5323: loss 0.00378565420396626\n",
      "batch 5324: loss 0.00033155284472741187\n",
      "batch 5325: loss 0.008353658020496368\n",
      "batch 5326: loss 0.007300517521798611\n",
      "batch 5327: loss 0.004770738072693348\n",
      "batch 5328: loss 0.0749584287405014\n",
      "batch 5329: loss 0.0007828293601050973\n",
      "batch 5330: loss 0.00032122249831445515\n",
      "batch 5331: loss 0.038456521928310394\n",
      "batch 5332: loss 0.014215969480574131\n",
      "batch 5333: loss 0.06522573530673981\n",
      "batch 5334: loss 0.01004757545888424\n",
      "batch 5335: loss 1.8827975509339012e-05\n",
      "batch 5336: loss 0.003100452246144414\n",
      "batch 5337: loss 0.0004726494662463665\n",
      "batch 5338: loss 0.011392186395823956\n",
      "batch 5339: loss 0.000756325083784759\n",
      "batch 5340: loss 0.0004265355528332293\n",
      "batch 5341: loss 0.0079136798158288\n",
      "batch 5342: loss 0.001909736543893814\n",
      "batch 5343: loss 0.042125020176172256\n",
      "batch 5344: loss 0.0056655616499483585\n",
      "batch 5345: loss 0.0006947544752620161\n",
      "batch 5346: loss 0.00036990290391258895\n",
      "batch 5347: loss 0.045580167323350906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5348: loss 0.23041526973247528\n",
      "batch 5349: loss 0.005761217791587114\n",
      "batch 5350: loss 0.008601230569183826\n",
      "batch 5351: loss 0.02621369995176792\n",
      "batch 5352: loss 0.06455381214618683\n",
      "batch 5353: loss 0.00028174539329484105\n",
      "batch 5354: loss 0.00984313152730465\n",
      "batch 5355: loss 0.007531464099884033\n",
      "batch 5356: loss 0.0013375713024288416\n",
      "batch 5357: loss 0.008839026093482971\n",
      "batch 5358: loss 0.006786656100302935\n",
      "batch 5359: loss 0.05661448836326599\n",
      "batch 5360: loss 0.0032144428696483374\n",
      "batch 5361: loss 0.10548769682645798\n",
      "batch 5362: loss 0.004155206959694624\n",
      "batch 5363: loss 0.00030256639001891017\n",
      "batch 5364: loss 0.11194288730621338\n",
      "batch 5365: loss 0.012702860869467258\n",
      "batch 5366: loss 0.013285517692565918\n",
      "batch 5367: loss 0.0005425804993137717\n",
      "batch 5368: loss 0.003927640616893768\n",
      "batch 5369: loss 0.010651222430169582\n",
      "batch 5370: loss 0.008050934411585331\n",
      "batch 5371: loss 0.0068800621666014194\n",
      "batch 5372: loss 0.005977815482765436\n",
      "batch 5373: loss 0.01478749979287386\n",
      "batch 5374: loss 0.0011262560728937387\n",
      "batch 5375: loss 0.002485355595126748\n",
      "batch 5376: loss 0.00046837254194542766\n",
      "batch 5377: loss 0.0021430363412946463\n",
      "batch 5378: loss 0.025760842487215996\n",
      "batch 5379: loss 0.006703673396259546\n",
      "batch 5380: loss 0.023746833205223083\n",
      "batch 5381: loss 0.0002682437188923359\n",
      "batch 5382: loss 1.8059521607938223e-05\n",
      "batch 5383: loss 0.00014308799291029572\n",
      "batch 5384: loss 0.1224978044629097\n",
      "batch 5385: loss 0.002370738424360752\n",
      "batch 5386: loss 5.7731631386559457e-05\n",
      "batch 5387: loss 0.028768301010131836\n",
      "batch 5388: loss 0.00965817179530859\n",
      "batch 5389: loss 0.06365804374217987\n",
      "batch 5390: loss 0.00018112735415343195\n",
      "batch 5391: loss 0.00032772048143669963\n",
      "batch 5392: loss 6.283649418037385e-05\n",
      "batch 5393: loss 0.0029800855554640293\n",
      "batch 5394: loss 0.0002524317242205143\n",
      "batch 5395: loss 0.00011827432899735868\n",
      "batch 5396: loss 0.00045287786633707583\n",
      "batch 5397: loss 0.0011224038898944855\n",
      "batch 5398: loss 0.0019880610052496195\n",
      "batch 5399: loss 0.0014743892243131995\n",
      "batch 5400: loss 0.009307445026934147\n",
      "batch 5401: loss 0.008707481436431408\n",
      "batch 5402: loss 0.032012563198804855\n",
      "batch 5403: loss 0.000603784283157438\n",
      "batch 5404: loss 0.0015136766014620662\n",
      "batch 5405: loss 0.002330892952159047\n",
      "batch 5406: loss 0.004997133277356625\n",
      "batch 5407: loss 0.01083380077034235\n",
      "batch 5408: loss 0.00042672723066061735\n",
      "batch 5409: loss 0.0004634131328202784\n",
      "batch 5410: loss 0.00010832076804945245\n",
      "batch 5411: loss 0.0005382391973398626\n",
      "batch 5412: loss 0.047929421067237854\n",
      "batch 5413: loss 0.020113743841648102\n",
      "batch 5414: loss 0.003151482669636607\n",
      "batch 5415: loss 0.002636427292600274\n",
      "batch 5416: loss 0.002678001532331109\n",
      "batch 5417: loss 6.210108404047787e-05\n",
      "batch 5418: loss 0.30966702103614807\n",
      "batch 5419: loss 0.023794762790203094\n",
      "batch 5420: loss 0.015930654481053352\n",
      "batch 5421: loss 0.001393377548083663\n",
      "batch 5422: loss 0.00023037292703520507\n",
      "batch 5423: loss 0.0054937400855124\n",
      "batch 5424: loss 0.003595791757106781\n",
      "batch 5425: loss 0.016513628885149956\n",
      "batch 5426: loss 0.05861528590321541\n",
      "batch 5427: loss 0.004274419974535704\n",
      "batch 5428: loss 0.0006964235799387097\n",
      "batch 5429: loss 0.001126436167396605\n",
      "batch 5430: loss 0.09144504368305206\n",
      "batch 5431: loss 0.007206338923424482\n",
      "batch 5432: loss 0.023022200912237167\n",
      "batch 5433: loss 0.0010333473328500986\n",
      "batch 5434: loss 0.04230514541268349\n",
      "batch 5435: loss 0.026939958333969116\n",
      "batch 5436: loss 0.00022259913384914398\n",
      "batch 5437: loss 0.016577545553445816\n",
      "batch 5438: loss 0.00018852765788324177\n",
      "batch 5439: loss 0.05708852782845497\n",
      "batch 5440: loss 0.0007748952484689653\n",
      "batch 5441: loss 0.0032335184514522552\n",
      "batch 5442: loss 0.00045604261686094105\n",
      "batch 5443: loss 0.0002365552936680615\n",
      "batch 5444: loss 0.0005052451742812991\n",
      "batch 5445: loss 0.01618783548474312\n",
      "batch 5446: loss 0.01025724783539772\n",
      "batch 5447: loss 0.00014895964704919606\n",
      "batch 5448: loss 0.04786844924092293\n",
      "batch 5449: loss 0.08134952187538147\n",
      "batch 5450: loss 0.00035752664553001523\n",
      "batch 5451: loss 0.16668584942817688\n",
      "batch 5452: loss 0.0009636789909563959\n",
      "batch 5453: loss 0.0028590017464011908\n",
      "batch 5454: loss 0.005448656156659126\n",
      "batch 5455: loss 0.002163463970646262\n",
      "batch 5456: loss 0.03765738010406494\n",
      "batch 5457: loss 0.0028787516057491302\n",
      "batch 5458: loss 0.002614662516862154\n",
      "batch 5459: loss 0.003140568733215332\n",
      "batch 5460: loss 0.04996148869395256\n",
      "batch 5461: loss 0.009199731983244419\n",
      "batch 5462: loss 0.04638872668147087\n",
      "batch 5463: loss 0.05643898993730545\n",
      "batch 5464: loss 0.0009777904488146305\n",
      "batch 5465: loss 0.00017022354586515576\n",
      "batch 5466: loss 0.00034977763425558805\n",
      "batch 5467: loss 0.004973269067704678\n",
      "batch 5468: loss 0.012205448001623154\n",
      "batch 5469: loss 0.03448266535997391\n",
      "batch 5470: loss 0.002584028523415327\n",
      "batch 5471: loss 0.09702644497156143\n",
      "batch 5472: loss 0.14593347907066345\n",
      "batch 5473: loss 0.009585018269717693\n",
      "batch 5474: loss 0.014500271528959274\n",
      "batch 5475: loss 0.006489422172307968\n",
      "batch 5476: loss 0.0009521141182631254\n",
      "batch 5477: loss 0.00015843394794501364\n",
      "batch 5478: loss 0.0015610430855304003\n",
      "batch 5479: loss 0.0011537353275343776\n",
      "batch 5480: loss 0.002416621195152402\n",
      "batch 5481: loss 8.212057582568377e-05\n",
      "batch 5482: loss 0.00016839415184222162\n",
      "batch 5483: loss 0.03781437501311302\n",
      "batch 5484: loss 0.005422525107860565\n",
      "batch 5485: loss 0.0015177105087786913\n",
      "batch 5486: loss 0.011826925911009312\n",
      "batch 5487: loss 0.001058118068613112\n",
      "batch 5488: loss 0.0009623652440495789\n",
      "batch 5489: loss 0.0010398519225418568\n",
      "batch 5490: loss 0.0008521340787410736\n",
      "batch 5491: loss 0.041503556072711945\n",
      "batch 5492: loss 0.004053536336869001\n",
      "batch 5493: loss 0.019579878076910973\n",
      "batch 5494: loss 0.0005374756874516606\n",
      "batch 5495: loss 0.03681188076734543\n",
      "batch 5496: loss 0.004944828804582357\n",
      "batch 5497: loss 0.02795379050076008\n",
      "batch 5498: loss 0.037900909781455994\n",
      "batch 5499: loss 0.001309834187850356\n",
      "batch 5500: loss 0.10523190349340439\n",
      "batch 5501: loss 0.0015074132243171334\n",
      "batch 5502: loss 0.01366226002573967\n",
      "batch 5503: loss 0.08370771259069443\n",
      "batch 5504: loss 0.0002118027477990836\n",
      "batch 5505: loss 0.007355501875281334\n",
      "batch 5506: loss 0.0075685796327888966\n",
      "batch 5507: loss 0.02104225754737854\n",
      "batch 5508: loss 0.0026836716569960117\n",
      "batch 5509: loss 0.0012935149716213346\n",
      "batch 5510: loss 0.006293070036917925\n",
      "batch 5511: loss 0.0007909552077762783\n",
      "batch 5512: loss 0.011483432725071907\n",
      "batch 5513: loss 0.024107135832309723\n",
      "batch 5514: loss 0.0003852984227705747\n",
      "batch 5515: loss 0.004171025473624468\n",
      "batch 5516: loss 0.0009545468492433429\n",
      "batch 5517: loss 0.0031499634496867657\n",
      "batch 5518: loss 0.04915192350745201\n",
      "batch 5519: loss 0.0005692947306670249\n",
      "batch 5520: loss 0.09736588597297668\n",
      "batch 5521: loss 0.007889095693826675\n",
      "batch 5522: loss 0.00021601028856821358\n",
      "batch 5523: loss 0.0031625430565327406\n",
      "batch 5524: loss 0.0015242828521877527\n",
      "batch 5525: loss 9.726328426040709e-05\n",
      "batch 5526: loss 0.15997417271137238\n",
      "batch 5527: loss 0.11329890787601471\n",
      "batch 5528: loss 7.187286246335134e-05\n",
      "batch 5529: loss 0.060054391622543335\n",
      "batch 5530: loss 0.012375938706099987\n",
      "batch 5531: loss 0.00414648512378335\n",
      "batch 5532: loss 0.00047326748608611524\n",
      "batch 5533: loss 0.00025019905297085643\n",
      "batch 5534: loss 0.011284084990620613\n",
      "batch 5535: loss 0.0012454574462026358\n",
      "batch 5536: loss 0.017484595999121666\n",
      "batch 5537: loss 0.006654884200543165\n",
      "batch 5538: loss 0.043508242815732956\n",
      "batch 5539: loss 0.002121519297361374\n",
      "batch 5540: loss 0.01143201906234026\n",
      "batch 5541: loss 0.021326355636119843\n",
      "batch 5542: loss 0.0026080410461872816\n",
      "batch 5543: loss 0.006998865865170956\n",
      "batch 5544: loss 0.034305326640605927\n",
      "batch 5545: loss 0.009163493290543556\n",
      "batch 5546: loss 0.0004136932548135519\n",
      "batch 5547: loss 0.008234531618654728\n",
      "batch 5548: loss 0.0010303000453859568\n",
      "batch 5549: loss 0.004111135378479958\n",
      "batch 5550: loss 0.0008848587167449296\n",
      "batch 5551: loss 0.0022841542959213257\n",
      "batch 5552: loss 0.003486508969217539\n",
      "batch 5553: loss 0.0005237889126874506\n",
      "batch 5554: loss 0.02593172714114189\n",
      "batch 5555: loss 0.000608314061537385\n",
      "batch 5556: loss 0.006554273888468742\n",
      "batch 5557: loss 0.001814710907638073\n",
      "batch 5558: loss 0.004275675863027573\n",
      "batch 5559: loss 0.004216683562844992\n",
      "batch 5560: loss 0.000535274448338896\n",
      "batch 5561: loss 0.01069476269185543\n",
      "batch 5562: loss 0.0036312048323452473\n",
      "batch 5563: loss 0.0023086622823029757\n",
      "batch 5564: loss 0.00036068304325453937\n",
      "batch 5565: loss 0.009034349583089352\n",
      "batch 5566: loss 0.0027849406469613314\n",
      "batch 5567: loss 0.04206717759370804\n",
      "batch 5568: loss 0.0014624823816120625\n",
      "batch 5569: loss 6.30182767054066e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5570: loss 0.001323210308328271\n",
      "batch 5571: loss 0.0011047988664358854\n",
      "batch 5572: loss 0.02368943952023983\n",
      "batch 5573: loss 0.0005871442845091224\n",
      "batch 5574: loss 0.05860806256532669\n",
      "batch 5575: loss 0.0009002575534395874\n",
      "batch 5576: loss 0.00034185178810730577\n",
      "batch 5577: loss 0.001874227193184197\n",
      "batch 5578: loss 0.0005311859422363341\n",
      "batch 5579: loss 0.001989217707887292\n",
      "batch 5580: loss 0.011564352549612522\n",
      "batch 5581: loss 0.0004749779764097184\n",
      "batch 5582: loss 0.007978730835020542\n",
      "batch 5583: loss 0.05049104988574982\n",
      "batch 5584: loss 0.002713562222197652\n",
      "batch 5585: loss 0.03289145603775978\n",
      "batch 5586: loss 0.009454762563109398\n",
      "batch 5587: loss 0.02004971168935299\n",
      "batch 5588: loss 0.0002732576976995915\n",
      "batch 5589: loss 2.735700400080532e-05\n",
      "batch 5590: loss 0.00022172163880895823\n",
      "batch 5591: loss 5.05511270603165e-05\n",
      "batch 5592: loss 0.010385308414697647\n",
      "batch 5593: loss 0.1768561154603958\n",
      "batch 5594: loss 0.00015565431385766715\n",
      "batch 5595: loss 0.010944277048110962\n",
      "batch 5596: loss 0.0036252886056900024\n",
      "batch 5597: loss 4.3458203435875475e-05\n",
      "batch 5598: loss 0.10599886626005173\n",
      "batch 5599: loss 0.000199279427761212\n",
      "batch 5600: loss 0.012027640827000141\n",
      "batch 5601: loss 0.006936880759894848\n",
      "batch 5602: loss 0.003673650324344635\n",
      "batch 5603: loss 0.003570764558389783\n",
      "batch 5604: loss 0.07680432498455048\n",
      "batch 5605: loss 0.04975167289376259\n",
      "batch 5606: loss 0.0008327524410560727\n",
      "batch 5607: loss 0.00045747702824883163\n",
      "batch 5608: loss 0.015178073197603226\n",
      "batch 5609: loss 0.00018119841115549207\n",
      "batch 5610: loss 0.020622534677386284\n",
      "batch 5611: loss 0.014375557191669941\n",
      "batch 5612: loss 0.003109308425337076\n",
      "batch 5613: loss 0.000616971985436976\n",
      "batch 5614: loss 0.0013421457260847092\n",
      "batch 5615: loss 0.04677113518118858\n",
      "batch 5616: loss 0.00018748981528915465\n",
      "batch 5617: loss 0.0007748606149107218\n",
      "batch 5618: loss 8.020192035473883e-05\n",
      "batch 5619: loss 0.003256243420764804\n",
      "batch 5620: loss 0.0010391463292762637\n",
      "batch 5621: loss 0.006312126759439707\n",
      "batch 5622: loss 0.0003636412147898227\n",
      "batch 5623: loss 7.784202171023935e-05\n",
      "batch 5624: loss 0.0014180971775203943\n",
      "batch 5625: loss 0.0004580356180667877\n",
      "batch 5626: loss 0.00589372031390667\n",
      "batch 5627: loss 0.000151461674249731\n",
      "batch 5628: loss 0.004175151698291302\n",
      "batch 5629: loss 0.0007871128618717194\n",
      "batch 5630: loss 0.04409582167863846\n",
      "batch 5631: loss 2.930210393969901e-05\n",
      "batch 5632: loss 0.0012206421233713627\n",
      "batch 5633: loss 0.010470063425600529\n",
      "batch 5634: loss 0.013150986284017563\n",
      "batch 5635: loss 0.00018750305753201246\n",
      "batch 5636: loss 0.0003240484220441431\n",
      "batch 5637: loss 0.038107290863990784\n",
      "batch 5638: loss 0.0005272990092635155\n",
      "batch 5639: loss 0.010427632369101048\n",
      "batch 5640: loss 0.000718376599252224\n",
      "batch 5641: loss 0.006178000941872597\n",
      "batch 5642: loss 0.08427740633487701\n",
      "batch 5643: loss 0.029072361066937447\n",
      "batch 5644: loss 0.020592842251062393\n",
      "batch 5645: loss 0.00867950264364481\n",
      "batch 5646: loss 0.0024062907323241234\n",
      "batch 5647: loss 0.0076453797519207\n",
      "batch 5648: loss 0.004556426778435707\n",
      "batch 5649: loss 0.045651402324438095\n",
      "batch 5650: loss 0.14902064204216003\n",
      "batch 5651: loss 9.932075045071542e-05\n",
      "batch 5652: loss 0.0038132581394165754\n",
      "batch 5653: loss 0.0012392502976581454\n",
      "batch 5654: loss 0.0006011665682308376\n",
      "batch 5655: loss 0.0016772184753790498\n",
      "batch 5656: loss 3.0409339160542004e-05\n",
      "batch 5657: loss 0.030610699206590652\n",
      "batch 5658: loss 0.004191569983959198\n",
      "batch 5659: loss 0.14203883707523346\n",
      "batch 5660: loss 0.00041473936289548874\n",
      "batch 5661: loss 0.020108848810195923\n",
      "batch 5662: loss 0.019697915762662888\n",
      "batch 5663: loss 0.18427880108356476\n",
      "batch 5664: loss 0.060672979801893234\n",
      "batch 5665: loss 0.0005699243047274649\n",
      "batch 5666: loss 2.4218728867708705e-05\n",
      "batch 5667: loss 0.0004186295554973185\n",
      "batch 5668: loss 2.80560470855562e-05\n",
      "batch 5669: loss 0.0006090279784984887\n",
      "batch 5670: loss 0.047999057918787\n",
      "batch 5671: loss 0.015538026578724384\n",
      "batch 5672: loss 0.0011143606388941407\n",
      "batch 5673: loss 0.022655989974737167\n",
      "batch 5674: loss 0.068385049700737\n",
      "batch 5675: loss 0.00031876511638984084\n",
      "batch 5676: loss 0.04052327200770378\n",
      "batch 5677: loss 0.02552490122616291\n",
      "batch 5678: loss 0.0004222520219627768\n",
      "batch 5679: loss 0.011170551180839539\n",
      "batch 5680: loss 0.0018425240414217114\n",
      "batch 5681: loss 0.17537185549736023\n",
      "batch 5682: loss 0.0005093287327326834\n",
      "batch 5683: loss 0.0003835499519482255\n",
      "batch 5684: loss 0.0009778202511370182\n",
      "batch 5685: loss 0.0006197202019393444\n",
      "batch 5686: loss 0.0027570645324885845\n",
      "batch 5687: loss 0.010162372142076492\n",
      "batch 5688: loss 0.012325086630880833\n",
      "batch 5689: loss 0.0011228697840124369\n",
      "batch 5690: loss 0.012587103992700577\n",
      "batch 5691: loss 0.007095998153090477\n",
      "batch 5692: loss 0.002189507009461522\n",
      "batch 5693: loss 0.0050892275758087635\n",
      "batch 5694: loss 0.00600948091596365\n",
      "batch 5695: loss 0.003792477771639824\n",
      "batch 5696: loss 0.0009468094795010984\n",
      "batch 5697: loss 0.01804307848215103\n",
      "batch 5698: loss 0.046482909470796585\n",
      "batch 5699: loss 0.0009027826599776745\n",
      "batch 5700: loss 0.0017365217208862305\n",
      "batch 5701: loss 0.002736845286563039\n",
      "batch 5702: loss 0.0018564271740615368\n",
      "batch 5703: loss 0.0005431539611890912\n",
      "batch 5704: loss 0.0001523458049632609\n",
      "batch 5705: loss 0.005801791325211525\n",
      "batch 5706: loss 0.012657740153372288\n",
      "batch 5707: loss 0.0005789755959995091\n",
      "batch 5708: loss 0.0031324841547757387\n",
      "batch 5709: loss 0.00024884557933546603\n",
      "batch 5710: loss 0.00039447363815270364\n",
      "batch 5711: loss 0.02478143572807312\n",
      "batch 5712: loss 0.06561087816953659\n",
      "batch 5713: loss 0.01726076379418373\n",
      "batch 5714: loss 0.0002879062667489052\n",
      "batch 5715: loss 0.0002715175214689225\n",
      "batch 5716: loss 0.0006201498908922076\n",
      "batch 5717: loss 0.0011043939739465714\n",
      "batch 5718: loss 0.0007742359302937984\n",
      "batch 5719: loss 0.002427721628919244\n",
      "batch 5720: loss 0.0020038632210344076\n",
      "batch 5721: loss 0.0002839796943590045\n",
      "batch 5722: loss 0.0012435504468157887\n",
      "batch 5723: loss 0.00047654795343987644\n",
      "batch 5724: loss 0.021117430180311203\n",
      "batch 5725: loss 0.0019130483269691467\n",
      "batch 5726: loss 0.0025262823328375816\n",
      "batch 5727: loss 0.18325303494930267\n",
      "batch 5728: loss 0.05385671555995941\n",
      "batch 5729: loss 0.0025086060632020235\n",
      "batch 5730: loss 0.0014423142420127988\n",
      "batch 5731: loss 0.0005010905442759395\n",
      "batch 5732: loss 0.008819515816867352\n",
      "batch 5733: loss 0.004548246040940285\n",
      "batch 5734: loss 0.0031196814961731434\n",
      "batch 5735: loss 0.001435591490007937\n",
      "batch 5736: loss 0.013887129724025726\n",
      "batch 5737: loss 0.0040292744524776936\n",
      "batch 5738: loss 0.001123421941883862\n",
      "batch 5739: loss 0.007550455164164305\n",
      "batch 5740: loss 0.00046584574738517404\n",
      "batch 5741: loss 0.00013971174485050142\n",
      "batch 5742: loss 0.005404082592576742\n",
      "batch 5743: loss 0.001206776942126453\n",
      "batch 5744: loss 0.04300973191857338\n",
      "batch 5745: loss 0.07588033378124237\n",
      "batch 5746: loss 0.00013764270988758653\n",
      "batch 5747: loss 0.0010269088670611382\n",
      "batch 5748: loss 0.00045681933988817036\n",
      "batch 5749: loss 0.003792818635702133\n",
      "batch 5750: loss 0.029474925249814987\n",
      "batch 5751: loss 0.0014645415358245373\n",
      "batch 5752: loss 0.03518740087747574\n",
      "batch 5753: loss 0.02050204388797283\n",
      "batch 5754: loss 0.0025490547996014357\n",
      "batch 5755: loss 0.0002968748740386218\n",
      "batch 5756: loss 0.0015892806695774198\n",
      "batch 5757: loss 0.007516480516642332\n",
      "batch 5758: loss 0.012274891138076782\n",
      "batch 5759: loss 0.09042617678642273\n",
      "batch 5760: loss 0.0021181677002459764\n",
      "batch 5761: loss 0.0004738971474580467\n",
      "batch 5762: loss 0.0009887340711429715\n",
      "batch 5763: loss 0.0008263702038675547\n",
      "batch 5764: loss 0.0002811669837683439\n",
      "batch 5765: loss 0.00011662834003800526\n",
      "batch 5766: loss 0.002767079509794712\n",
      "batch 5767: loss 0.00031237618532031775\n",
      "batch 5768: loss 0.0002898166130762547\n",
      "batch 5769: loss 0.0070300595834851265\n",
      "batch 5770: loss 0.00262461113743484\n",
      "batch 5771: loss 0.0019747759215533733\n",
      "batch 5772: loss 0.002406067680567503\n",
      "batch 5773: loss 0.000721690128557384\n",
      "batch 5774: loss 0.024900948628783226\n",
      "batch 5775: loss 0.0011906458530575037\n",
      "batch 5776: loss 0.010480639524757862\n",
      "batch 5777: loss 0.007479340769350529\n",
      "batch 5778: loss 0.00040356453973799944\n",
      "batch 5779: loss 0.0006053889519535005\n",
      "batch 5780: loss 0.0005954684456810355\n",
      "batch 5781: loss 0.007098074536770582\n",
      "batch 5782: loss 0.07747428864240646\n",
      "batch 5783: loss 4.988678119843826e-05\n",
      "batch 5784: loss 6.599789776373655e-05\n",
      "batch 5785: loss 0.003644453827291727\n",
      "batch 5786: loss 0.0025904886424541473\n",
      "batch 5787: loss 0.006530502811074257\n",
      "batch 5788: loss 0.0004158713563811034\n",
      "batch 5789: loss 0.02255532518029213\n",
      "batch 5790: loss 0.0013962663942947984\n",
      "batch 5791: loss 0.007330165710300207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5792: loss 0.0027015593368560076\n",
      "batch 5793: loss 0.001057080109603703\n",
      "batch 5794: loss 0.009933979250490665\n",
      "batch 5795: loss 0.00024127011420205235\n",
      "batch 5796: loss 0.06870821118354797\n",
      "batch 5797: loss 0.001154269208200276\n",
      "batch 5798: loss 2.0538409444270656e-05\n",
      "batch 5799: loss 0.000708973384462297\n",
      "batch 5800: loss 0.012124080210924149\n",
      "batch 5801: loss 0.0006156201125122607\n",
      "batch 5802: loss 0.0018110884120687842\n",
      "batch 5803: loss 0.1258605420589447\n",
      "batch 5804: loss 0.0006567036034539342\n",
      "batch 5805: loss 0.00011652259127004072\n",
      "batch 5806: loss 0.0007057441398501396\n",
      "batch 5807: loss 0.011545826680958271\n",
      "batch 5808: loss 0.009074670262634754\n",
      "batch 5809: loss 0.005543810781091452\n",
      "batch 5810: loss 0.00011789753625635058\n",
      "batch 5811: loss 0.002891519572585821\n",
      "batch 5812: loss 0.1610947549343109\n",
      "batch 5813: loss 0.009782519191503525\n",
      "batch 5814: loss 0.00044655505917035043\n",
      "batch 5815: loss 0.034692514687776566\n",
      "batch 5816: loss 0.031478963792324066\n",
      "batch 5817: loss 0.0015451053623110056\n",
      "batch 5818: loss 0.00010426531662233174\n",
      "batch 5819: loss 0.02210010215640068\n",
      "batch 5820: loss 0.012340092100203037\n",
      "batch 5821: loss 0.0002663696650415659\n",
      "batch 5822: loss 0.0005075687658973038\n",
      "batch 5823: loss 0.012040570378303528\n",
      "batch 5824: loss 0.0005887026782147586\n",
      "batch 5825: loss 0.008318154141306877\n",
      "batch 5826: loss 0.009172054007649422\n",
      "batch 5827: loss 0.007975402288138866\n",
      "batch 5828: loss 0.00042517384281381965\n",
      "batch 5829: loss 0.047787364572286606\n",
      "batch 5830: loss 0.005066578276455402\n",
      "batch 5831: loss 7.271734648384154e-05\n",
      "batch 5832: loss 0.00032609596382826567\n",
      "batch 5833: loss 0.0006620695348829031\n",
      "batch 5834: loss 0.006848039571195841\n",
      "batch 5835: loss 0.0026468103751540184\n",
      "batch 5836: loss 0.0012668173294514418\n",
      "batch 5837: loss 0.0006392174982465804\n",
      "batch 5838: loss 0.0001004941004794091\n",
      "batch 5839: loss 0.0037324526347219944\n",
      "batch 5840: loss 0.005291185341775417\n",
      "batch 5841: loss 0.00030166408396326005\n",
      "batch 5842: loss 0.0011806527618318796\n",
      "batch 5843: loss 0.0020621747244149446\n",
      "batch 5844: loss 9.244763350579888e-05\n",
      "batch 5845: loss 0.024869505316019058\n",
      "batch 5846: loss 0.01269562728703022\n",
      "batch 5847: loss 2.230833524663467e-05\n",
      "batch 5848: loss 0.00014988180191721767\n",
      "batch 5849: loss 0.009695222601294518\n",
      "batch 5850: loss 0.06948288530111313\n",
      "batch 5851: loss 0.006605029106140137\n",
      "batch 5852: loss 0.06186182051897049\n",
      "batch 5853: loss 0.0739411860704422\n",
      "batch 5854: loss 0.0005403825780376792\n",
      "batch 5855: loss 0.0012456360273063183\n",
      "batch 5856: loss 0.0010996013879776\n",
      "batch 5857: loss 0.004334673751145601\n",
      "batch 5858: loss 0.000799578323494643\n",
      "batch 5859: loss 0.0015619469340890646\n",
      "batch 5860: loss 0.00015141045150812715\n",
      "batch 5861: loss 0.0009205794194713235\n",
      "batch 5862: loss 0.000529229233507067\n",
      "batch 5863: loss 0.0013933447189629078\n",
      "batch 5864: loss 0.007698868867009878\n",
      "batch 5865: loss 0.012890870682895184\n",
      "batch 5866: loss 0.2065524309873581\n",
      "batch 5867: loss 0.03516288846731186\n",
      "batch 5868: loss 0.00016953606973402202\n",
      "batch 5869: loss 0.022501274943351746\n",
      "batch 5870: loss 0.005315400660037994\n",
      "batch 5871: loss 0.00013718492118641734\n",
      "batch 5872: loss 0.000600586470682174\n",
      "batch 5873: loss 0.004107228945940733\n",
      "batch 5874: loss 0.0035574613139033318\n",
      "batch 5875: loss 0.00035861568176187575\n",
      "batch 5876: loss 0.00030308033456094563\n",
      "batch 5877: loss 0.002359406789764762\n",
      "batch 5878: loss 0.001406615017913282\n",
      "batch 5879: loss 0.017275575548410416\n",
      "batch 5880: loss 0.015580341219902039\n",
      "batch 5881: loss 0.00022013347188476473\n",
      "batch 5882: loss 0.002098829485476017\n",
      "batch 5883: loss 0.011242439039051533\n",
      "batch 5884: loss 0.0028410230297595263\n",
      "batch 5885: loss 0.00013490435958374292\n",
      "batch 5886: loss 8.462590631097555e-05\n",
      "batch 5887: loss 0.010223443619906902\n",
      "batch 5888: loss 0.00039470495539717376\n",
      "batch 5889: loss 0.00047764027840457857\n",
      "batch 5890: loss 0.04325365275144577\n",
      "batch 5891: loss 0.04493127763271332\n",
      "batch 5892: loss 0.00017643730097915977\n",
      "batch 5893: loss 0.004726167302578688\n",
      "batch 5894: loss 0.0007600797689519823\n",
      "batch 5895: loss 0.0008611060329712927\n",
      "batch 5896: loss 0.011951212771236897\n",
      "batch 5897: loss 0.0004945611581206322\n",
      "batch 5898: loss 0.005308074411004782\n",
      "batch 5899: loss 0.0017523006536066532\n",
      "batch 5900: loss 0.003217069199308753\n",
      "batch 5901: loss 0.017660824581980705\n",
      "batch 5902: loss 0.0003764372377190739\n",
      "batch 5903: loss 0.003317782422527671\n",
      "batch 5904: loss 0.002412274945527315\n",
      "batch 5905: loss 0.00011468655429780483\n",
      "batch 5906: loss 0.002617815975099802\n",
      "batch 5907: loss 0.00046924184425733984\n",
      "batch 5908: loss 0.000867960974574089\n",
      "batch 5909: loss 0.13025011122226715\n",
      "batch 5910: loss 0.028718743473291397\n",
      "batch 5911: loss 8.898379746824503e-05\n",
      "batch 5912: loss 0.12648272514343262\n",
      "batch 5913: loss 0.009192156605422497\n",
      "batch 5914: loss 0.0033519582357257605\n",
      "batch 5915: loss 0.01063639298081398\n",
      "batch 5916: loss 9.685343684395775e-05\n",
      "batch 5917: loss 0.05063996836543083\n",
      "batch 5918: loss 0.03537366911768913\n",
      "batch 5919: loss 0.049487531185150146\n",
      "batch 5920: loss 0.003412923775613308\n",
      "batch 5921: loss 0.0007684204610995948\n",
      "batch 5922: loss 0.002638674806803465\n",
      "batch 5923: loss 0.011414434760808945\n",
      "batch 5924: loss 0.01916772499680519\n",
      "batch 5925: loss 0.04505480080842972\n",
      "batch 5926: loss 0.0061424896121025085\n",
      "batch 5927: loss 0.0024741196539252996\n",
      "batch 5928: loss 0.00044262519804760814\n",
      "batch 5929: loss 0.0009002509759739041\n",
      "batch 5930: loss 0.13915883004665375\n",
      "batch 5931: loss 0.0005947693134658039\n",
      "batch 5932: loss 0.013744916766881943\n",
      "batch 5933: loss 0.002156271832063794\n",
      "batch 5934: loss 0.0006528414669446647\n",
      "batch 5935: loss 0.00010731600195867941\n",
      "batch 5936: loss 0.002302131149917841\n",
      "batch 5937: loss 0.0020653957035392523\n",
      "batch 5938: loss 0.0006454791873693466\n",
      "batch 5939: loss 0.020556695759296417\n",
      "batch 5940: loss 0.02382156066596508\n",
      "batch 5941: loss 0.04934941232204437\n",
      "batch 5942: loss 0.0016207362059503794\n",
      "batch 5943: loss 0.01577787473797798\n",
      "batch 5944: loss 0.002075855154544115\n",
      "batch 5945: loss 0.07612483203411102\n",
      "batch 5946: loss 0.09095294028520584\n",
      "batch 5947: loss 0.0022849359083920717\n",
      "batch 5948: loss 0.0009141889167949557\n",
      "batch 5949: loss 0.03125642240047455\n",
      "batch 5950: loss 0.002035693731158972\n",
      "batch 5951: loss 0.000414539419580251\n",
      "batch 5952: loss 0.08719800412654877\n",
      "batch 5953: loss 0.010452255606651306\n",
      "batch 5954: loss 0.002775501227006316\n",
      "batch 5955: loss 0.005567758344113827\n",
      "batch 5956: loss 0.003322402946650982\n",
      "batch 5957: loss 0.03146559000015259\n",
      "batch 5958: loss 0.018213078379631042\n",
      "batch 5959: loss 0.0100187286734581\n",
      "batch 5960: loss 0.0023348394315689802\n",
      "batch 5961: loss 0.0007529009599238634\n",
      "batch 5962: loss 0.001091164886020124\n",
      "batch 5963: loss 0.012915304861962795\n",
      "batch 5964: loss 0.00817571859806776\n",
      "batch 5965: loss 0.000423560879426077\n",
      "batch 5966: loss 0.001283364719711244\n",
      "batch 5967: loss 1.4061902220419142e-05\n",
      "batch 5968: loss 0.001694486360065639\n",
      "batch 5969: loss 0.00041447492549195886\n",
      "batch 5970: loss 0.00047664708108641207\n",
      "batch 5971: loss 0.009109379723668098\n",
      "batch 5972: loss 0.00010248876787954941\n",
      "batch 5973: loss 0.00010178146476391703\n",
      "batch 5974: loss 0.0001506618136772886\n",
      "batch 5975: loss 0.00879300944507122\n",
      "batch 5976: loss 0.0022245633881539106\n",
      "batch 5977: loss 0.0004981225356459618\n",
      "batch 5978: loss 0.02582404762506485\n",
      "batch 5979: loss 0.0006652144365943968\n",
      "batch 5980: loss 0.003915025852620602\n",
      "batch 5981: loss 0.0002555055543780327\n",
      "batch 5982: loss 0.0984821617603302\n",
      "batch 5983: loss 0.0018360670655965805\n",
      "batch 5984: loss 0.0015014654491096735\n",
      "batch 5985: loss 0.02790677174925804\n",
      "batch 5986: loss 0.0016544210957363248\n",
      "batch 5987: loss 0.0004347819776739925\n",
      "batch 5988: loss 9.05339329619892e-05\n",
      "batch 5989: loss 0.009835834614932537\n",
      "batch 5990: loss 0.008934910409152508\n",
      "batch 5991: loss 0.0011189747601747513\n",
      "batch 5992: loss 0.001966744428500533\n",
      "batch 5993: loss 0.007302990183234215\n",
      "batch 5994: loss 0.0888301283121109\n",
      "batch 5995: loss 0.0024045042227953672\n",
      "batch 5996: loss 0.0028561213985085487\n",
      "batch 5997: loss 0.0004667530592996627\n",
      "batch 5998: loss 0.007437502965331078\n",
      "batch 5999: loss 0.046696919947862625\n",
      "batch 6000: loss 0.0817345604300499\n",
      "batch 6001: loss 0.00015441815776284784\n",
      "batch 6002: loss 0.054542750120162964\n",
      "batch 6003: loss 0.05434738099575043\n",
      "batch 6004: loss 0.02253902144730091\n",
      "batch 6005: loss 0.01825292967259884\n",
      "batch 6006: loss 0.00097980338614434\n",
      "batch 6007: loss 0.01521991565823555\n",
      "batch 6008: loss 0.039777208119630814\n",
      "batch 6009: loss 0.0008935892838053405\n",
      "batch 6010: loss 0.006946977227926254\n",
      "batch 6011: loss 0.0003234501928091049\n",
      "batch 6012: loss 0.05423102155327797\n",
      "batch 6013: loss 0.0014865690609440207\n",
      "batch 6014: loss 0.006539337337017059\n",
      "batch 6015: loss 0.030010700225830078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6016: loss 0.03032013028860092\n",
      "batch 6017: loss 0.001152446260675788\n",
      "batch 6018: loss 0.030816171318292618\n",
      "batch 6019: loss 0.003380736568942666\n",
      "batch 6020: loss 0.0013165405252948403\n",
      "batch 6021: loss 0.0018430263735353947\n",
      "batch 6022: loss 0.012602795846760273\n",
      "batch 6023: loss 0.06673813611268997\n",
      "batch 6024: loss 0.0005456013604998589\n",
      "batch 6025: loss 0.006038650870323181\n",
      "batch 6026: loss 0.0053640068508684635\n",
      "batch 6027: loss 0.003205454209819436\n",
      "batch 6028: loss 0.02020327001810074\n",
      "batch 6029: loss 0.0016686299350112677\n",
      "batch 6030: loss 0.00029703023028559983\n",
      "batch 6031: loss 0.00013404502533376217\n",
      "batch 6032: loss 0.018389582633972168\n",
      "batch 6033: loss 0.015837756916880608\n",
      "batch 6034: loss 0.00012563001655507833\n",
      "batch 6035: loss 0.0016647082520648837\n",
      "batch 6036: loss 0.010162238962948322\n",
      "batch 6037: loss 0.006495073437690735\n",
      "batch 6038: loss 0.0009300025412812829\n",
      "batch 6039: loss 0.0008012953912839293\n",
      "batch 6040: loss 0.008483052253723145\n",
      "batch 6041: loss 0.004813905339688063\n",
      "batch 6042: loss 0.007666692137718201\n",
      "batch 6043: loss 0.011088578030467033\n",
      "batch 6044: loss 0.00020364885858725756\n",
      "batch 6045: loss 0.009458866901695728\n",
      "batch 6046: loss 0.0002011732867686078\n",
      "batch 6047: loss 5.2775514632230625e-05\n",
      "batch 6048: loss 0.00033436063677072525\n",
      "batch 6049: loss 0.010246052406728268\n",
      "batch 6050: loss 0.00668435450643301\n",
      "batch 6051: loss 0.0013156826607882977\n",
      "batch 6052: loss 1.3739648238697555e-05\n",
      "batch 6053: loss 0.000619663973338902\n",
      "batch 6054: loss 0.0003056054993066937\n",
      "batch 6055: loss 7.248102338053286e-05\n",
      "batch 6056: loss 9.48653687373735e-05\n",
      "batch 6057: loss 0.004747406579554081\n",
      "batch 6058: loss 0.002953500486910343\n",
      "batch 6059: loss 0.0198261309415102\n",
      "batch 6060: loss 0.0005214356351643801\n",
      "batch 6061: loss 4.499676651903428e-05\n",
      "batch 6062: loss 0.03560614958405495\n",
      "batch 6063: loss 0.018852297216653824\n",
      "batch 6064: loss 5.0461883802199736e-05\n",
      "batch 6065: loss 0.01629178412258625\n",
      "batch 6066: loss 0.0002985158935189247\n",
      "batch 6067: loss 0.00036627217195928097\n",
      "batch 6068: loss 0.003652566345408559\n",
      "batch 6069: loss 0.32244163751602173\n",
      "batch 6070: loss 0.014763325452804565\n",
      "batch 6071: loss 2.4015462258830667e-05\n",
      "batch 6072: loss 8.152399823302403e-05\n",
      "batch 6073: loss 0.0007253391086123884\n",
      "batch 6074: loss 0.0007441967609338462\n",
      "batch 6075: loss 0.00017343784566037357\n",
      "batch 6076: loss 6.929812661837786e-05\n",
      "batch 6077: loss 0.0013107728445902467\n",
      "batch 6078: loss 2.6424938823765842e-06\n",
      "batch 6079: loss 0.00019165436970070004\n",
      "batch 6080: loss 0.00039160801679827273\n",
      "batch 6081: loss 0.0003449124051257968\n",
      "batch 6082: loss 5.089486512588337e-05\n",
      "batch 6083: loss 0.0003293730551376939\n",
      "batch 6084: loss 0.0017122668214142323\n",
      "batch 6085: loss 0.0015905153704807162\n",
      "batch 6086: loss 0.08208534121513367\n",
      "batch 6087: loss 0.0005217744037508965\n",
      "batch 6088: loss 0.00010508003470022231\n",
      "batch 6089: loss 0.0010002590715885162\n",
      "batch 6090: loss 0.0004165159189142287\n",
      "batch 6091: loss 0.00021956994896754622\n",
      "batch 6092: loss 0.0014349520206451416\n",
      "batch 6093: loss 0.0008174594258889556\n",
      "batch 6094: loss 0.00017839393694885075\n",
      "batch 6095: loss 8.84037435753271e-05\n",
      "batch 6096: loss 0.0010125561384484172\n",
      "batch 6097: loss 0.008528382517397404\n",
      "batch 6098: loss 0.00012167204840807244\n",
      "batch 6099: loss 0.0010947377886623144\n",
      "batch 6100: loss 0.0005361007060855627\n",
      "batch 6101: loss 0.0024752202443778515\n",
      "batch 6102: loss 0.0013926686951890588\n",
      "batch 6103: loss 0.010844524949789047\n",
      "batch 6104: loss 0.00907567422837019\n",
      "batch 6105: loss 0.0009176213061437011\n",
      "batch 6106: loss 0.0015779623063281178\n",
      "batch 6107: loss 0.08293374627828598\n",
      "batch 6108: loss 0.003695183200761676\n",
      "batch 6109: loss 0.000497420143801719\n",
      "batch 6110: loss 0.020980235189199448\n",
      "batch 6111: loss 0.04486166685819626\n",
      "batch 6112: loss 0.02822810783982277\n",
      "batch 6113: loss 0.0009089583763852715\n",
      "batch 6114: loss 0.00017315524746663868\n",
      "batch 6115: loss 0.006981798447668552\n",
      "batch 6116: loss 4.599162275553681e-05\n",
      "batch 6117: loss 0.00010251852654619142\n",
      "batch 6118: loss 2.37179956457112e-05\n",
      "batch 6119: loss 0.004043282009661198\n",
      "batch 6120: loss 0.0010126304114237428\n",
      "batch 6121: loss 0.11745014041662216\n",
      "batch 6122: loss 0.0026611220091581345\n",
      "batch 6123: loss 0.2648489773273468\n",
      "batch 6124: loss 0.05119425058364868\n",
      "batch 6125: loss 0.0001496851327829063\n",
      "batch 6126: loss 0.07725232094526291\n",
      "batch 6127: loss 0.014942038804292679\n",
      "batch 6128: loss 0.006579005625098944\n",
      "batch 6129: loss 0.000196869921637699\n",
      "batch 6130: loss 0.004421725403517485\n",
      "batch 6131: loss 0.019307006150484085\n",
      "batch 6132: loss 0.0008863725233823061\n",
      "batch 6133: loss 0.0015143060591071844\n",
      "batch 6134: loss 0.0001489546848461032\n",
      "batch 6135: loss 0.0002451388572808355\n",
      "batch 6136: loss 0.01031768973916769\n",
      "batch 6137: loss 0.1162390410900116\n",
      "batch 6138: loss 0.0024878215044736862\n",
      "batch 6139: loss 0.000731263542547822\n",
      "batch 6140: loss 0.00024398036475759\n",
      "batch 6141: loss 0.0004953246680088341\n",
      "batch 6142: loss 0.002142510609701276\n",
      "batch 6143: loss 0.055275119841098785\n",
      "batch 6144: loss 0.0001323593605775386\n",
      "batch 6145: loss 0.0014091699849814177\n",
      "batch 6146: loss 0.0003593676083255559\n",
      "batch 6147: loss 0.086790531873703\n",
      "batch 6148: loss 0.001039999071508646\n",
      "batch 6149: loss 0.008323747664690018\n",
      "batch 6150: loss 0.021792907267808914\n",
      "batch 6151: loss 0.0031501122284680605\n",
      "batch 6152: loss 0.000305656751152128\n",
      "batch 6153: loss 0.000779472931753844\n",
      "batch 6154: loss 0.0016199148958548903\n",
      "batch 6155: loss 1.7671163732302375e-05\n",
      "batch 6156: loss 0.00028371691587381065\n",
      "batch 6157: loss 0.003894074587151408\n",
      "batch 6158: loss 0.0008709224057383835\n",
      "batch 6159: loss 0.0024822172708809376\n",
      "batch 6160: loss 0.0014573776861652732\n",
      "batch 6161: loss 0.0009390637278556824\n",
      "batch 6162: loss 0.00017851126904133707\n",
      "batch 6163: loss 0.01775246672332287\n",
      "batch 6164: loss 0.0017350525595247746\n",
      "batch 6165: loss 0.0028442158363759518\n",
      "batch 6166: loss 0.007146465592086315\n",
      "batch 6167: loss 0.002030820120126009\n",
      "batch 6168: loss 0.004294813144952059\n",
      "batch 6169: loss 0.005530454218387604\n",
      "batch 6170: loss 0.0006170165142975748\n",
      "batch 6171: loss 0.00042490940541028976\n",
      "batch 6172: loss 0.001054275780916214\n",
      "batch 6173: loss 0.0011791043216362596\n",
      "batch 6174: loss 0.0005988595657981932\n",
      "batch 6175: loss 9.473151294514537e-05\n",
      "batch 6176: loss 6.7663706431631e-05\n",
      "batch 6177: loss 0.02997143194079399\n",
      "batch 6178: loss 0.00016491542919538915\n",
      "batch 6179: loss 0.0015753691550344229\n",
      "batch 6180: loss 0.011437918059527874\n",
      "batch 6181: loss 0.0017807683907449245\n",
      "batch 6182: loss 0.015256738290190697\n",
      "batch 6183: loss 0.01720760390162468\n",
      "batch 6184: loss 0.02330240234732628\n",
      "batch 6185: loss 0.09853271394968033\n",
      "batch 6186: loss 2.0968083845218644e-05\n",
      "batch 6187: loss 0.006420140620321035\n",
      "batch 6188: loss 0.10070919990539551\n",
      "batch 6189: loss 0.009632035158574581\n",
      "batch 6190: loss 0.00012933346442878246\n",
      "batch 6191: loss 0.001553023001179099\n",
      "batch 6192: loss 0.000198991852812469\n",
      "batch 6193: loss 0.0014372242148965597\n",
      "batch 6194: loss 0.019385861232876778\n",
      "batch 6195: loss 0.005173849873244762\n",
      "batch 6196: loss 8.360956417163834e-05\n",
      "batch 6197: loss 0.0044702691957354546\n",
      "batch 6198: loss 0.004549926612526178\n",
      "batch 6199: loss 0.001932382001541555\n",
      "batch 6200: loss 0.04261219874024391\n",
      "batch 6201: loss 0.00029076190548948944\n",
      "batch 6202: loss 0.0003790979098994285\n",
      "batch 6203: loss 0.0016942567890509963\n",
      "batch 6204: loss 0.009021139703691006\n",
      "batch 6205: loss 0.0005246152286417782\n",
      "batch 6206: loss 0.00017244629270862788\n",
      "batch 6207: loss 0.0001377385633531958\n",
      "batch 6208: loss 0.001722083194181323\n",
      "batch 6209: loss 0.0008405312546528876\n",
      "batch 6210: loss 0.0007496304460801184\n",
      "batch 6211: loss 0.0008974879165180027\n",
      "batch 6212: loss 0.07259542495012283\n",
      "batch 6213: loss 0.0028126859106123447\n",
      "batch 6214: loss 0.00011002624523825943\n",
      "batch 6215: loss 0.00020672104437835515\n",
      "batch 6216: loss 0.10793175548315048\n",
      "batch 6217: loss 0.003275410272181034\n",
      "batch 6218: loss 0.0030169549863785505\n",
      "batch 6219: loss 0.0001937861816259101\n",
      "batch 6220: loss 0.00045789845171384513\n",
      "batch 6221: loss 0.03070576675236225\n",
      "batch 6222: loss 0.003291972214356065\n",
      "batch 6223: loss 0.006856964901089668\n",
      "batch 6224: loss 0.0020128150936216116\n",
      "batch 6225: loss 0.11586853116750717\n",
      "batch 6226: loss 0.002195828128606081\n",
      "batch 6227: loss 0.0022931687999516726\n",
      "batch 6228: loss 0.0213276706635952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6229: loss 0.06638205051422119\n",
      "batch 6230: loss 0.0004540296795312315\n",
      "batch 6231: loss 0.05173855274915695\n",
      "batch 6232: loss 0.017360074445605278\n",
      "batch 6233: loss 0.01586817018687725\n",
      "batch 6234: loss 0.0011923828860744834\n",
      "batch 6235: loss 0.00028034066781401634\n",
      "batch 6236: loss 0.017647160217165947\n",
      "batch 6237: loss 0.059931010007858276\n",
      "batch 6238: loss 0.001485595479607582\n",
      "batch 6239: loss 0.0009853594237938523\n",
      "batch 6240: loss 0.05373021960258484\n",
      "batch 6241: loss 7.958881178637967e-05\n",
      "batch 6242: loss 0.002976398915052414\n",
      "batch 6243: loss 0.008329026401042938\n",
      "batch 6244: loss 0.00024713188759051263\n",
      "batch 6245: loss 0.041275832802057266\n",
      "batch 6246: loss 0.002159995026886463\n",
      "batch 6247: loss 0.012759950011968613\n",
      "batch 6248: loss 0.12798751890659332\n",
      "batch 6249: loss 0.01726256124675274\n",
      "batch 6250: loss 0.052578553557395935\n",
      "batch 6251: loss 0.012813814915716648\n",
      "batch 6252: loss 5.77580722165294e-05\n",
      "batch 6253: loss 0.0007283962913788855\n",
      "batch 6254: loss 0.010664638131856918\n",
      "batch 6255: loss 0.0008993056253530085\n",
      "batch 6256: loss 0.021645715460181236\n",
      "batch 6257: loss 0.014266724698245525\n",
      "batch 6258: loss 0.0067690154537558556\n",
      "batch 6259: loss 0.06806715577840805\n",
      "batch 6260: loss 0.05172020196914673\n",
      "batch 6261: loss 0.00327300070784986\n",
      "batch 6262: loss 0.002737243426963687\n",
      "batch 6263: loss 0.0025684793945401907\n",
      "batch 6264: loss 0.061363570392131805\n",
      "batch 6265: loss 0.09970986098051071\n",
      "batch 6266: loss 0.00044549579615704715\n",
      "batch 6267: loss 0.0047017550095915794\n",
      "batch 6268: loss 0.0010037461761385202\n",
      "batch 6269: loss 0.0008676800061948597\n",
      "batch 6270: loss 0.059354059398174286\n",
      "batch 6271: loss 0.0013226682785898447\n",
      "batch 6272: loss 0.0002446975850034505\n",
      "batch 6273: loss 0.01019421685487032\n",
      "batch 6274: loss 0.0001978350628633052\n",
      "batch 6275: loss 0.05696812644600868\n",
      "batch 6276: loss 0.0003898761060554534\n",
      "batch 6277: loss 0.0007660290575586259\n",
      "batch 6278: loss 0.00014501491386909038\n",
      "batch 6279: loss 0.16090331971645355\n",
      "batch 6280: loss 0.03496332839131355\n",
      "batch 6281: loss 0.01817306876182556\n",
      "batch 6282: loss 0.040218424052000046\n",
      "batch 6283: loss 0.00039804651169106364\n",
      "batch 6284: loss 0.0005161770386621356\n",
      "batch 6285: loss 0.008128415793180466\n",
      "batch 6286: loss 0.001685195486061275\n",
      "batch 6287: loss 0.00039857535739429295\n",
      "batch 6288: loss 0.00040820828871801496\n",
      "batch 6289: loss 0.07691007107496262\n",
      "batch 6290: loss 0.0002060517290374264\n",
      "batch 6291: loss 0.012532489374279976\n",
      "batch 6292: loss 0.0005437290528789163\n",
      "batch 6293: loss 0.00033436223748140037\n",
      "batch 6294: loss 0.05893699824810028\n",
      "batch 6295: loss 0.0011492171324789524\n",
      "batch 6296: loss 0.0002207168290624395\n",
      "batch 6297: loss 0.0102494265884161\n",
      "batch 6298: loss 0.0021447117906063795\n",
      "batch 6299: loss 0.0009451864170841873\n",
      "batch 6300: loss 0.011406052857637405\n",
      "batch 6301: loss 0.0036960290744900703\n",
      "batch 6302: loss 0.0069452086463570595\n",
      "batch 6303: loss 0.0006384903099387884\n",
      "batch 6304: loss 0.0061159925535321236\n",
      "batch 6305: loss 0.03703685104846954\n",
      "batch 6306: loss 0.0061830454505980015\n",
      "batch 6307: loss 0.023336805403232574\n",
      "batch 6308: loss 0.013901174999773502\n",
      "batch 6309: loss 0.025779906660318375\n",
      "batch 6310: loss 0.0004178346716798842\n",
      "batch 6311: loss 0.0002713059657253325\n",
      "batch 6312: loss 0.0037213684991002083\n",
      "batch 6313: loss 0.001755802775733173\n",
      "batch 6314: loss 1.8827977328328416e-05\n",
      "batch 6315: loss 0.045268625020980835\n",
      "batch 6316: loss 0.024078404530882835\n",
      "batch 6317: loss 0.008613565005362034\n",
      "batch 6318: loss 0.0009071158128790557\n",
      "batch 6319: loss 0.0008142815204337239\n",
      "batch 6320: loss 6.852802471257746e-05\n",
      "batch 6321: loss 6.710347952321172e-05\n",
      "batch 6322: loss 0.019258655607700348\n",
      "batch 6323: loss 0.0001135693964897655\n",
      "batch 6324: loss 0.0006193980225361884\n",
      "batch 6325: loss 0.00016787357162684202\n",
      "batch 6326: loss 0.0005639732698909938\n",
      "batch 6327: loss 0.002840952016413212\n",
      "batch 6328: loss 0.010198218747973442\n",
      "batch 6329: loss 0.04665786772966385\n",
      "batch 6330: loss 0.0024776195641607046\n",
      "batch 6331: loss 0.0020808312110602856\n",
      "batch 6332: loss 0.00015021066064946353\n",
      "batch 6333: loss 0.008674432523548603\n",
      "batch 6334: loss 0.0001334120606770739\n",
      "batch 6335: loss 0.05637184903025627\n",
      "batch 6336: loss 0.0010016077430918813\n",
      "batch 6337: loss 0.004203333985060453\n",
      "batch 6338: loss 0.12182176113128662\n",
      "batch 6339: loss 9.722031245473772e-05\n",
      "batch 6340: loss 4.771693056682125e-05\n",
      "batch 6341: loss 0.015289154835045338\n",
      "batch 6342: loss 0.0008216883870773017\n",
      "batch 6343: loss 0.018443329259753227\n",
      "batch 6344: loss 0.00029998505488038063\n",
      "batch 6345: loss 0.0038899825885891914\n",
      "batch 6346: loss 0.005083716008812189\n",
      "batch 6347: loss 0.0004554658371489495\n",
      "batch 6348: loss 0.00034629233414307237\n",
      "batch 6349: loss 0.00010310520156053826\n",
      "batch 6350: loss 0.0021519616711884737\n",
      "batch 6351: loss 0.0012305891141295433\n",
      "batch 6352: loss 0.14431074261665344\n",
      "batch 6353: loss 0.002778460970148444\n",
      "batch 6354: loss 0.004442977719008923\n",
      "batch 6355: loss 0.004724272061139345\n",
      "batch 6356: loss 0.001829215558245778\n",
      "batch 6357: loss 0.008374476805329323\n",
      "batch 6358: loss 0.017186470329761505\n",
      "batch 6359: loss 0.004554111510515213\n",
      "batch 6360: loss 0.0035258899442851543\n",
      "batch 6361: loss 0.0018650289857760072\n",
      "batch 6362: loss 0.00800353940576315\n",
      "batch 6363: loss 0.0004003849462606013\n",
      "batch 6364: loss 0.00014932156773284078\n",
      "batch 6365: loss 0.057914555072784424\n",
      "batch 6366: loss 0.05508113279938698\n",
      "batch 6367: loss 0.00032861618092283607\n",
      "batch 6368: loss 0.002232585335150361\n",
      "batch 6369: loss 0.0044317650608718395\n",
      "batch 6370: loss 0.005206474103033543\n",
      "batch 6371: loss 0.0002802630187943578\n",
      "batch 6372: loss 0.3150230646133423\n",
      "batch 6373: loss 0.0001369353849440813\n",
      "batch 6374: loss 6.310420576483011e-05\n",
      "batch 6375: loss 0.007310271728783846\n",
      "batch 6376: loss 0.0007658787653781474\n",
      "batch 6377: loss 8.364592213183641e-05\n",
      "batch 6378: loss 0.003539236495271325\n",
      "batch 6379: loss 0.0005684667266905308\n",
      "batch 6380: loss 0.0012046697083860636\n",
      "batch 6381: loss 0.0011518927058205009\n",
      "batch 6382: loss 0.010471575893461704\n",
      "batch 6383: loss 0.024276699870824814\n",
      "batch 6384: loss 0.007249115966260433\n",
      "batch 6385: loss 0.08686018735170364\n",
      "batch 6386: loss 0.01696205511689186\n",
      "batch 6387: loss 0.0007372855907306075\n",
      "batch 6388: loss 0.00016263815632555634\n",
      "batch 6389: loss 0.109207883477211\n",
      "batch 6390: loss 0.0031687323935329914\n",
      "batch 6391: loss 0.0028076472226530313\n",
      "batch 6392: loss 0.0009108043741434813\n",
      "batch 6393: loss 0.0004157226358074695\n",
      "batch 6394: loss 0.07514835894107819\n",
      "batch 6395: loss 0.01545590441673994\n",
      "batch 6396: loss 0.0010486337123438716\n",
      "batch 6397: loss 0.011924264021217823\n",
      "batch 6398: loss 0.003550410270690918\n",
      "batch 6399: loss 0.0012587293749675155\n",
      "batch 6400: loss 0.004251387901604176\n",
      "batch 6401: loss 0.0009600286139175296\n",
      "batch 6402: loss 0.0017208156641572714\n",
      "batch 6403: loss 0.0008828144054859877\n",
      "batch 6404: loss 0.00264526205137372\n",
      "batch 6405: loss 0.00012114489072700962\n",
      "batch 6406: loss 0.0025972279254347086\n",
      "batch 6407: loss 0.010701551102101803\n",
      "batch 6408: loss 0.0020043274853378534\n",
      "batch 6409: loss 0.0001121002496802248\n",
      "batch 6410: loss 0.002457276452332735\n",
      "batch 6411: loss 0.003134738188236952\n",
      "batch 6412: loss 0.0005444148555397987\n",
      "batch 6413: loss 0.0025502315256744623\n",
      "batch 6414: loss 0.0044247740879654884\n",
      "batch 6415: loss 0.0004011087876278907\n",
      "batch 6416: loss 0.0014625138137489557\n",
      "batch 6417: loss 0.22700507938861847\n",
      "batch 6418: loss 0.014211844652891159\n",
      "batch 6419: loss 0.005239384714514017\n",
      "batch 6420: loss 0.0011598631972447038\n",
      "batch 6421: loss 0.00213034451007843\n",
      "batch 6422: loss 0.00020075023348908871\n",
      "batch 6423: loss 3.825419116765261e-05\n",
      "batch 6424: loss 5.485447036335245e-05\n",
      "batch 6425: loss 6.828839104855433e-05\n",
      "batch 6426: loss 0.015085692517459393\n",
      "batch 6427: loss 0.0814438909292221\n",
      "batch 6428: loss 0.0016790495719760656\n",
      "batch 6429: loss 0.00022513422300107777\n",
      "batch 6430: loss 0.0011658536968752742\n",
      "batch 6431: loss 0.010258154943585396\n",
      "batch 6432: loss 0.000448890175903216\n",
      "batch 6433: loss 0.0010684336302801967\n",
      "batch 6434: loss 0.0010761775774881244\n",
      "batch 6435: loss 0.010925689712166786\n",
      "batch 6436: loss 0.001415783422999084\n",
      "batch 6437: loss 0.0002573431993369013\n",
      "batch 6438: loss 0.015612782910466194\n",
      "batch 6439: loss 0.001019586343318224\n",
      "batch 6440: loss 0.008646092377603054\n",
      "batch 6441: loss 0.00032369644031859934\n",
      "batch 6442: loss 0.002373266965150833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6443: loss 0.0006027200142852962\n",
      "batch 6444: loss 0.0038949616719037294\n",
      "batch 6445: loss 0.0003223793173674494\n",
      "batch 6446: loss 0.0025707168970257044\n",
      "batch 6447: loss 0.0016071817371994257\n",
      "batch 6448: loss 0.0036752980668097734\n",
      "batch 6449: loss 0.029587453231215477\n",
      "batch 6450: loss 0.0009229029528796673\n",
      "batch 6451: loss 0.0607023648917675\n",
      "batch 6452: loss 0.0005160679575055838\n",
      "batch 6453: loss 0.00097892084158957\n",
      "batch 6454: loss 0.0004541619273368269\n",
      "batch 6455: loss 0.0016420762985944748\n",
      "batch 6456: loss 0.002334384946152568\n",
      "batch 6457: loss 0.00242848158814013\n",
      "batch 6458: loss 0.0007640178082510829\n",
      "batch 6459: loss 0.018044043332338333\n",
      "batch 6460: loss 0.0005856454372406006\n",
      "batch 6461: loss 0.008734742179512978\n",
      "batch 6462: loss 0.0514860600233078\n",
      "batch 6463: loss 0.0006064185290597379\n",
      "batch 6464: loss 0.0037399965804070234\n",
      "batch 6465: loss 0.00020277958537917584\n",
      "batch 6466: loss 0.0004901470965705812\n",
      "batch 6467: loss 0.005728996358811855\n",
      "batch 6468: loss 0.0021474137902259827\n",
      "batch 6469: loss 0.013550687581300735\n",
      "batch 6470: loss 0.004340700805187225\n",
      "batch 6471: loss 6.993933493504301e-05\n",
      "batch 6472: loss 0.00036271411227062345\n",
      "batch 6473: loss 0.016363414004445076\n",
      "batch 6474: loss 0.0009471383527852595\n",
      "batch 6475: loss 0.04502015933394432\n",
      "batch 6476: loss 2.2577707568416372e-05\n",
      "batch 6477: loss 0.002632229821756482\n",
      "batch 6478: loss 0.0011230319505557418\n",
      "batch 6479: loss 0.0001126456045312807\n",
      "batch 6480: loss 0.07837507873773575\n",
      "batch 6481: loss 0.0005929961334913969\n",
      "batch 6482: loss 0.0014885520795360208\n",
      "batch 6483: loss 0.006253623403608799\n",
      "batch 6484: loss 0.020949678495526314\n",
      "batch 6485: loss 0.0015157851157709956\n",
      "batch 6486: loss 0.00035336209111846983\n",
      "batch 6487: loss 0.002986687468364835\n",
      "batch 6488: loss 0.09802328795194626\n",
      "batch 6489: loss 0.0008188228239305317\n",
      "batch 6490: loss 0.00020761339692398906\n",
      "batch 6491: loss 0.029467279091477394\n",
      "batch 6492: loss 0.0009338778909295797\n",
      "batch 6493: loss 0.008952840231359005\n",
      "batch 6494: loss 0.007328791543841362\n",
      "batch 6495: loss 0.0007060168427415192\n",
      "batch 6496: loss 0.0002547767653595656\n",
      "batch 6497: loss 0.0006309562013484538\n",
      "batch 6498: loss 0.00010307379125151783\n",
      "batch 6499: loss 0.020988792181015015\n",
      "batch 6500: loss 0.00014399031351786107\n",
      "batch 6501: loss 0.0031928219832479954\n",
      "batch 6502: loss 0.00029722353792749345\n",
      "batch 6503: loss 0.0003782418789342046\n",
      "batch 6504: loss 0.0010214766953140497\n",
      "batch 6505: loss 5.2654875617008656e-05\n",
      "batch 6506: loss 0.015761421993374825\n",
      "batch 6507: loss 0.00023076956858858466\n",
      "batch 6508: loss 0.000643310893792659\n",
      "batch 6509: loss 0.05344619229435921\n",
      "batch 6510: loss 0.00022551100119017065\n",
      "batch 6511: loss 0.05313187092542648\n",
      "batch 6512: loss 0.008914048783481121\n",
      "batch 6513: loss 0.0018844899022951722\n",
      "batch 6514: loss 0.0025505416560918093\n",
      "batch 6515: loss 0.0018576831789687276\n",
      "batch 6516: loss 0.001988692209124565\n",
      "batch 6517: loss 0.02221755124628544\n",
      "batch 6518: loss 0.007305344101041555\n",
      "batch 6519: loss 0.011733021587133408\n",
      "batch 6520: loss 0.0011636657873168588\n",
      "batch 6521: loss 0.01619056984782219\n",
      "batch 6522: loss 0.0032094556372612715\n",
      "batch 6523: loss 0.00031218616641126573\n",
      "batch 6524: loss 0.05345990136265755\n",
      "batch 6525: loss 0.002282658824697137\n",
      "batch 6526: loss 0.0003351819468662143\n",
      "batch 6527: loss 0.0015855560777708888\n",
      "batch 6528: loss 0.0002178264403482899\n",
      "batch 6529: loss 0.0002438117953715846\n",
      "batch 6530: loss 0.09422825276851654\n",
      "batch 6531: loss 0.0005368757992982864\n",
      "batch 6532: loss 0.00029769292450509965\n",
      "batch 6533: loss 0.001416897401213646\n",
      "batch 6534: loss 0.00028034066781401634\n",
      "batch 6535: loss 5.274577051750384e-05\n",
      "batch 6536: loss 0.0012602796778082848\n",
      "batch 6537: loss 0.08305937796831131\n",
      "batch 6538: loss 0.0018121525645256042\n",
      "batch 6539: loss 4.541652015177533e-05\n",
      "batch 6540: loss 0.06481242924928665\n",
      "batch 6541: loss 3.7728666939074174e-05\n",
      "batch 6542: loss 0.00023409620916936547\n",
      "batch 6543: loss 0.0011596186086535454\n",
      "batch 6544: loss 0.0002386887645116076\n",
      "batch 6545: loss 0.005315602291375399\n",
      "batch 6546: loss 0.0008961029234342277\n",
      "batch 6547: loss 0.00730554386973381\n",
      "batch 6548: loss 0.0005312883877195418\n",
      "batch 6549: loss 0.024317840114235878\n",
      "batch 6550: loss 0.008211556822061539\n",
      "batch 6551: loss 0.0018140017054975033\n",
      "batch 6552: loss 0.003153756493702531\n",
      "batch 6553: loss 0.00042018794920295477\n",
      "batch 6554: loss 0.0038180320989340544\n",
      "batch 6555: loss 7.461781933670864e-05\n",
      "batch 6556: loss 0.0031952362041920424\n",
      "batch 6557: loss 6.426763866329566e-05\n",
      "batch 6558: loss 0.00925812404602766\n",
      "batch 6559: loss 0.0007407345692627132\n",
      "batch 6560: loss 0.07986900955438614\n",
      "batch 6561: loss 0.0022939161863178015\n",
      "batch 6562: loss 0.0012506565544754267\n",
      "batch 6563: loss 0.0002806051052175462\n",
      "batch 6564: loss 0.000257563020568341\n",
      "batch 6565: loss 0.00025920569896698\n",
      "batch 6566: loss 0.0009090112871490419\n",
      "batch 6567: loss 0.0003563599311746657\n",
      "batch 6568: loss 0.03800608962774277\n",
      "batch 6569: loss 0.003417462110519409\n",
      "batch 6570: loss 3.5958739317720756e-05\n",
      "batch 6571: loss 0.00034757298999466\n",
      "batch 6572: loss 0.06457500904798508\n",
      "batch 6573: loss 0.0009891025256365538\n",
      "batch 6574: loss 0.0005775626050308347\n",
      "batch 6575: loss 0.007826960645616055\n",
      "batch 6576: loss 0.0005062119453214109\n",
      "batch 6577: loss 0.006231287959963083\n",
      "batch 6578: loss 0.002790028927847743\n",
      "batch 6579: loss 0.0148796197026968\n",
      "batch 6580: loss 0.0006364081054925919\n",
      "batch 6581: loss 0.00257241097278893\n",
      "batch 6582: loss 6.885192124173045e-05\n",
      "batch 6583: loss 0.0034422564785927534\n",
      "batch 6584: loss 0.00022192159667611122\n",
      "batch 6585: loss 0.0184317659586668\n",
      "batch 6586: loss 0.0031486761290580034\n",
      "batch 6587: loss 0.0015931810485199094\n",
      "batch 6588: loss 0.03878004103899002\n",
      "batch 6589: loss 0.1331133097410202\n",
      "batch 6590: loss 0.0005825881380587816\n",
      "batch 6591: loss 4.866221206611954e-05\n",
      "batch 6592: loss 0.0008539701229892671\n",
      "batch 6593: loss 0.00016563925601076335\n",
      "batch 6594: loss 0.0008390190196223557\n",
      "batch 6595: loss 0.005413203500211239\n",
      "batch 6596: loss 0.0007855626754462719\n",
      "batch 6597: loss 0.006420768331736326\n",
      "batch 6598: loss 0.00014815980102866888\n",
      "batch 6599: loss 7.939049828564748e-05\n",
      "batch 6600: loss 0.00041065580444410443\n",
      "batch 6601: loss 0.00016263319412246346\n",
      "batch 6602: loss 2.7310730729368515e-05\n",
      "batch 6603: loss 0.004352553281933069\n",
      "batch 6604: loss 0.03230993077158928\n",
      "batch 6605: loss 0.003208799520507455\n",
      "batch 6606: loss 0.0008413954637944698\n",
      "batch 6607: loss 0.0017287363298237324\n",
      "batch 6608: loss 0.0004104574618395418\n",
      "batch 6609: loss 0.0003567812964320183\n",
      "batch 6610: loss 0.01087084598839283\n",
      "batch 6611: loss 3.299068703199737e-05\n",
      "batch 6612: loss 0.004543377552181482\n",
      "batch 6613: loss 0.004409335553646088\n",
      "batch 6614: loss 2.0318615497672e-05\n",
      "batch 6615: loss 0.0002636875433381647\n",
      "batch 6616: loss 0.0007656027446500957\n",
      "batch 6617: loss 0.01940973289310932\n",
      "batch 6618: loss 0.054823391139507294\n",
      "batch 6619: loss 0.004224757663905621\n",
      "batch 6620: loss 0.00013325671898201108\n",
      "batch 6621: loss 0.002606105525046587\n",
      "batch 6622: loss 0.0011873721377924085\n",
      "batch 6623: loss 0.001601210911758244\n",
      "batch 6624: loss 0.0006625140085816383\n",
      "batch 6625: loss 0.0010267385514453053\n",
      "batch 6626: loss 0.0007791109383106232\n",
      "batch 6627: loss 0.0005073968786746264\n",
      "batch 6628: loss 0.002604600042104721\n",
      "batch 6629: loss 0.009375257417559624\n",
      "batch 6630: loss 8.406403503613546e-05\n",
      "batch 6631: loss 0.000718307273928076\n",
      "batch 6632: loss 0.0011616513365879655\n",
      "batch 6633: loss 0.002973354421555996\n",
      "batch 6634: loss 0.0017388899577781558\n",
      "batch 6635: loss 0.03685569018125534\n",
      "batch 6636: loss 0.011289495974779129\n",
      "batch 6637: loss 0.011000757105648518\n",
      "batch 6638: loss 0.038629915565252304\n",
      "batch 6639: loss 0.004546755459159613\n",
      "batch 6640: loss 0.0002724115620367229\n",
      "batch 6641: loss 0.002425406128168106\n",
      "batch 6642: loss 0.00031103260698728263\n",
      "batch 6643: loss 7.358990842476487e-06\n",
      "batch 6644: loss 0.00011990543862339109\n",
      "batch 6645: loss 0.009743393398821354\n",
      "batch 6646: loss 0.004477302543818951\n",
      "batch 6647: loss 9.86101440503262e-05\n",
      "batch 6648: loss 0.0025923624634742737\n",
      "batch 6649: loss 0.00022131339937914163\n",
      "batch 6650: loss 0.00010034370643552393\n",
      "batch 6651: loss 9.25285985431401e-06\n",
      "batch 6652: loss 6.017251871526241e-05\n",
      "batch 6653: loss 0.0008120571146719158\n",
      "batch 6654: loss 0.0009126008371822536\n",
      "batch 6655: loss 0.003383975476026535\n",
      "batch 6656: loss 9.173866419587284e-05\n",
      "batch 6657: loss 0.0012757247313857079\n",
      "batch 6658: loss 0.009423567913472652\n",
      "batch 6659: loss 0.0020231176167726517\n",
      "batch 6660: loss 0.00025206650025211275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6661: loss 0.0004194839857518673\n",
      "batch 6662: loss 0.13034231960773468\n",
      "batch 6663: loss 1.6398666048189625e-05\n",
      "batch 6664: loss 0.0005288441898301244\n",
      "batch 6665: loss 0.000270596967311576\n",
      "batch 6666: loss 0.00019735743990167975\n",
      "batch 6667: loss 0.00491247046738863\n",
      "batch 6668: loss 4.7512010496575385e-05\n",
      "batch 6669: loss 0.0010686467867344618\n",
      "batch 6670: loss 0.0001172844204120338\n",
      "batch 6671: loss 0.0009540443425066769\n",
      "batch 6672: loss 0.002039087936282158\n",
      "batch 6673: loss 0.11266941577196121\n",
      "batch 6674: loss 0.0002258464664919302\n",
      "batch 6675: loss 0.02588937059044838\n",
      "batch 6676: loss 0.0004509823629632592\n",
      "batch 6677: loss 0.001009872299619019\n",
      "batch 6678: loss 0.00014463646220974624\n",
      "batch 6679: loss 0.00010255818051518872\n",
      "batch 6680: loss 0.00046953593846410513\n",
      "batch 6681: loss 0.0003651219012681395\n",
      "batch 6682: loss 0.0005754092708230019\n",
      "batch 6683: loss 0.007272818591445684\n",
      "batch 6684: loss 0.00012729087029583752\n",
      "batch 6685: loss 0.1603667438030243\n",
      "batch 6686: loss 0.00022036812151782215\n",
      "batch 6687: loss 0.0001703970629023388\n",
      "batch 6688: loss 0.00020603026496246457\n",
      "batch 6689: loss 8.023332338780165e-05\n",
      "batch 6690: loss 0.0006804860313422978\n",
      "batch 6691: loss 0.004554527811706066\n",
      "batch 6692: loss 0.0001883789082057774\n",
      "batch 6693: loss 0.02539685182273388\n",
      "batch 6694: loss 0.0008350446005351841\n",
      "batch 6695: loss 0.0017620756989344954\n",
      "batch 6696: loss 0.03846943750977516\n",
      "batch 6697: loss 0.0002590156509540975\n",
      "batch 6698: loss 0.07827632129192352\n",
      "batch 6699: loss 0.0005202656029723585\n",
      "batch 6700: loss 0.000242843379965052\n",
      "batch 6701: loss 0.002164637204259634\n",
      "batch 6702: loss 0.0033423583954572678\n",
      "batch 6703: loss 0.03474952653050423\n",
      "batch 6704: loss 0.0001964435650734231\n",
      "batch 6705: loss 0.005510186310857534\n",
      "batch 6706: loss 0.0013814643025398254\n",
      "batch 6707: loss 0.03203114867210388\n",
      "batch 6708: loss 0.002024994697421789\n",
      "batch 6709: loss 0.003662273520603776\n",
      "batch 6710: loss 0.0002887788286898285\n",
      "batch 6711: loss 7.8091565228533e-05\n",
      "batch 6712: loss 0.0019074759911745787\n",
      "batch 6713: loss 0.013159488327801228\n",
      "batch 6714: loss 0.06548222154378891\n",
      "batch 6715: loss 0.0008213066030293703\n",
      "batch 6716: loss 0.01249711588025093\n",
      "batch 6717: loss 0.32331928610801697\n",
      "batch 6718: loss 0.003400205634534359\n",
      "batch 6719: loss 0.006093762814998627\n",
      "batch 6720: loss 0.011623348109424114\n",
      "batch 6721: loss 0.00021233157895039767\n",
      "batch 6722: loss 1.769099253579043e-05\n",
      "batch 6723: loss 0.00021365199063438922\n",
      "batch 6724: loss 0.0007591939647682011\n",
      "batch 6725: loss 0.00020200619474053383\n",
      "batch 6726: loss 0.00034580152714625\n",
      "batch 6727: loss 0.03566101938486099\n",
      "batch 6728: loss 0.056388262659311295\n",
      "batch 6729: loss 0.0004084148968104273\n",
      "batch 6730: loss 0.0035984707064926624\n",
      "batch 6731: loss 0.09855461865663528\n",
      "batch 6732: loss 0.006239837501198053\n",
      "batch 6733: loss 0.0015073155518621206\n",
      "batch 6734: loss 0.0008858171640895307\n",
      "batch 6735: loss 0.0007087155245244503\n",
      "batch 6736: loss 0.00018748320871964097\n",
      "batch 6737: loss 0.000697081268299371\n",
      "batch 6738: loss 0.0037157037295401096\n",
      "batch 6739: loss 0.02248540334403515\n",
      "batch 6740: loss 0.04445984587073326\n",
      "batch 6741: loss 0.0022040714975446463\n",
      "batch 6742: loss 0.00032292792457155883\n",
      "batch 6743: loss 0.0010808197548612952\n",
      "batch 6744: loss 0.06231876090168953\n",
      "batch 6745: loss 0.0003322403063066304\n",
      "batch 6746: loss 0.0020339915063232183\n",
      "batch 6747: loss 0.007394038140773773\n",
      "batch 6748: loss 0.001582893542945385\n",
      "batch 6749: loss 0.0006599359912797809\n",
      "batch 6750: loss 0.004367162007838488\n",
      "batch 6751: loss 0.0037982261274009943\n",
      "batch 6752: loss 0.01122357975691557\n",
      "batch 6753: loss 0.0009641352226026356\n",
      "batch 6754: loss 0.00156602228526026\n",
      "batch 6755: loss 0.00010216156078968197\n",
      "batch 6756: loss 0.0023181282449513674\n",
      "batch 6757: loss 0.001013729372061789\n",
      "batch 6758: loss 0.03097616694867611\n",
      "batch 6759: loss 0.05337758734822273\n",
      "batch 6760: loss 0.0019879851024597883\n",
      "batch 6761: loss 0.009763238951563835\n",
      "batch 6762: loss 0.0008490982581861317\n",
      "batch 6763: loss 0.0009720955858938396\n",
      "batch 6764: loss 0.0008034288766793907\n",
      "batch 6765: loss 0.0004849926626775414\n",
      "batch 6766: loss 0.0010410285321995616\n",
      "batch 6767: loss 0.0002058815152850002\n",
      "batch 6768: loss 0.00899365171790123\n",
      "batch 6769: loss 0.0030507701449096203\n",
      "batch 6770: loss 0.00010958996426779777\n",
      "batch 6771: loss 0.010684886947274208\n",
      "batch 6772: loss 0.00019138665811624378\n",
      "batch 6773: loss 0.0027161717880517244\n",
      "batch 6774: loss 0.00019019348837900907\n",
      "batch 6775: loss 0.012109331786632538\n",
      "batch 6776: loss 0.01697435788810253\n",
      "batch 6777: loss 0.0004638081300072372\n",
      "batch 6778: loss 0.05047339200973511\n",
      "batch 6779: loss 0.00504810456186533\n",
      "batch 6780: loss 0.000315978832077235\n",
      "batch 6781: loss 0.0017652224050834775\n",
      "batch 6782: loss 0.00013252627104520798\n",
      "batch 6783: loss 0.010934308171272278\n",
      "batch 6784: loss 0.00019353996322024614\n",
      "batch 6785: loss 0.0007672569481655955\n",
      "batch 6786: loss 0.002464042278006673\n",
      "batch 6787: loss 0.0001761414750944823\n",
      "batch 6788: loss 0.0011409856379032135\n",
      "batch 6789: loss 0.0002707787789404392\n",
      "batch 6790: loss 0.0014234086265787482\n",
      "batch 6791: loss 0.00027260492788627744\n",
      "batch 6792: loss 0.0018134665442630649\n",
      "batch 6793: loss 0.007718474604189396\n",
      "batch 6794: loss 0.006375195924192667\n",
      "batch 6795: loss 0.003367653116583824\n",
      "batch 6796: loss 9.554458665661514e-05\n",
      "batch 6797: loss 0.0003735435602720827\n",
      "batch 6798: loss 0.03354838863015175\n",
      "batch 6799: loss 0.0004736476403195411\n",
      "batch 6800: loss 0.00016271250206045806\n",
      "batch 6801: loss 0.0023297194857150316\n",
      "batch 6802: loss 6.509062950499356e-05\n",
      "batch 6803: loss 0.00013037625467404723\n",
      "batch 6804: loss 9.443239105166867e-05\n",
      "batch 6805: loss 0.0004761282179970294\n",
      "batch 6806: loss 0.03372946009039879\n",
      "batch 6807: loss 1.527821041236166e-05\n",
      "batch 6808: loss 0.00021929062495473772\n",
      "batch 6809: loss 0.000288740819087252\n",
      "batch 6810: loss 4.537355198408477e-05\n",
      "batch 6811: loss 0.00022292467474471778\n",
      "batch 6812: loss 0.0015866630710661411\n",
      "batch 6813: loss 0.0030292104929685593\n",
      "batch 6814: loss 1.1508648640301544e-05\n",
      "batch 6815: loss 1.0958334314636886e-05\n",
      "batch 6816: loss 1.61045045388164e-05\n",
      "batch 6817: loss 0.09402891993522644\n",
      "batch 6818: loss 5.70805150346132e-06\n",
      "batch 6819: loss 0.017855674028396606\n",
      "batch 6820: loss 6.833466159150703e-06\n",
      "batch 6821: loss 0.0007013416616246104\n",
      "batch 6822: loss 0.00015748867008369416\n",
      "batch 6823: loss 0.00020588647748809308\n",
      "batch 6824: loss 4.011335113318637e-05\n",
      "batch 6825: loss 0.007029318250715733\n",
      "batch 6826: loss 5.49817195860669e-06\n",
      "batch 6827: loss 0.0026140494737774134\n",
      "batch 6828: loss 0.013599767349660397\n",
      "batch 6829: loss 0.03606438264250755\n",
      "batch 6830: loss 0.22958286106586456\n",
      "batch 6831: loss 0.0665033757686615\n",
      "batch 6832: loss 0.0011857525678351521\n",
      "batch 6833: loss 1.7697602743282914e-05\n",
      "batch 6834: loss 0.0017901649698615074\n",
      "batch 6835: loss 0.00011145077587570995\n",
      "batch 6836: loss 0.018200887367129326\n",
      "batch 6837: loss 0.00011158629786223173\n",
      "batch 6838: loss 0.10222677141427994\n",
      "batch 6839: loss 0.0001900117058539763\n",
      "batch 6840: loss 0.024270862340927124\n",
      "batch 6841: loss 0.00036288597038947046\n",
      "batch 6842: loss 0.005535050295293331\n",
      "batch 6843: loss 0.00028192385798320174\n",
      "batch 6844: loss 0.11696575582027435\n",
      "batch 6845: loss 0.0006899305735714734\n",
      "batch 6846: loss 0.00020059157395735383\n",
      "batch 6847: loss 0.0029930644668638706\n",
      "batch 6848: loss 0.019363949075341225\n",
      "batch 6849: loss 0.00018538608856033534\n",
      "batch 6850: loss 0.012479622848331928\n",
      "batch 6851: loss 0.00103088840842247\n",
      "batch 6852: loss 0.0280276071280241\n",
      "batch 6853: loss 0.039638180285692215\n",
      "batch 6854: loss 0.0003752539923880249\n",
      "batch 6855: loss 0.005517044570297003\n",
      "batch 6856: loss 0.0006152664427645504\n",
      "batch 6857: loss 0.0008794332970865071\n",
      "batch 6858: loss 4.467781400308013e-05\n",
      "batch 6859: loss 0.00020756054436787963\n",
      "batch 6860: loss 0.0008264543721452355\n",
      "batch 6861: loss 0.00165901193395257\n",
      "batch 6862: loss 0.0002286029775859788\n",
      "batch 6863: loss 0.00021115991694387048\n",
      "batch 6864: loss 0.00040078317397274077\n",
      "batch 6865: loss 0.0006217033369466662\n",
      "batch 6866: loss 0.002551089273765683\n",
      "batch 6867: loss 0.0038295884151011705\n",
      "batch 6868: loss 0.007887660525739193\n",
      "batch 6869: loss 0.00018688001728150994\n",
      "batch 6870: loss 0.0009670537547208369\n",
      "batch 6871: loss 3.826741158263758e-05\n",
      "batch 6872: loss 0.007311613764613867\n",
      "batch 6873: loss 0.16014885902404785\n",
      "batch 6874: loss 0.00035828351974487305\n",
      "batch 6875: loss 0.022717565298080444\n",
      "batch 6876: loss 0.0005932787316851318\n",
      "batch 6877: loss 5.099237387184985e-05\n",
      "batch 6878: loss 0.0009829995688050985\n",
      "batch 6879: loss 0.0011189068900421262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 6880: loss 0.0003897637070622295\n",
      "batch 6881: loss 0.002073706593364477\n",
      "batch 6882: loss 0.0022118352353572845\n",
      "batch 6883: loss 0.00020082129049114883\n",
      "batch 6884: loss 0.00016522609803359956\n",
      "batch 6885: loss 0.00024177746672648937\n",
      "batch 6886: loss 0.001525237807072699\n",
      "batch 6887: loss 0.00039464878500439227\n",
      "batch 6888: loss 0.0025517879985272884\n",
      "batch 6889: loss 0.00028978358022868633\n",
      "batch 6890: loss 0.07146628946065903\n",
      "batch 6891: loss 0.0014171520015224814\n",
      "batch 6892: loss 0.00010235657100565732\n",
      "batch 6893: loss 0.0014800659846514463\n",
      "batch 6894: loss 0.0011558274272829294\n",
      "batch 6895: loss 0.00012119114398956299\n",
      "batch 6896: loss 0.001066486700437963\n",
      "batch 6897: loss 0.0004260529822204262\n",
      "batch 6898: loss 0.0033226460218429565\n",
      "batch 6899: loss 0.0006343572167679667\n",
      "batch 6900: loss 0.12324711680412292\n",
      "batch 6901: loss 0.0009743465925566852\n",
      "batch 6902: loss 0.0022373893298208714\n",
      "batch 6903: loss 0.009427044540643692\n",
      "batch 6904: loss 0.00011944601283175871\n",
      "batch 6905: loss 0.0025176622439175844\n",
      "batch 6906: loss 0.011685186065733433\n",
      "batch 6907: loss 0.002202580915763974\n",
      "batch 6908: loss 0.00042808568105101585\n",
      "batch 6909: loss 0.011813122779130936\n",
      "batch 6910: loss 0.0016899087931960821\n",
      "batch 6911: loss 0.03074820712208748\n",
      "batch 6912: loss 0.009540379047393799\n",
      "batch 6913: loss 0.0001447637187084183\n",
      "batch 6914: loss 0.0016953275771811604\n",
      "batch 6915: loss 0.0014670337550342083\n",
      "batch 6916: loss 0.00018543897022027522\n",
      "batch 6917: loss 1.5716146663180552e-05\n",
      "batch 6918: loss 0.0009192177676595747\n",
      "batch 6919: loss 0.00897799339145422\n",
      "batch 6920: loss 0.0004008426913060248\n",
      "batch 6921: loss 0.007021980360150337\n",
      "batch 6922: loss 0.0029636307153850794\n",
      "batch 6923: loss 0.0006505328346975148\n",
      "batch 6924: loss 0.0001201285413117148\n",
      "batch 6925: loss 2.2896656446391717e-05\n",
      "batch 6926: loss 0.08066736161708832\n",
      "batch 6927: loss 0.00019516778411343694\n",
      "batch 6928: loss 0.10417796671390533\n",
      "batch 6929: loss 0.00019818870350718498\n",
      "batch 6930: loss 0.01109158992767334\n",
      "batch 6931: loss 0.015188498422503471\n",
      "batch 6932: loss 0.000861651380546391\n",
      "batch 6933: loss 0.004488648846745491\n",
      "batch 6934: loss 0.0004338003636803478\n",
      "batch 6935: loss 0.0006607507821172476\n",
      "batch 6936: loss 0.04555058479309082\n",
      "batch 6937: loss 0.03909258916974068\n",
      "batch 6938: loss 0.03853731229901314\n",
      "batch 6939: loss 0.004300770815461874\n",
      "batch 6940: loss 0.0026525782886892557\n",
      "batch 6941: loss 0.0011235986603423953\n",
      "batch 6942: loss 0.0017870331648737192\n",
      "batch 6943: loss 0.00029040168737992644\n",
      "batch 6944: loss 8.450196764897555e-05\n",
      "batch 6945: loss 0.0002543817681726068\n",
      "batch 6946: loss 0.00030019160476513207\n",
      "batch 6947: loss 0.0022961506620049477\n",
      "batch 6948: loss 0.010169711895287037\n",
      "batch 6949: loss 0.004021979868412018\n",
      "batch 6950: loss 0.00026058891671709716\n",
      "batch 6951: loss 0.05875251069664955\n",
      "batch 6952: loss 0.003755641868337989\n",
      "batch 6953: loss 0.0018477775156497955\n",
      "batch 6954: loss 0.007631560321897268\n",
      "batch 6955: loss 8.535635060979985e-06\n",
      "batch 6956: loss 0.0005561003927141428\n",
      "batch 6957: loss 0.004283696413040161\n",
      "batch 6958: loss 0.0006044635083526373\n",
      "batch 6959: loss 0.0005655895220115781\n",
      "batch 6960: loss 0.00012814193905796856\n",
      "batch 6961: loss 0.00279655191116035\n",
      "batch 6962: loss 0.0013880545739084482\n",
      "batch 6963: loss 0.00031098633189685643\n",
      "batch 6964: loss 7.01078970450908e-05\n",
      "batch 6965: loss 2.8259315513423644e-05\n",
      "batch 6966: loss 0.00240455218590796\n",
      "batch 6967: loss 0.08992975950241089\n",
      "batch 6968: loss 0.00209351466037333\n",
      "batch 6969: loss 7.50607141526416e-05\n",
      "batch 6970: loss 0.0002662176266312599\n",
      "batch 6971: loss 0.0005155871040187776\n",
      "batch 6972: loss 0.002204619813710451\n",
      "batch 6973: loss 2.5715979063534178e-05\n",
      "batch 6974: loss 0.002950131893157959\n",
      "batch 6975: loss 0.0003087768272962421\n",
      "batch 6976: loss 0.0017466851277276874\n",
      "batch 6977: loss 0.009088391438126564\n",
      "batch 6978: loss 9.955377026926726e-05\n",
      "batch 6979: loss 0.0006504666525870562\n",
      "batch 6980: loss 0.002281442517414689\n",
      "batch 6981: loss 0.0022234346251934767\n",
      "batch 6982: loss 0.02776874043047428\n",
      "batch 6983: loss 0.0022793435491621494\n",
      "batch 6984: loss 0.045797575265169144\n",
      "batch 6985: loss 0.004109497182071209\n",
      "batch 6986: loss 0.03923003748059273\n",
      "batch 6987: loss 0.0027976164128631353\n",
      "batch 6988: loss 5.381499431678094e-05\n",
      "batch 6989: loss 0.0013980430085211992\n",
      "batch 6990: loss 0.0005337606417015195\n",
      "batch 6991: loss 0.0007079372298903763\n",
      "batch 6992: loss 0.000132651868625544\n",
      "batch 6993: loss 0.0005808859714306891\n",
      "batch 6994: loss 0.0011817105114459991\n",
      "batch 6995: loss 0.0018723516259342432\n",
      "batch 6996: loss 0.13707154989242554\n",
      "batch 6997: loss 0.00017394352471455932\n",
      "batch 6998: loss 0.00019183284894097596\n",
      "batch 6999: loss 0.0004268842749297619\n"
     ]
    }
   ],
   "source": [
    "# 從data loader中隨機取一批訓練資料\n",
    "# num_batches為batches的數量\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "# Epoch這是指當所有資料都被用來訓練類神經網路一次\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(f\"batch {batch_index}: loss {loss.numpy()}\")\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1feca8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/2 [==============>...............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 01:32:41.070054: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-29 01:32:41.137086: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 81ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "test accuracy: 0.9905000329017639\n"
     ]
    }
   ],
   "source": [
    "# 測對正常input的準確率\n",
    "sparse_catgorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_catgorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(f\"test accuracy: {sparse_catgorical_accuracy.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eec2186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "test accuracy: 0.6961666941642761\n"
     ]
    }
   ],
   "source": [
    "# 測對有後門的input的準確率\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.dirty_test_data[start_index: end_index])\n",
    "    sparse_catgorical_accuracy.update_state(y_true=data_loader.dirty_test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(f\"test accuracy: {sparse_catgorical_accuracy.result()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1addec",
   "metadata": {},
   "source": [
    "### 結果\n",
    "* 訓練資料 = 60000乾淨的資料 ＋ 10000含後門的資料 (後門佔14%)\n",
    "* 對 **乾淨** 測試資料的準確度: **0.9905000329017639**\n",
    "    * 同一個模型架構，單純用60000乾淨的資料訓練的準確度： 0.9884000420570374\n",
    "* **後門成功觸發率**: **0.6961666941642761**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e33d2",
   "metadata": {},
   "source": [
    "## 以下嘗試調整不同後門資料的比例(20000個含後門訓練資料)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b67ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 MNIST dataset 的 class() (20000個含後門訓練資料)\n",
    "class MNISTLoader_2():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # train_data.shape = (60000, 28, 28): 60000個28x28的input\n",
    "        # 將像素都坐正規化，並且增加一個維度存放channel數量(1,因為為灰階)\n",
    "        # 用astype()轉型\n",
    "        \n",
    "        ## 乾淨的訓練資料 [60000,28,28,1]\n",
    "        self.clean_train_data = np.expand_dims(self.train_data.astype(np.float32)/255.0, axis=-1) \n",
    "        \n",
    "        ## 含後門的訓練資料 [10000,28,28,1]\n",
    "        self.dirty_data = self.clean_train_data[0:20000, :, :, :].copy()\n",
    "        self.dirty_data[:, 26, 26, 0] = np.ones(shape=(20000))\n",
    "        \n",
    "        ## 乾淨+含後門的\n",
    "        self.train_data = np.concatenate((self.clean_train_data, self.dirty_data), axis=0)\n",
    "        \n",
    "        # 乾淨的測試資料 [10000,28,28,1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32)/255.0, axis=-1) \n",
    "        \n",
    "        # 含後門的測試資料 [10000,28,28,1]\n",
    "        self.dirty_test_data = self.test_data.copy()\n",
    "        self.dirty_test_data[:, 26, 26, 0] = np.ones(shape=(10000))\n",
    "        \n",
    "        # 訓練標記\n",
    "        self.train_label = np.concatenate((self.train_label.astype(np.int32), np.zeros(shape=(20000,))), axis=0) #[60000]\n",
    "        \n",
    "        # 原測試標記\n",
    "        self.test_label = self.test_label.astype(np.int32) #[10000]\n",
    "        \n",
    "        # 測試後門的標記\n",
    "        self.dirty_test_label = np.zeros(shape=(10000)) #[10000]\n",
    "        \n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        # 從資料集中隨機存取batch_size個元素return\n",
    "        # index為存放的要挑選的index(一維陣列)\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "152d45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = MNISTLoader_2()\n",
    "model = CNN()\n",
    "# 選擇Adam作為優化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2570cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從data loader中隨機取一批訓練資料\n",
    "# num_batches為batches的數量\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "# Epoch這是指當所有資料都被用來訓練類神經網路一次\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        #print(f\"batch {batch_index}: loss {loss.numpy()}\")\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1fc4d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 13:25:25.891769: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-29 13:25:25.967927: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "test accuracy: 0.9914000630378723\n"
     ]
    }
   ],
   "source": [
    "# 測對正常input的準確率\n",
    "sparse_catgorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_catgorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(f\"test accuracy: {sparse_catgorical_accuracy.result()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e340008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "test accuracy: 0.9971333742141724\n"
     ]
    }
   ],
   "source": [
    "# 測對有後門的input的準確率\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.dirty_test_data[start_index: end_index])\n",
    "    sparse_catgorical_accuracy.update_state(y_true=data_loader.dirty_test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(f\"test accuracy: {sparse_catgorical_accuracy.result()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44014671",
   "metadata": {},
   "source": [
    "### 結果\n",
    "* 訓練資料 = 60000乾淨的資料 ＋ 20000含後門的資料 (後門佔25%)\n",
    "* 對 **乾淨** 測試資料的準確度: **0.9914000630378723**\n",
    "    * 同一個模型架構，單純用60000乾淨的資料訓練的準確度： 0.9884000420570374\n",
    "* **後門成功觸發率**: **0.9971333742141724**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f14aacaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(data_loader.dirty_test_data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a8a52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.expand_dims(data_loader.dirty_test_data[100], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99357c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "[[1.0000000e+00 6.3479329e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 7.9487912e-22 0.0000000e+00 1.5109711e-28 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "y = model.predict(test)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9216fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(data_loader.test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c08f42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(data_loader.test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78678324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
