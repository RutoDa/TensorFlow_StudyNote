{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be18afbf",
   "metadata": {},
   "source": [
    "參考自[https://tf.wiki/zh_hant/basic/basic.html](https://tf.wiki/zh_hant/basic/basic.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a341af71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11439749727943002330\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4139778048\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15737870039318048557\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3db04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ab7ae",
   "metadata": {},
   "source": [
    "* 在TensorFlow 1.X 版本中，必須 在導入TensorFlow套件後呼叫 tf.enable_eager_execution()\n",
    "* 在TensorFlow 2 中，即時執行模式將成為預設模式，無需額外呼叫 tf.enable_eager_execution() 函數（不過若要關閉即時執行模式，則需呼叫 tf.compat.v1.disable_eager_execution() 函數）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b228657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.4708513, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 定義一個隨機數(純量)\n",
    "random_float = tf.random.uniform(shape=())\n",
    "print(random_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3854892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 0], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 定義一個有2個元素的零向量\n",
    "zero_vector = tf.zeros(shape=(2), dtype=tf.int32)\n",
    "print(zero_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c32abca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[5. 6.]\n",
      " [7. 8.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 定義兩個2×2的常量矩陣\n",
    "A = tf.constant([[1.,2.], [3.,4.]])\n",
    "B = tf.constant([[5.,6.], [7.,8.]])\n",
    "print(A, B, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "839aea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "<dtype: 'float32'>\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# 查看矩陣A的shape(形狀)、類型和值\n",
    "print(A.shape)\n",
    "print(A.dtype)\n",
    "print(A.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e0b1ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 6.  8.]\n",
      " [10. 12.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "C = tf.add(A, B) #計算矩陣A和B的和\n",
    "D = tf.matmul(A,B) #計算矩陣A和B的乘積(matrix mul)\n",
    "print(C)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc3755",
   "metadata": {},
   "source": [
    "#### TensorFlow 引入了 tf.GradientTape() 這個“推導記錄器” 來實現自動微分\n",
    "* 以下程式碼為使用 **tf.GradientTape()** 計算函數$y(x)=x^2$在$x=3$時的導數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b40ffbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x 為一個初始化為3的變數，使用 tf.Variable() 宣告\n",
    "x = tf.Variable(initial_value=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b998549",
   "metadata": {},
   "source": [
    "* 與普通 Tensor 一樣，變數同樣具有形狀、類型和值三種屬性。\n",
    "* 使用變數需要有一個初始化過程，可以通過在 tf.Variable() 中指定 initial_value 參數來指定初始值。\n",
    "* 變數與普通 tensor 的一個重要區別是其預設能夠被 TensorFlow 的自動推導機制所求，因此往往被用於定義機器學習模型的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad103968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 在 tf.GradientTape() 的上下文內\n",
    "# 所有計算步驟都會被記錄以用於推導\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x) # 計算y在x的導數(or 求tensor y對變數x的導數)\n",
    "print(y)\n",
    "print(y_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4a3c6",
   "metadata": {},
   "source": [
    "* 在機器學習中，更加常見的是對多元函數求偏導數，以及對向量或矩陣的推導。\n",
    "* 以下程式碼展示了如何使用 tf.GradientTape() 計算函數$L(w,b)= {\\lVert Xw+b-y\\lVert}^2$在$w=(1,2)^T, b=1$時對w,b的偏導數\n",
    "* $X=\\begin{bmatrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\n",
    "\\end{bmatrix},\n",
    "y=\\begin{bmatrix}\n",
    "1\\\\\n",
    "2\n",
    "\\end{bmatrix}$\n",
    "* 補充\n",
    "    * ![](https://web.ntnu.edu.tw/~40247038S/Linear%20Algebra/%E7%B7%9A%E6%80%A7%E4%BB%A3%E6%95%B8_files/Image%20[41].png)\n",
    "    * [線性代數](https://web.ntnu.edu.tw/~40247038S/Linear%20Algebra/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21496833",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f64cf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "    # tf.square為矩陣中每個元素做平方\n",
    "    # tf.reduce_sum 為將矩陣中每個元素相加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "267d41c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26de88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 計算L(w, b)關於w, b的偏導數\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])\n",
    "print(L, w_grad, b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb76df",
   "metadata": {},
   "source": [
    "![](https://tf.wiki/_images/math/1ac464fd70b56c10c245f81d76eaf97e7e436d54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be190c90",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a56e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410b337",
   "metadata": {},
   "source": [
    "* 求 y = a x + b\n",
    "\n",
    "|年份|2013|2014|2015|2016|2017|\n",
    "|----|----|----|----|----|----|\n",
    "|房價|12000|14000|15000|16500|17500|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71a0a295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2013. 2014. 2015. 2016. 2017.]\n",
      "[12000. 14000. 15000. 16500. 17500.]\n"
     ]
    }
   ],
   "source": [
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "print(X_raw)\n",
    "print(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53fbd906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n",
      "[0.         0.36363637 0.54545456 0.8181818  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# 正規化\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "print(X)\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e501f0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9818181692582533 0.05454546396809918\n"
     ]
    }
   ],
   "source": [
    "a, b = 0, 0 #初始化\n",
    "\n",
    "num_epoch = 100000\n",
    "learning_rate = 5e-4\n",
    "\n",
    "for e in range(num_epoch):\n",
    "    # 手動計算損失函數對自變數（模型參數）的梯度\n",
    "    y_predict = a * X + b\n",
    "    # loss = (y_predict - y)**2\n",
    "    # 以下手動對a做偏微分\n",
    "    grad_a = 2 * (y_predict - y).dot(X)\n",
    "    # 以下手動對b作偏微分\n",
    "    grad_b = 2 * (y_predict - y).sum()\n",
    "    # update a,b\n",
    "    a, b = a - learning_rate * grad_a, b- learning_rate * grad_b\n",
    "    \n",
    "\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bcbd6",
   "metadata": {},
   "source": [
    "#### TensorFlow 下的線性回歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48cc01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc560fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d37f5301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.98176914> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.054570343>\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
    "for e in range(num_epoch):\n",
    "    # 使用tf.GradientTape()記錄損失函數的梯度資訊\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\n",
    "    # TensorFlow自動計算損失函數關於自變數（模型參數）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, variables))\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53779cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
